{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.7.1\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/03_pipelines/env`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3 - Transformers and Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unsupervised models, which receive no target `y` during training,\n",
    "always have a `transform` operation. They sometimes also support an\n",
    "`inverse_transform` operation, with obvious meaning, and sometimes\n",
    "support a `predict` operation (see the clustering example discussed\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Transformers-that-also-predict-1)).\n",
    "Otherwise, they are handled much like supervised models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a simple standardization example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(x) = 0.47253618410343046\n",
      "std(x) = 0.3027701918985175\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "\n",
    "x = rand(100);\n",
    "@show mean(x) std(x);"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{Standardizer,…}.\n",
      "mean(xhat) = 5.773159728050814e-17\n",
      "std(xhat) = 1.0\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "model = Standardizer() # a built-in model\n",
    "mach = machine(model, x)\n",
    "fit!(mach)\n",
    "xhat = transform(mach, x);\n",
    "@show mean(xhat) std(xhat);"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This particular model has an `inverse_transform`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "inverse_transform(mach, xhat) ≈ x"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Re-encoding the King County House data as continuous"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For further illustrations of transformers, let's re-encode *all* of the\n",
    "King County House input features (see [Ex\n",
    "3](#exercise-3-fixing-scitypes-in-a-table)) into a set of `Continuous`\n",
    "features. We do this with the `ContinuousEncoder` model, which, by\n",
    "default, will:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- one-hot encode all `Multiclass` features\n",
    "- coerce all `OrderedFactor` features to `Continuous` ones\n",
    "- coerce all `Count` features to `Continuous` ones (there aren't any)\n",
    "- drop any remaining non-Continuous features (none of these either)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we reload the data and fix the scitypes (Exercise 3):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌───────────────┬───────────────────┬───────────────────────────────────┐\n│\u001b[22m names         \u001b[0m│\u001b[22m scitypes          \u001b[0m│\u001b[22m types                             \u001b[0m│\n├───────────────┼───────────────────┼───────────────────────────────────┤\n│ price         │ Continuous        │ Float64                           │\n│ bedrooms      │ OrderedFactor{13} │ CategoricalValue{Int64, UInt32}   │\n│ bathrooms     │ OrderedFactor{30} │ CategoricalValue{Float64, UInt32} │\n│ sqft_living   │ Continuous        │ Float64                           │\n│ sqft_lot      │ Continuous        │ Float64                           │\n│ floors        │ OrderedFactor{6}  │ CategoricalValue{Float64, UInt32} │\n│ waterfront    │ OrderedFactor{2}  │ CategoricalValue{Int64, UInt32}   │\n│ view          │ OrderedFactor{5}  │ CategoricalValue{Int64, UInt32}   │\n│ condition     │ OrderedFactor{5}  │ CategoricalValue{Int64, UInt32}   │\n│ grade         │ OrderedFactor{12} │ CategoricalValue{Int64, UInt32}   │\n│ sqft_above    │ Continuous        │ Float64                           │\n│ sqft_basement │ Continuous        │ Float64                           │\n│ yr_built      │ Continuous        │ Float64                           │\n│ zipcode       │ Multiclass{70}    │ CategoricalValue{Int64, UInt32}   │\n│ lat           │ Continuous        │ Float64                           │\n│ long          │ Continuous        │ Float64                           │\n│       ⋮       │         ⋮         │                 ⋮                 │\n└───────────────┴───────────────────┴───────────────────────────────────┘\n\u001b[36m                                                           3 rows omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "import DataFrames\n",
    "house_csv = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                        \"MachineLearningInJulia2020/for-MLJ-version-0.16/\"*\n",
    "                        \"data/house.csv\");\n",
    "house = DataFrames.DataFrame(house_csv)\n",
    "coerce!(house, autotype(house_csv));\n",
    "coerce!(house, Count => Continuous, :zipcode => Multiclass);\n",
    "schema(house)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(house, ==(:price), rng=123);"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate the unsupervised model (transformer):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ContinuousEncoder(\n    drop_last = false,\n    one_hot_ordered_factors = false)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "encoder = ContinuousEncoder() # a built-in model; no need to @load it"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bind the model to the data and fit!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ContinuousEncoder,…}.\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(encoder, X) |> fit!;"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform and inspect the result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌────────────────┬────────────┬─────────┐\n│\u001b[22m names          \u001b[0m│\u001b[22m scitypes   \u001b[0m│\u001b[22m types   \u001b[0m│\n├────────────────┼────────────┼─────────┤\n│ bedrooms       │ Continuous │ Float64 │\n│ bathrooms      │ Continuous │ Float64 │\n│ sqft_living    │ Continuous │ Float64 │\n│ sqft_lot       │ Continuous │ Float64 │\n│ floors         │ Continuous │ Float64 │\n│ waterfront     │ Continuous │ Float64 │\n│ view           │ Continuous │ Float64 │\n│ condition      │ Continuous │ Float64 │\n│ grade          │ Continuous │ Float64 │\n│ sqft_above     │ Continuous │ Float64 │\n│ sqft_basement  │ Continuous │ Float64 │\n│ yr_built       │ Continuous │ Float64 │\n│ zipcode__98001 │ Continuous │ Float64 │\n│ zipcode__98002 │ Continuous │ Float64 │\n│ zipcode__98003 │ Continuous │ Float64 │\n│ zipcode__98004 │ Continuous │ Float64 │\n│       ⋮        │     ⋮      │    ⋮    │\n└────────────────┴────────────┴─────────┘\n\u001b[36m                          71 rows omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "Xcont = transform(mach, X);\n",
    "schema(Xcont)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to list all of MLJ's unsupervised models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "60-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BM25Transformer, package_name = MLJText, ... )\n (name = BagOfWordsTransformer, package_name = MLJText, ... )\n (name = Birch, package_name = ScikitLearn, ... )\n (name = CBLOFDetector, package_name = OutlierDetectionPython, ... )\n (name = COFDetector, package_name = OutlierDetectionNeighbors, ... )\n ⋮\n (name = SpectralClustering, package_name = ScikitLearn, ... )\n (name = Standardizer, package_name = MLJModels, ... )\n (name = TSVDTransformer, package_name = TSVD, ... )\n (name = TfidfTransformer, package_name = MLJText, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "models(m->!m.is_supervised)"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some commonly used ones are built-in (do not require `@load`ing):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model type                  | does what?\n",
    "----------------------------|----------------------------------------------\n",
    "ContinuousEncoder | transform input table to a table of `Continuous` features (see above)\n",
    "FeatureSelector | retain or dump selected features\n",
    "FillImputer | impute missing values\n",
    "OneHotEncoder | one-hot encoder `Multiclass` (and optionally `OrderedFactor`) features\n",
    "Standardizer | standardize (whiten) a vector or all `Continuous` features of a table\n",
    "UnivariateBoxCoxTransformer | apply a learned Box-Cox transformation to a vector\n",
    "UnivariateDiscretizer | discretize a `Continuous` vector, and hence render its elscitypw `OrderedFactor`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to \"dynamic\" transformers (ones that learn something\n",
    "from the data and must be `fit!`) users can wrap ordinary functions\n",
    "as transformers, and such *static* transformers can depend on\n",
    "parameters, like the dynamic ones. See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Static-transformers-1)\n",
    "for how to define your own static transformers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipelines"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "87"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "length(schema(Xcont).names)"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's suppose that additionally we'd like to reduce the dimension of\n",
    "our data.  A model that will do this is `PCA` from\n",
    "`MultivariateStats.jl`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJMultivariateStatsInterface ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PCA(\n    maxoutdim = 0,\n    method = :auto,\n    pratio = 0.99,\n    mean = nothing)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "PCA = @load PCA\n",
    "reducer = PCA()"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, rather simply repeating the work-flow above, applying the new\n",
    "transformation to `Xcont`, we can combine both the encoding and the\n",
    "dimension-reducing models into a single model, known as a\n",
    "*pipeline*. While MLJ offers a powerful interface for composing\n",
    "models in a variety of ways, we'll stick to these simplest class of\n",
    "composite models for now. The simplest way to construct a pipeline\n",
    "is using the Julia's `|>` syntax:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "UnsupervisedPipeline(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = false,\n            one_hot_ordered_factors = false),\n    pca = PCA(\n            maxoutdim = 0,\n            method = :auto,\n            pratio = 0.99,\n            mean = nothing),\n    cache = true)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe = encoder |> reducer"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the model `pipe` has other models as hyperparameters\n",
    "(with names automatically generated based on the mode type\n",
    "name). The hyperparameters of the component models are are now\n",
    "*nested*, but we can still access them:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe.pca.pratio = 0.99\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.85"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "@show pipe.pca.pratio\n",
    "pipe.pca.pratio = 0.85"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline model behaves like any other transformer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{UnsupervisedPipeline{NamedTuple{,…},…},…}.\n",
      "[ Info: Training Machine{ContinuousEncoder,…}.\n",
      "[ Info: Training Machine{PCA,…}.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌───────┬────────────┬─────────┐\n│\u001b[22m names \u001b[0m│\u001b[22m scitypes   \u001b[0m│\u001b[22m types   \u001b[0m│\n├───────┼────────────┼─────────┤\n│ x1    │ Continuous │ Float64 │\n└───────┴────────────┴─────────┘\n"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(pipe, X)\n",
    "fit!(mach)\n",
    "Xsmall = transform(mach, X)\n",
    "schema(Xsmall)"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Want to combine this pre-processing with ridge regression?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJLinearModels ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicPipeline(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = false,\n            one_hot_ordered_factors = false),\n    pca = PCA(\n            maxoutdim = 0,\n            method = :auto,\n            pratio = 0.85,\n            mean = nothing),\n    ridge_regressor = RidgeRegressor(\n            lambda = 1.0,\n            fit_intercept = true,\n            penalize_intercept = false,\n            solver = nothing),\n    cache = true)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "RidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels\n",
    "rgs = RidgeRegressor()\n",
    "pipe2 = pipe |> rgs"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now our pipeline is a supervised model, instead of a transformer,\n",
    "whose performance we can evaluate:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌─────────────────────┬─────────────┬───────────┬────────────┐\n│\u001b[22m measure             \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold   \u001b[0m│\n├─────────────────────┼─────────────┼───────────┼────────────┤\n│ MeanAbsoluteError() │ 234000.0    │ predict   │ [234000.0] │\n└─────────────────────┴─────────────┴───────────┴────────────┘\n"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(pipe2, X, y)\n",
    "evaluate!(mach, measure=mae, resampling=Holdout()) # CV(nfolds=6) is default"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training of composite models is \"smart\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now notice what happens if we train on all the data, then change a\n",
    "regressor hyper-parameter and retrain:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{DeterministicPipeline{NamedTuple{,…},…},…}.\n",
      "[ Info: Training Machine{ContinuousEncoder,…}.\n",
      "[ Info: Training Machine{PCA,…}.\n",
      "[ Info: Training Machine{RidgeRegressor,…}.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicPipeline{NamedTuple{,…},…},…} trained 2 times; caches data\n  model: MLJBase.DeterministicPipeline{NamedTuple{(:continuous_encoder, :pca, :ridge_regressor), Tuple{MLJModelInterface.Unsupervised, MLJModelInterface.Unsupervised, MLJModelInterface.Deterministic}}, MLJModelInterface.predict}\n  args: \n    1:\tSource @864 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{70}}, AbstractVector{ScientificTypesBase.OrderedFactor{6}}, AbstractVector{ScientificTypesBase.OrderedFactor{13}}, AbstractVector{ScientificTypesBase.OrderedFactor{30}}, AbstractVector{ScientificTypesBase.OrderedFactor{5}}, AbstractVector{ScientificTypesBase.OrderedFactor{12}}, AbstractVector{ScientificTypesBase.OrderedFactor{2}}}}`\n    2:\tSource @134 ⏎ `AbstractVector{ScientificTypesBase.Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{DeterministicPipeline{NamedTuple{,…},…},…}.\n",
      "[ Info: Not retraining Machine{ContinuousEncoder,…}. Use `force=true` to force.\n",
      "[ Info: Not retraining Machine{PCA,…}. Use `force=true` to force.\n",
      "[ Info: Updating Machine{RidgeRegressor,…}.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicPipeline{NamedTuple{,…},…},…} trained 3 times; caches data\n  model: MLJBase.DeterministicPipeline{NamedTuple{(:continuous_encoder, :pca, :ridge_regressor), Tuple{MLJModelInterface.Unsupervised, MLJModelInterface.Unsupervised, MLJModelInterface.Deterministic}}, MLJModelInterface.predict}\n  args: \n    1:\tSource @864 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{70}}, AbstractVector{ScientificTypesBase.OrderedFactor{6}}, AbstractVector{ScientificTypesBase.OrderedFactor{13}}, AbstractVector{ScientificTypesBase.OrderedFactor{30}}, AbstractVector{ScientificTypesBase.OrderedFactor{5}}, AbstractVector{ScientificTypesBase.OrderedFactor{12}}, AbstractVector{ScientificTypesBase.OrderedFactor{2}}}}`\n    2:\tSource @134 ⏎ `AbstractVector{ScientificTypesBase.Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe2.ridge_regressor.lambda = 0.1\n",
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second time only the ridge regressor is retrained!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mutate a hyper-parameter of the `PCA` model and every model except\n",
    "the `ContinuousEncoder` (which comes before it will be retrained):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{DeterministicPipeline{NamedTuple{,…},…},…}.\n",
      "[ Info: Not retraining Machine{ContinuousEncoder,…}. Use `force=true` to force.\n",
      "[ Info: Updating Machine{PCA,…}.\n",
      "[ Info: Training Machine{RidgeRegressor,…}.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicPipeline{NamedTuple{,…},…},…} trained 4 times; caches data\n  model: MLJBase.DeterministicPipeline{NamedTuple{(:continuous_encoder, :pca, :ridge_regressor), Tuple{MLJModelInterface.Unsupervised, MLJModelInterface.Unsupervised, MLJModelInterface.Deterministic}}, MLJModelInterface.predict}\n  args: \n    1:\tSource @864 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{70}}, AbstractVector{ScientificTypesBase.OrderedFactor{6}}, AbstractVector{ScientificTypesBase.OrderedFactor{13}}, AbstractVector{ScientificTypesBase.OrderedFactor{30}}, AbstractVector{ScientificTypesBase.OrderedFactor{5}}, AbstractVector{ScientificTypesBase.OrderedFactor{12}}, AbstractVector{ScientificTypesBase.OrderedFactor{2}}}}`\n    2:\tSource @134 ⏎ `AbstractVector{ScientificTypesBase.Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe2.pca.pratio = 0.9999\n",
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspecting composite models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dot syntax used above to change the values of *nested*\n",
    "hyper-parameters is also useful when inspecting the learned\n",
    "parameters and report generated when training a composite model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(coefs = [:x1 => -0.7328956348956909, :x2 => -0.16590563202915437, :x3 => 194.59515890822126, :x4 => 102.71301756136401],\n intercept = 540085.6428739978,)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach).ridge_regressor"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(indim = 87,\n outdim = 4,\n tprincipalvar = 2.463215246230867e9,\n tresidualvar = 157533.26199626923,\n tvar = 2.463372779492863e9,\n mean = [4.369869985656781, 8.45912182482765, 2079.8997362698374, 15106.967565816869, 1.988617961412113, 1.0075417572757137, 1.2343034284921113, 3.4094295100171195, 6.6569194466293435, 1788.3906907879516  …  0.011798454633785222, 0.012122333780595013, 0.006292509138018785, 0.012955165872391617, 0.01466709850552908, 47.560052519317075, -122.21389640494147, 1986.552491556008, 12768.455651691113, 1.9577106371165502],\n principalvars = [2.177071551045086e9, 2.8418139726430327e8, 1.685016083064339e6, 277281.8384131848],)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach).pca"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incorporating target transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, suppose that instead of using the raw `:price` as the training\n",
    "target, we want to use the log-price (a common practice in dealing\n",
    "with house price data). However, suppose that we still want to\n",
    "report final *predictions* on the original linear scale (and use\n",
    "these for evaluation purposes). Then we wrap our supervised model\n",
    "using `TransformedTargetModel`, which has to key-word arguments\n",
    "`target` and `inverse`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we'll overload `log` and `exp` for broadcasting:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Base.log(v::AbstractArray) = log.(v)\n",
    "Base.exp(v::AbstractArray) = exp.(v)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for the new pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:12\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:01\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:06\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌─────────────────────┬─────────────┬───────────┬───────────────────────────────\n│\u001b[22m measure             \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                    \u001b[0m ⋯\n├─────────────────────┼─────────────┼───────────┼───────────────────────────────\n│ MeanAbsoluteError() │ 162000.0    │ predict   │ [160000.0, 161000.0, 164000. ⋯\n└─────────────────────┴─────────────┴───────────┴───────────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "rgs_log = TransformedTargetModel(rgs, target=log, inverse=exp)\n",
    "\n",
    "pipe3 = pipe |> rgs_log\n",
    "mach = machine(pipe3, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLJ will also allow you to insert *learned* target\n",
    "transformations. For example, we might want to apply\n",
    "`Standardizer()` to the target, to standardize it, or\n",
    "`UnivariateBoxCoxTransformer()` to make it look Gaussian. Then\n",
    "instead of specifying a *function* for `target`, we specify a\n",
    "unsupervised *model* (or model type). One does not specify `inverse`\n",
    "because only models implementing `inverse_transform` are\n",
    "allowed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see which of these two options results in a better outcome:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:14\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:09\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:15\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌─────────────────────┬─────────────┬───────────┬───────────────────────────────\n│\u001b[22m measure             \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                    \u001b[0m ⋯\n├─────────────────────┼─────────────┼───────────┼───────────────────────────────\n│ MeanAbsoluteError() │ 479000.0    │ predict   │ [168000.0, 172000.0, 170000. ⋯\n└─────────────────────┴─────────────┴───────────┴───────────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "cell_type": "code",
   "source": [
    "box = UnivariateBoxCoxTransformer(n=20)\n",
    "stand = Standardizer()\n",
    "\n",
    "rgs_box = TransformedTargetModel(rgs, target=box)\n",
    "pipe4 = pipe |> rgs_box\n",
    "mach = machine(pipe4, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:06\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:01\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:03\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌─────────────────────┬─────────────┬───────────┬───────────────────────────────\n│\u001b[22m measure             \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                    \u001b[0m ⋯\n├─────────────────────┼─────────────┼───────────┼───────────────────────────────\n│ MeanAbsoluteError() │ 172000.0    │ predict   │ [173000.0, 171000.0, 172000. ⋯\n└─────────────────────┴─────────────┴───────────┴───────────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe4.transformed_target_model_deterministic.target = stand\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Transformers and other unsupervised models](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/)\n",
    "    - [Linear pipelines](https://alan-turing-institute.github.io/MLJ.jl/dev/linear_pipelines/#Linear-Pipelines)\n",
    "- From Data Science Tutorials:\n",
    "    - [Composing models](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/composing-models/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider again the Horse Colic classification problem considered in\n",
    "Exercise 6, but with all features, `Finite` and `Infinite`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬──────────────────┬──────────────────────────────────\n│\u001b[22m names                   \u001b[0m│\u001b[22m scitypes         \u001b[0m│\u001b[22m types                          \u001b[0m ⋯\n├─────────────────────────┼──────────────────┼──────────────────────────────────\n│ surgery                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ age                     │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ rectal_temperature      │ Continuous       │ Float64                         ⋯\n│ pulse                   │ Continuous       │ Float64                         ⋯\n│ respiratory_rate        │ Continuous       │ Float64                         ⋯\n│ temperature_extremities │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ mucous_membranes        │ Multiclass{6}    │ CategoricalValue{Int64, UInt32} ⋯\n│ capillary_refill_time   │ Multiclass{3}    │ CategoricalValue{Int64, UInt32} ⋯\n│ pain                    │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} ⋯\n│ peristalsis             │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ abdominal_distension    │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ packed_cell_volume      │ Continuous       │ Float64                         ⋯\n│ total_protein           │ Continuous       │ Float64                         ⋯\n│ surgical_lesion         │ OrderedFactor{2} │ CategoricalValue{Int64, UInt32} ⋯\n│ cp_data                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n└─────────────────────────┴──────────────────┴──────────────────────────────────\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "\n",
    "y, X = unpack(horse, ==(:outcome));\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Define a pipeline that:\n",
    "- uses `Standardizer` to ensure that features that are already\n",
    "  continuous are centered at zero and have unit variance\n",
    "- re-encodes the full set of features as `Continuous`, using\n",
    "  `ContinuousEncoder`\n",
    "- uses the `KMeans` clustering model from `Clustering.jl`\n",
    "  to reduce the dimension of the feature space to `k=10`.\n",
    "- trains a `EvoTreeClassifier` (a gradient tree boosting\n",
    "  algorithm in `EvoTrees.jl`) on the reduced data, using\n",
    "  `nrounds=50` and default values for the other\n",
    "   hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Evaluate the pipeline on all data, using 6-fold cross-validation\n",
    "and `cross_entropy` loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "&star;(c) Plot a learning curve which examines the effect on this loss\n",
    "as the tree booster parameter `max_depth` varies from 2 to 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-4-tuning-hyper-parameters'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
