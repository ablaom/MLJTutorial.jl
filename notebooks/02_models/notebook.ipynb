{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.6.3\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env/Project.toml`\n",
      "Precompiling project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJEnsembles\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJIteration\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJTuning\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJSerialization\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModels\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJ\n",
      "\u001b[32m  ✓ \u001b[39mPlots\n",
      "  8 dependencies successfully precompiled in 44 seconds (200 already precompiled)\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Precompiling MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 -\n  Donated by Michael Marshall \u001b[1mPlease cite\u001b[22m:\n\n  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in the pattern\n  recognition literature. Fisher's paper is a classic in the field and is referenced\n  frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes\n  of 50 instances each, where each class refers to a type of iris plant. One class is\n  linearly separable from the other 2; the latter are NOT linearly separable from each\n  other.\n\n  Predicted attribute: class of iris plant. This is an exceedingly simple domain.\n\n\u001b[1m  Attribute Information:\u001b[22m\n\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n\n\u001b[36m  1. sepal length in cm\u001b[39m\n\u001b[36m  2. sepal width in cm\u001b[39m\n\u001b[36m  3. petal length in cm\u001b[39m\n\u001b[36m  4. petal width in cm\u001b[39m\n\u001b[36m  5. class: \u001b[39m\n\u001b[36m     -- Iris Setosa\u001b[39m\n\u001b[36m     -- Iris Versicolour\u001b[39m\n\u001b[36m     -- Iris Virginica\u001b[39m",
      "text/markdown": "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n\n**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n\n### Attribute Information:\n\n```\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n   -- Iris Setosa\n   -- Iris Versicolour\n   -- Iris Virginica\n```\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n─────┼───────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n   4 │         4.6         3.1          1.5         0.2  Iris-setosa",
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "using DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬──────────────────────────────────┬───────────────┐\n│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n├─────────────┼──────────────────────────────────┼───────────────┤\n│ sepallength │ Float64                          │ Continuous    │\n│ sepalwidth  │ Float64                          │ Continuous    │\n│ petallength │ Float64                          │ Continuous    │\n│ petalwidth  │ Float64                          │ Continuous    │\n│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n└─────────────┴──────────────────────────────────┴───────────────┘\n_.nrows = 150\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "These look fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We randomize the data at the\n",
    "same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{ScientificTypesBase.Multiclass{3}} (alias for AbstractArray{ScientificTypesBase.Multiclass{3}, 1})"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), name->true; rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[36m  t1, t2, ...., tk = unpack(table, f1, f2, ... fk;\u001b[39m\n\u001b[36m                           wrap_singles=false,\u001b[39m\n\u001b[36m                           shuffle=false,\u001b[39m\n\u001b[36m                           rng::Union{AbstractRNG,Int,Nothing}=nothing)\u001b[39m\n\n  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables (or vectors) \u001b[36mt1,\n  t2, ..., tk\u001b[39m by making column selections \u001b[1mwithout replacement\u001b[22m by successively applying the\n  columnn name filters \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m, ..., \u001b[36mfk\u001b[39m. A \u001b[4mfilter\u001b[24m is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m\n  or \u001b[36mfalse\u001b[39m for each column \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m. For example, use the filter \u001b[36m_ -> true\u001b[39m to\n  pick up all remaining columns of the table.\n\n  Whenever a returned table contains a single column, it is converted to a vector unless\n  \u001b[36mwrap_singles=true\u001b[39m.\n\n  Scientific type conversions can be optionally specified (note semicolon):\n\n\u001b[36m  unpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\u001b[39m\n\n  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global RNG, unless\n  \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of an automatically\n  generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then \u001b[36mshuffle=true\u001b[39m is implicit.\n\n\u001b[1m  Example\u001b[22m\n\u001b[1m  –––––––––\u001b[22m\n\n\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n\u001b[36m  julia> Z, XY = unpack(table, ==(:z), !=(:w);\u001b[39m\n\u001b[36m                 :x=>Continuous, :y=>Multiclass)\u001b[39m\n\u001b[36m  julia> XY\u001b[39m\n\u001b[36m  2×2 DataFrame\u001b[39m\n\u001b[36m  │ Row │ x       │ y            │\u001b[39m\n\u001b[36m  │     │ Float64 │ Categorical… │\u001b[39m\n\u001b[36m  ├─────┼─────────┼──────────────┤\u001b[39m\n\u001b[36m  │ 1   │ 1.0     │ 'a'          │\u001b[39m\n\u001b[36m  │ 2   │ 2.0     │ 'b'          │\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> Z\u001b[39m\n\u001b[36m  2-element Array{Float64,1}:\u001b[39m\n\u001b[36m   10.0\u001b[39m\n\u001b[36m   20.0\u001b[39m",
      "text/markdown": "```\nt1, t2, ...., tk = unpack(table, f1, f2, ... fk;\n                         wrap_singles=false,\n                         shuffle=false,\n                         rng::Union{AbstractRNG,Int,Nothing}=nothing)\n```\n\nHorizontally split any Tables.jl compatible `table` into smaller tables (or vectors) `t1, t2, ..., tk` by making column selections **without replacement** by successively applying the columnn name filters `f1`, `f2`, ..., `fk`. A *filter* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`. For example, use the filter `_ -> true` to pick up all remaining columns of the table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n\nScientific type conversions can be optionally specified (note semicolon):\n\n```\nunpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\n```\n\nIf `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n\n### Example\n\n```\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n```\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "183-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = ARDRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BaggingRegressor, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )\n (name = Birch, package_name = ScikitLearn, ... )\n (name = CBLOFDetector, package_name = OutlierDetectionPython, ... )\n (name = COFDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = COFDetector, package_name = OutlierDetectionPython, ... )\n (name = COPODDetector, package_name = OutlierDetectionPython, ... )\n (name = ComplementNBClassifier, package_name = ScikitLearn, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = ConstantRegressor, package_name = MLJModels, ... )\n (name = ContinuousEncoder, package_name = MLJModels, ... )\n ⋮\n (name = RidgeRegressor, package_name = ScikitLearn, ... )\n (name = RobustRegressor, package_name = MLJLinearModels, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SGDRegressor, package_name = ScikitLearn, ... )\n (name = SODDetector, package_name = OutlierDetectionPython, ... )\n (name = SOSDetector, package_name = OutlierDetectionPython, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearRegressor, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuRegressor, package_name = ScikitLearn, ... )\n (name = SVMRegressor, package_name = ScikitLearn, ... )\n (name = SpectralClustering, package_name = ScikitLearn, ... )\n (name = Standardizer, package_name = MLJModels, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = TSVDTransformer, package_name = TSVD, ... )\n (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )\n (name = XGBoostCount, package_name = XGBoost, ... )\n (name = XGBoostRegressor, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mAEDetector from OutlierDetectionNetworks.jl.\u001b[39m\n\u001b[35m[Documentation](https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl).\u001b[39m\n(name = \"AEDetector\",\n package_name = \"OutlierDetectionNetworks\",\n is_supervised = false,\n abstract_type = MLJModelInterface.UnsupervisedDetector,\n deep_properties = (),\n docstring = \"AEDetector from OutlierDetectionNetworks.jl.\\n[Documentation](https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl).\",\n fit_data_scitype = Tuple{Union{ScientificTypesBase.Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:ScientificTypesBase.Continuous), AbstractMatrix{_s689} where _s689<:ScientificTypesBase.Continuous}},\n hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n hyperparameter_types = (\"Flux.Chain\", \"Flux.Chain\", \"Integer\", \"Integer\", \"Bool\", \"Bool\", \"Any\", \"Function\"),\n hyperparameters = (:encoder, :decoder, :batchsize, :epochs, :shuffle, :partial, :opt, :loss),\n implemented_methods = [:clean!, :fit, :transform],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = nothing,\n load_path = \"OutlierDetectionNetworks.AEDetector\",\n package_license = \"MIT\",\n package_url = \"https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl\",\n package_uuid = \"c7f57e37-4fcb-4a0b-a36c-c2204bc839a7\",\n predict_scitype = ScientificTypesBase.Unknown,\n prediction_type = :unknown,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = false,\n supports_weights = false,\n transform_scitype = AbstractVector{_s689} where _s689<:ScientificTypesBase.Continuous,\n input_scitype = Union{ScientificTypesBase.Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:ScientificTypesBase.Continuous), AbstractMatrix{_s689} where _s689<:ScientificTypesBase.Continuous},\n target_scitype = AbstractVector{_s689} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}},\n output_scitype = AbstractVector{_s689} where _s689<:ScientificTypesBase.Continuous,)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "meta = all_models[3]"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{_s689} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}} (alias for AbstractArray{_s689, 1} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}})"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "false"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's  find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n (name = LDA, package_name = MultivariateStats, ... )\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all models with \"Classifier\" in `name` (or `docstring`):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "45-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )\n (name = ComplementNBClassifier, package_name = ScikitLearn, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = DummyClassifier, package_name = ScikitLearn, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n (name = ImageClassifier, package_name = MLJFlux, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n (name = LGBMClassifier, package_name = LightGBM, ... )\n (name = LinearBinaryClassifier, package_name = GLM, ... )\n (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = LogisticClassifier, package_name = ScikitLearn, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = MultinomialNBClassifier, package_name = ScikitLearn, ... )\n (name = MultitargetKNNClassifier, package_name = NearestNeighborModels, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "models(\"Classifier\")"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = DummyClassifier, package_name = ScikitLearn, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n (name = LDA, package_name = MultivariateStats, ... )\n (name = LGBMClassifier, package_name = LightGBM, ... )\n (name = LinearSVC, package_name = LIBSVM, ... )\n (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = LogisticClassifier, package_name = ScikitLearn, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = NuSVC, package_name = LIBSVM, ... )\n (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux[ Info: Precompiling MLJFlux [094fc8d1-fd35-5302-93ea-dabda2abf845]\n",
      " ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n    builder = Short(\n            n_hidden = 0,\n            dropout = 0.5,\n            σ = NNlib.σ),\n    finaliser = NNlib.softmax,\n    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n    loss = Flux.Losses.crossentropy,\n    epochs = 10,\n    batch_size = 1,\n    lambda = 0.0,\n    alpha = 0.0,\n    rng = Random._GLOBAL_RNG(),\n    optimiser_changes_trigger_retraining = false,\n    acceleration = ComputationalResources.CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n(name = \"NeuralNetworkClassifier\",\n package_name = \"MLJFlux\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (:optimiser, :builder),\n docstring = \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n fit_data_scitype = Tuple{ScientificTypesBase.Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:ScientificTypesBase.Continuous), AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite},\n hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n hyperparameter_types = (\"MLJFlux.Short\", \"typeof(NNlib.softmax)\", \"Flux.Optimise.ADAM\", \"typeof(Flux.Losses.crossentropy)\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\"),\n hyperparameters = (:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration),\n implemented_methods = Any[],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = :epochs,\n load_path = \"MLJFlux.NeuralNetworkClassifier\",\n package_license = \"MIT\",\n package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n predict_scitype = AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = true,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype = ScientificTypesBase.Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:ScientificTypesBase.Continuous),\n target_scitype = AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite,\n output_scitype = ScientificTypesBase.Unknown,)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n  args: \n    1:\tSource @922 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @606 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.085\n",
      "[ Info: Loss is 1.077\n",
      "[ Info: Loss is 1.075\n",
      "[ Info: Loss is 1.089\n",
      "[ Info: Loss is 1.07\n",
      "[ Info: Loss is 1.059\n",
      "[ Info: Loss is 1.046\n",
      "[ Info: Loss is 1.002\n",
      "[ Info: Loss is 1.06\n",
      "[ Info: Loss is 1.015\n",
      "[ Info: Loss is 1.018\n",
      "[ Info: Loss is 1.001\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n  args: \n    1:\tSource @922 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @606 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.236, Iris-versicolor=>0.368, Iris-virginica=>0.396)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.219, Iris-versicolor=>0.377, Iris-virginica=>0.404)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.326, Iris-versicolor=>0.342, Iris-virginica=>0.332)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(training_losses = [1.0861040699057927, 1.0850897919767752, 1.0769053903007801, 1.0750227728096013, 1.089012598973693, 1.0704945614082946, 1.0594718355110957, 1.0464876223048019, 1.0018646685106747, 1.059771648406736, 1.0152558096358695, 1.0180116154154317, 1.001068094119868],)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.203, Iris-versicolor=>0.366, Iris-virginica=>0.431)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.236, Iris-versicolor=>0.37, Iris-virginica=>0.395)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.21, Iris-versicolor=>0.384, Iris-virginica=>0.407)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net: 15%[===>                     ]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @879 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @071 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.043\n",
      "[ Info: Loss is 1.01\n",
      "[ Info: Loss is 0.9724\n",
      "[ Info: Loss is 1.002\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @922 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @606 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9614\n",
      "[ Info: Loss is 0.8292\n",
      "[ Info: Loss is 0.7718\n",
      "[ Info: Loss is 0.7561\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n  args: \n    1:\tSource @922 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @606 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.086\n",
      "[ Info: Loss is 1.042\n",
      "[ Info: Loss is 0.8886\n",
      "[ Info: Loss is 0.8922\n",
      "[ Info: Loss is 0.8259\n",
      "[ Info: Loss is 0.7921\n",
      "[ Info: Loss is 0.7022\n",
      "[ Info: Loss is 0.7267\n",
      "[ Info: Loss is 0.6664\n",
      "[ Info: Loss is 0.7875\n",
      "[ Info: Loss is 0.6484\n",
      "[ Info: Loss is 0.6612\n",
      "[ Info: Loss is 0.7152\n",
      "[ Info: Loss is 0.6611\n",
      "[ Info: Loss is 0.6801\n",
      "[ Info: Loss is 0.6801\n",
      "[ Info: Loss is 0.6743\n",
      "[ Info: Loss is 0.7638\n",
      "[ Info: Loss is 0.7066\n",
      "[ Info: Loss is 0.6635\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n  args: \n    1:\tSource @922 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @606 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 13%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 19%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 26%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 32%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 42%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 48%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 52%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 58%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 68%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 74%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 81%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 87%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 97%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.141, Iris-versicolor=>0.55, Iris-virginica=>0.308)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":probabilistic"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**:\n",
    "- In MLJ, a model that can predict probabilities (and not just point values) will do so by default.\n",
    "- For most probabilistic predictors, the predicted object is a `Distributions.Distribution` object, supporting the `Distributions.jl` [API](https://juliastats.org/Distributions.jl/latest/extends/#Create-a-Distribution-1) for such objects. In particular, the methods `rand`,  `pdf`, `logpdf`, `mode`, `median` and `mean` will apply, where appropriate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3083039233899632"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-versicolor\""
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Float64}:\n 0.5504965452205032\n 0.5154036056708229\n 0.05542404604641261\n 0.4085753827071587"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": 36
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4×3 Matrix{Float64}:\n 0.1412     0.550497  0.308304\n 0.093372   0.515404  0.391224\n 0.943101   0.055424  0.00147495\n 0.0330795  0.408575  0.558345"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3979336466814202"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.022222222222222223"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "61-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type), T} where T<:Tuple}:\n (name = BrierLoss, instances = [brier_loss], ...)\n (name = BrierScore, instances = [brier_score], ...)\n (name = LPLoss, instances = [l1, l2], ...)\n (name = LogCoshLoss, instances = [log_cosh, log_cosh_loss], ...)\n (name = LogLoss, instances = [log_loss, cross_entropy], ...)\n (name = LogScore, instances = [log_score], ...)\n (name = SphericalScore, instances = [spherical_score], ...)\n (name = Accuracy, instances = [accuracy], ...)\n (name = AreaUnderCurve, instances = [area_under_curve, auc], ...)\n (name = BalancedAccuracy, instances = [balanced_accuracy, bacc, bac], ...)\n (name = ConfusionMatrix, instances = [confusion_matrix, confmat], ...)\n (name = FScore, instances = [f1score], ...)\n (name = FalseDiscoveryRate, instances = [false_discovery_rate, falsediscovery_rate, fdr], ...)\n (name = FalseNegative, instances = [false_negative, falsenegative], ...)\n (name = FalseNegativeRate, instances = [false_negative_rate, falsenegative_rate, fnr, miss_rate], ...)\n (name = FalsePositive, instances = [false_positive, falsepositive], ...)\n (name = FalsePositiveRate, instances = [false_positive_rate, falsepositive_rate, fpr, fallout], ...)\n (name = MatthewsCorrelation, instances = [matthews_correlation, mcc], ...)\n (name = MeanAbsoluteError, instances = [mae, mav, mean_absolute_error, mean_absolute_value], ...)\n (name = MeanAbsoluteProportionalError, instances = [mape], ...)\n (name = MisclassificationRate, instances = [misclassification_rate, mcr], ...)\n (name = MulticlassFScore, instances = [macro_f1score, micro_f1score, multiclass_f1score], ...)\n (name = MulticlassFalseDiscoveryRate, instances = [multiclass_falsediscovery_rate, multiclass_fdr], ...)\n (name = MulticlassFalseNegative, instances = [multiclass_false_negative, multiclass_falsenegative], ...)\n (name = MulticlassFalseNegativeRate, instances = [multiclass_false_negative_rate, multiclass_fnr, multiclass_miss_rate, multiclass_falsenegative_rate], ...)\n (name = MulticlassFalsePositive, instances = [multiclass_false_positive, multiclass_falsepositive], ...)\n ⋮\n (name = RootMeanSquaredError, instances = [rms, rmse, root_mean_squared_error], ...)\n (name = RootMeanSquaredLogError, instances = [rmsl, rmsle, root_mean_squared_log_error], ...)\n (name = RootMeanSquaredLogProportionalError, instances = [rmslp1], ...)\n (name = RootMeanSquaredProportionalError, instances = [rmsp], ...)\n (name = TrueNegative, instances = [true_negative, truenegative], ...)\n (name = TrueNegativeRate, instances = [true_negative_rate, truenegative_rate, tnr, specificity, selectivity], ...)\n (name = TruePositive, instances = [true_positive, truepositive], ...)\n (name = TruePositiveRate, instances = [true_positive_rate, truepositive_rate, tpr, sensitivity, recall, hit_rate], ...)\n (name = DWDMarginLoss, instances = [dwd_margin_loss], ...)\n (name = ExpLoss, instances = [exp_loss], ...)\n (name = L1HingeLoss, instances = [l1_hinge_loss], ...)\n (name = L2HingeLoss, instances = [l2_hinge_loss], ...)\n (name = L2MarginLoss, instances = [l2_margin_loss], ...)\n (name = LogitMarginLoss, instances = [logit_margin_loss], ...)\n (name = ModifiedHuberLoss, instances = [modified_huber_loss], ...)\n (name = PerceptronLoss, instances = [perceptron_loss], ...)\n (name = SigmoidLoss, instances = [sigmoid_loss], ...)\n (name = SmoothedL1HingeLoss, instances = [smoothed_l1_hinge_loss], ...)\n (name = ZeroOneLoss, instances = [zero_one_loss], ...)\n (name = HuberLoss, instances = [huber_loss], ...)\n (name = L1EpsilonInsLoss, instances = [l1_epsilon_ins_loss], ...)\n (name = L2EpsilonInsLoss, instances = [l2_epsilon_ins_loss], ...)\n (name = LPDistLoss, instances = [lp_dist_loss], ...)\n (name = LogitDistLoss, instances = [logit_dist_loss], ...)\n (name = PeriodicLoss, instances = [periodic_loss], ...)\n (name = QuantileLoss, instances = [quantile_loss], ...)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬──────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold \u001b[0m│\n├────────────────────────────┼─────────────┼───────────┼──────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.398       │ predict   │ [0.398]  │\n│ BrierScore()               │ -0.223      │ predict   │ [-0.223] │\n└────────────────────────────┴─────────────┴───────────┴──────────┘\n"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:22\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:17\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:11\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:06\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:35\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬────────────────────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                         \u001b[0m ⋯\n├────────────────────────────┼─────────────┼───────────┼────────────────────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.401       │ predict   │ [0.435, 0.298, 0.456, 0.399, 0.45 ⋯\n│ BrierScore()               │ -0.224      │ predict   │ [-0.256, -0.155, -0.256, -0.218,  ⋯\n└────────────────────────────┴─────────────┴───────────┴────────────────────────────────────\n\u001b[36m                                                                            1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 18 folds:  11%[==>                      ]  ETA: 0:01:39\u001b[K\rEvaluating over 18 folds:  17%[====>                    ]  ETA: 0:01:35\u001b[K\rEvaluating over 18 folds:  22%[=====>                   ]  ETA: 0:01:22\u001b[K\rEvaluating over 18 folds:  28%[======>                  ]  ETA: 0:01:12\u001b[K\rEvaluating over 18 folds:  33%[========>                ]  ETA: 0:01:03\u001b[K\rEvaluating over 18 folds:  39%[=========>               ]  ETA: 0:00:55\u001b[K\rEvaluating over 18 folds:  44%[===========>             ]  ETA: 0:00:49\u001b[K\rEvaluating over 18 folds:  50%[============>            ]  ETA: 0:00:42\u001b[K\rEvaluating over 18 folds:  56%[=============>           ]  ETA: 0:00:37\u001b[K\rEvaluating over 18 folds:  61%[===============>         ]  ETA: 0:00:31\u001b[K\rEvaluating over 18 folds:  67%[================>        ]  ETA: 0:00:26\u001b[K\rEvaluating over 18 folds:  72%[==================>      ]  ETA: 0:00:22\u001b[K\rEvaluating over 18 folds:  78%[===================>     ]  ETA: 0:00:17\u001b[K\rEvaluating over 18 folds:  83%[====================>    ]  ETA: 0:00:13\u001b[K\rEvaluating over 18 folds:  89%[======================>  ]  ETA: 0:00:08\u001b[K\rEvaluating over 18 folds:  94%[=======================> ]  ETA: 0:00:04\u001b[K\rEvaluating over 18 folds: 100%[=========================] Time: 0:01:13\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬────────────────────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                         \u001b[0m ⋯\n├────────────────────────────┼─────────────┼───────────┼────────────────────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.402       │ predict   │ [0.304, 0.343, 0.351, 0.543, 0.51 ⋯\n│ BrierScore()               │ -0.223      │ predict   │ [-0.17, -0.161, -0.182, -0.327, - ⋯\n└────────────────────────────┴─────────────┴───────────┴────────────────────────────────────\n\u001b[36m                                                                            1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Creating subsamples from a subset of all rows. \n",
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:10\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:15\u001b[K\n",
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  4%[>                        ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net:  8%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 12%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 14%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 16%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 18%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 20%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 22%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 24%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 25%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 27%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 33%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 37%[=========>               ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 41%[==========>              ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 43%[==========>              ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 47%[===========>             ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 49%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 51%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 53%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 57%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 59%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 63%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 67%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 73%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 75%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 76%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 78%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 80%[====================>    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 82%[====================>    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:02\u001b[K\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5) on log scale"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log)"
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "[ Info: Attempting to evaluate 22 models.\n",
      "\rEvaluating over 22 metamodels:   0%[>                        ]  ETA: N/A\u001b[K\rEvaluating over 22 metamodels:   5%[=>                       ]  ETA: 0:00:30\u001b[K\rEvaluating over 22 metamodels:   9%[==>                      ]  ETA: 0:00:25\u001b[K\rEvaluating over 22 metamodels:  14%[===>                     ]  ETA: 0:00:16\u001b[K\rEvaluating over 22 metamodels:  18%[====>                    ]  ETA: 0:00:12\u001b[K\rEvaluating over 22 metamodels:  23%[=====>                   ]  ETA: 0:00:09\u001b[K\rEvaluating over 22 metamodels:  27%[======>                  ]  ETA: 0:00:07\u001b[K\rEvaluating over 22 metamodels:  32%[=======>                 ]  ETA: 0:00:06\u001b[K\rEvaluating over 22 metamodels:  36%[=========>               ]  ETA: 0:00:05\u001b[K\rEvaluating over 22 metamodels:  41%[==========>              ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  45%[===========>             ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  50%[============>            ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  55%[=============>           ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  59%[==============>          ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  64%[===============>         ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  68%[=================>       ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  73%[==================>      ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  77%[===================>     ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  82%[====================>    ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  86%[=====================>   ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  95%[=======================> ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels: 100%[=========================] Time: 0:00:05\u001b[K\n",
      "[ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Plot{Plots.GRBackend() n=1}",
      "image/png": "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",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip320\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip321\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip322\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,982.365 1912.76,982.365 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,822.711 1912.76,822.711 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,663.056 1912.76,663.056 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,503.401 1912.76,503.401 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,343.746 1912.76,343.746 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,184.091 1912.76,184.091 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,982.365 230.485,982.365 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,822.711 230.485,822.711 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,663.056 230.485,663.056 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,503.401 230.485,503.401 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,343.746 230.485,343.746 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,184.091 230.485,184.091 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M124.525 968.164 Q120.914 968.164 119.086 971.729 Q117.28 975.271 117.28 982.4 Q117.28 989.507 119.086 993.071 Q120.914 996.613 124.525 996.613 Q128.16 996.613 129.965 993.071 Q131.794 989.507 131.794 982.4 Q131.794 975.271 129.965 971.729 Q128.16 968.164 124.525 968.164 M124.525 964.46 Q130.336 964.46 133.391 969.067 Q136.47 973.65 136.47 982.4 Q136.47 991.127 133.391 995.733 Q130.336 1000.32 124.525 1000.32 Q118.715 1000.32 115.637 995.733 Q112.581 991.127 112.581 982.4 Q112.581 973.65 115.637 969.067 Q118.715 964.46 124.525 964.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M144.687 993.766 L149.572 993.766 L149.572 999.645 L144.687 999.645 L144.687 993.766 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M172.604 969.16 L160.798 987.609 L172.604 987.609 L172.604 969.16 M171.377 965.085 L177.257 965.085 L177.257 987.609 L182.187 987.609 L182.187 991.497 L177.257 991.497 L177.257 999.645 L172.604 999.645 L172.604 991.497 L157.002 991.497 L157.002 986.984 L171.377 965.085 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M126.007 808.509 Q122.396 808.509 120.567 812.074 Q118.761 815.616 118.761 822.745 Q118.761 829.852 120.567 833.417 Q122.396 836.958 126.007 836.958 Q129.641 836.958 131.447 833.417 Q133.275 829.852 133.275 822.745 Q133.275 815.616 131.447 812.074 Q129.641 808.509 126.007 808.509 M126.007 804.806 Q131.817 804.806 134.873 809.412 Q137.951 813.995 137.951 822.745 Q137.951 831.472 134.873 836.079 Q131.817 840.662 126.007 840.662 Q120.197 840.662 117.118 836.079 Q114.062 831.472 114.062 822.745 Q114.062 813.995 117.118 809.412 Q120.197 804.806 126.007 804.806 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M146.169 834.111 L151.053 834.111 L151.053 839.991 L146.169 839.991 L146.169 834.111 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M161.284 805.431 L179.641 805.431 L179.641 809.366 L165.567 809.366 L165.567 817.838 Q166.585 817.491 167.604 817.329 Q168.622 817.144 169.641 817.144 Q175.428 817.144 178.807 820.315 Q182.187 823.486 182.187 828.903 Q182.187 834.481 178.715 837.583 Q175.243 840.662 168.923 840.662 Q166.747 840.662 164.479 840.292 Q162.233 839.921 159.826 839.18 L159.826 834.481 Q161.909 835.616 164.132 836.171 Q166.354 836.727 168.831 836.727 Q172.835 836.727 175.173 834.62 Q177.511 832.514 177.511 828.903 Q177.511 825.292 175.173 823.185 Q172.835 821.079 168.831 821.079 Q166.956 821.079 165.081 821.495 Q163.229 821.912 161.284 822.792 L161.284 805.431 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M124.849 648.854 Q121.238 648.854 119.41 652.419 Q117.604 655.961 117.604 663.09 Q117.604 670.197 119.41 673.762 Q121.238 677.303 124.849 677.303 Q128.484 677.303 130.289 673.762 Q132.118 670.197 132.118 663.09 Q132.118 655.961 130.289 652.419 Q128.484 648.854 124.849 648.854 M124.849 645.151 Q130.66 645.151 133.715 649.757 Q136.794 654.341 136.794 663.09 Q136.794 671.817 133.715 676.424 Q130.66 681.007 124.849 681.007 Q119.039 681.007 115.961 676.424 Q112.905 671.817 112.905 663.09 Q112.905 654.341 115.961 649.757 Q119.039 645.151 124.849 645.151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M145.011 674.456 L149.896 674.456 L149.896 680.336 L145.011 680.336 L145.011 674.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M170.659 661.192 Q167.511 661.192 165.659 663.345 Q163.831 665.498 163.831 669.248 Q163.831 672.975 165.659 675.151 Q167.511 677.303 170.659 677.303 Q173.807 677.303 175.636 675.151 Q177.488 672.975 177.488 669.248 Q177.488 665.498 175.636 663.345 Q173.807 661.192 170.659 661.192 M179.942 646.54 L179.942 650.799 Q178.182 649.966 176.377 649.526 Q174.595 649.086 172.835 649.086 Q168.206 649.086 165.752 652.211 Q163.321 655.336 162.974 661.655 Q164.34 659.641 166.4 658.577 Q168.46 657.489 170.937 657.489 Q176.145 657.489 179.155 660.66 Q182.187 663.808 182.187 669.248 Q182.187 674.572 179.039 677.789 Q175.891 681.007 170.659 681.007 Q164.664 681.007 161.493 676.424 Q158.321 671.817 158.321 663.09 Q158.321 654.896 162.21 650.035 Q166.099 645.151 172.65 645.151 Q174.409 645.151 176.192 645.498 Q177.997 645.845 179.942 646.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M125.914 489.2 Q122.303 489.2 120.474 492.764 Q118.669 496.306 118.669 503.436 Q118.669 510.542 120.474 514.107 Q122.303 517.648 125.914 517.648 Q129.548 517.648 131.354 514.107 Q133.183 510.542 133.183 503.436 Q133.183 496.306 131.354 492.764 Q129.548 489.2 125.914 489.2 M125.914 485.496 Q131.724 485.496 134.78 490.102 Q137.859 494.686 137.859 503.436 Q137.859 512.162 134.78 516.769 Q131.724 521.352 125.914 521.352 Q120.104 521.352 117.025 516.769 Q113.97 512.162 113.97 503.436 Q113.97 494.686 117.025 490.102 Q120.104 485.496 125.914 485.496 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M146.076 514.801 L150.96 514.801 L150.96 520.681 L146.076 520.681 L146.076 514.801 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M159.965 486.121 L182.187 486.121 L182.187 488.112 L169.641 520.681 L164.757 520.681 L176.562 490.056 L159.965 490.056 L159.965 486.121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M125.104 329.545 Q121.493 329.545 119.664 333.109 Q117.859 336.651 117.859 343.781 Q117.859 350.887 119.664 354.452 Q121.493 357.994 125.104 357.994 Q128.738 357.994 130.544 354.452 Q132.373 350.887 132.373 343.781 Q132.373 336.651 130.544 333.109 Q128.738 329.545 125.104 329.545 M125.104 325.841 Q130.914 325.841 133.97 330.447 Q137.048 335.031 137.048 343.781 Q137.048 352.507 133.97 357.114 Q130.914 361.697 125.104 361.697 Q119.294 361.697 116.215 357.114 Q113.16 352.507 113.16 343.781 Q113.16 335.031 116.215 330.447 Q119.294 325.841 125.104 325.841 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M145.266 355.146 L150.15 355.146 L150.15 361.026 L145.266 361.026 L145.266 355.146 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M170.335 344.614 Q167.002 344.614 165.081 346.396 Q163.183 348.179 163.183 351.304 Q163.183 354.429 165.081 356.211 Q167.002 357.994 170.335 357.994 Q173.669 357.994 175.59 356.211 Q177.511 354.406 177.511 351.304 Q177.511 348.179 175.59 346.396 Q173.692 344.614 170.335 344.614 M165.659 342.623 Q162.65 341.883 160.96 339.822 Q159.294 337.762 159.294 334.799 Q159.294 330.656 162.233 328.248 Q165.196 325.841 170.335 325.841 Q175.497 325.841 178.437 328.248 Q181.377 330.656 181.377 334.799 Q181.377 337.762 179.687 339.822 Q178.02 341.883 175.034 342.623 Q178.414 343.41 180.289 345.702 Q182.187 347.994 182.187 351.304 Q182.187 356.327 179.108 359.012 Q176.053 361.697 170.335 361.697 Q164.618 361.697 161.539 359.012 Q158.484 356.327 158.484 351.304 Q158.484 347.994 160.382 345.702 Q162.28 343.41 165.659 342.623 M163.946 335.239 Q163.946 337.924 165.613 339.429 Q167.303 340.933 170.335 340.933 Q173.345 340.933 175.034 339.429 Q176.747 337.924 176.747 335.239 Q176.747 332.554 175.034 331.049 Q173.345 329.545 170.335 329.545 Q167.303 329.545 165.613 331.049 Q163.946 332.554 163.946 335.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M125.197 169.89 Q121.586 169.89 119.757 173.455 Q117.951 176.996 117.951 184.126 Q117.951 191.232 119.757 194.797 Q121.586 198.339 125.197 198.339 Q128.831 198.339 130.636 194.797 Q132.465 191.232 132.465 184.126 Q132.465 176.996 130.636 173.455 Q128.831 169.89 125.197 169.89 M125.197 166.186 Q131.007 166.186 134.062 170.793 Q137.141 175.376 137.141 184.126 Q137.141 192.853 134.062 197.459 Q131.007 202.042 125.197 202.042 Q119.386 202.042 116.308 197.459 Q113.252 192.853 113.252 184.126 Q113.252 175.376 116.308 170.793 Q119.386 166.186 125.197 166.186 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M145.359 195.491 L150.243 195.491 L150.243 201.371 L145.359 201.371 L145.359 195.491 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M160.567 200.654 L160.567 196.394 Q162.326 197.228 164.132 197.667 Q165.937 198.107 167.673 198.107 Q172.303 198.107 174.733 195.005 Q177.187 191.88 177.534 185.538 Q176.192 187.529 174.132 188.593 Q172.071 189.658 169.571 189.658 Q164.386 189.658 161.354 186.533 Q158.345 183.385 158.345 177.945 Q158.345 172.621 161.493 169.404 Q164.641 166.186 169.872 166.186 Q175.868 166.186 179.016 170.793 Q182.187 175.376 182.187 184.126 Q182.187 192.297 178.298 197.181 Q174.432 202.042 167.882 202.042 Q166.122 202.042 164.317 201.695 Q162.511 201.348 160.567 200.654 M169.872 186.001 Q173.02 186.001 174.849 183.848 Q176.701 181.695 176.701 177.945 Q176.701 174.218 174.849 172.066 Q173.02 169.89 169.872 169.89 Q166.724 169.89 164.872 172.066 Q163.044 174.218 163.044 177.945 Q163.044 181.695 164.872 183.848 Q166.724 186.001 169.872 186.001 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,196.565 325.239,377.677 357.991,494.453 390.744,605.214 423.496,706.185 456.249,737.296 489.001,749.871 521.754,797.079 554.507,789.97 \n",
       "  587.259,823.967 652.764,892.758 718.269,885.477 783.775,883.678 849.28,905.42 947.537,933.853 1045.8,952.499 1176.81,999.048 1307.82,940.296 1471.58,919.901 \n",
       "  1668.09,1005.96 1864.61,913.396 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ],
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip290\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip291\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip292\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,982.365 1912.76,982.365 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,822.711 1912.76,822.711 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,663.056 1912.76,663.056 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,503.401 1912.76,503.401 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,343.746 1912.76,343.746 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,184.091 1912.76,184.091 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,982.365 230.485,982.365 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,822.711 230.485,822.711 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,663.056 230.485,663.056 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,503.401 230.485,503.401 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,343.746 230.485,343.746 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,184.091 230.485,184.091 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M124.525 968.164 Q120.914 968.164 119.086 971.729 Q117.28 975.271 117.28 982.4 Q117.28 989.507 119.086 993.071 Q120.914 996.613 124.525 996.613 Q128.16 996.613 129.965 993.071 Q131.794 989.507 131.794 982.4 Q131.794 975.271 129.965 971.729 Q128.16 968.164 124.525 968.164 M124.525 964.46 Q130.336 964.46 133.391 969.067 Q136.47 973.65 136.47 982.4 Q136.47 991.127 133.391 995.733 Q130.336 1000.32 124.525 1000.32 Q118.715 1000.32 115.637 995.733 Q112.581 991.127 112.581 982.4 Q112.581 973.65 115.637 969.067 Q118.715 964.46 124.525 964.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M144.687 993.766 L149.572 993.766 L149.572 999.645 L144.687 999.645 L144.687 993.766 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M172.604 969.16 L160.798 987.609 L172.604 987.609 L172.604 969.16 M171.377 965.085 L177.257 965.085 L177.257 987.609 L182.187 987.609 L182.187 991.497 L177.257 991.497 L177.257 999.645 L172.604 999.645 L172.604 991.497 L157.002 991.497 L157.002 986.984 L171.377 965.085 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M126.007 808.509 Q122.396 808.509 120.567 812.074 Q118.761 815.616 118.761 822.745 Q118.761 829.852 120.567 833.417 Q122.396 836.958 126.007 836.958 Q129.641 836.958 131.447 833.417 Q133.275 829.852 133.275 822.745 Q133.275 815.616 131.447 812.074 Q129.641 808.509 126.007 808.509 M126.007 804.806 Q131.817 804.806 134.873 809.412 Q137.951 813.995 137.951 822.745 Q137.951 831.472 134.873 836.079 Q131.817 840.662 126.007 840.662 Q120.197 840.662 117.118 836.079 Q114.062 831.472 114.062 822.745 Q114.062 813.995 117.118 809.412 Q120.197 804.806 126.007 804.806 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M146.169 834.111 L151.053 834.111 L151.053 839.991 L146.169 839.991 L146.169 834.111 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M161.284 805.431 L179.641 805.431 L179.641 809.366 L165.567 809.366 L165.567 817.838 Q166.585 817.491 167.604 817.329 Q168.622 817.144 169.641 817.144 Q175.428 817.144 178.807 820.315 Q182.187 823.486 182.187 828.903 Q182.187 834.481 178.715 837.583 Q175.243 840.662 168.923 840.662 Q166.747 840.662 164.479 840.292 Q162.233 839.921 159.826 839.18 L159.826 834.481 Q161.909 835.616 164.132 836.171 Q166.354 836.727 168.831 836.727 Q172.835 836.727 175.173 834.62 Q177.511 832.514 177.511 828.903 Q177.511 825.292 175.173 823.185 Q172.835 821.079 168.831 821.079 Q166.956 821.079 165.081 821.495 Q163.229 821.912 161.284 822.792 L161.284 805.431 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M124.849 648.854 Q121.238 648.854 119.41 652.419 Q117.604 655.961 117.604 663.09 Q117.604 670.197 119.41 673.762 Q121.238 677.303 124.849 677.303 Q128.484 677.303 130.289 673.762 Q132.118 670.197 132.118 663.09 Q132.118 655.961 130.289 652.419 Q128.484 648.854 124.849 648.854 M124.849 645.151 Q130.66 645.151 133.715 649.757 Q136.794 654.341 136.794 663.09 Q136.794 671.817 133.715 676.424 Q130.66 681.007 124.849 681.007 Q119.039 681.007 115.961 676.424 Q112.905 671.817 112.905 663.09 Q112.905 654.341 115.961 649.757 Q119.039 645.151 124.849 645.151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M145.011 674.456 L149.896 674.456 L149.896 680.336 L145.011 680.336 L145.011 674.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M170.659 661.192 Q167.511 661.192 165.659 663.345 Q163.831 665.498 163.831 669.248 Q163.831 672.975 165.659 675.151 Q167.511 677.303 170.659 677.303 Q173.807 677.303 175.636 675.151 Q177.488 672.975 177.488 669.248 Q177.488 665.498 175.636 663.345 Q173.807 661.192 170.659 661.192 M179.942 646.54 L179.942 650.799 Q178.182 649.966 176.377 649.526 Q174.595 649.086 172.835 649.086 Q168.206 649.086 165.752 652.211 Q163.321 655.336 162.974 661.655 Q164.34 659.641 166.4 658.577 Q168.46 657.489 170.937 657.489 Q176.145 657.489 179.155 660.66 Q182.187 663.808 182.187 669.248 Q182.187 674.572 179.039 677.789 Q175.891 681.007 170.659 681.007 Q164.664 681.007 161.493 676.424 Q158.321 671.817 158.321 663.09 Q158.321 654.896 162.21 650.035 Q166.099 645.151 172.65 645.151 Q174.409 645.151 176.192 645.498 Q177.997 645.845 179.942 646.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M125.914 489.2 Q122.303 489.2 120.474 492.764 Q118.669 496.306 118.669 503.436 Q118.669 510.542 120.474 514.107 Q122.303 517.648 125.914 517.648 Q129.548 517.648 131.354 514.107 Q133.183 510.542 133.183 503.436 Q133.183 496.306 131.354 492.764 Q129.548 489.2 125.914 489.2 M125.914 485.496 Q131.724 485.496 134.78 490.102 Q137.859 494.686 137.859 503.436 Q137.859 512.162 134.78 516.769 Q131.724 521.352 125.914 521.352 Q120.104 521.352 117.025 516.769 Q113.97 512.162 113.97 503.436 Q113.97 494.686 117.025 490.102 Q120.104 485.496 125.914 485.496 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M146.076 514.801 L150.96 514.801 L150.96 520.681 L146.076 520.681 L146.076 514.801 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M159.965 486.121 L182.187 486.121 L182.187 488.112 L169.641 520.681 L164.757 520.681 L176.562 490.056 L159.965 490.056 L159.965 486.121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M125.104 329.545 Q121.493 329.545 119.664 333.109 Q117.859 336.651 117.859 343.781 Q117.859 350.887 119.664 354.452 Q121.493 357.994 125.104 357.994 Q128.738 357.994 130.544 354.452 Q132.373 350.887 132.373 343.781 Q132.373 336.651 130.544 333.109 Q128.738 329.545 125.104 329.545 M125.104 325.841 Q130.914 325.841 133.97 330.447 Q137.048 335.031 137.048 343.781 Q137.048 352.507 133.97 357.114 Q130.914 361.697 125.104 361.697 Q119.294 361.697 116.215 357.114 Q113.16 352.507 113.16 343.781 Q113.16 335.031 116.215 330.447 Q119.294 325.841 125.104 325.841 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M145.266 355.146 L150.15 355.146 L150.15 361.026 L145.266 361.026 L145.266 355.146 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M170.335 344.614 Q167.002 344.614 165.081 346.396 Q163.183 348.179 163.183 351.304 Q163.183 354.429 165.081 356.211 Q167.002 357.994 170.335 357.994 Q173.669 357.994 175.59 356.211 Q177.511 354.406 177.511 351.304 Q177.511 348.179 175.59 346.396 Q173.692 344.614 170.335 344.614 M165.659 342.623 Q162.65 341.883 160.96 339.822 Q159.294 337.762 159.294 334.799 Q159.294 330.656 162.233 328.248 Q165.196 325.841 170.335 325.841 Q175.497 325.841 178.437 328.248 Q181.377 330.656 181.377 334.799 Q181.377 337.762 179.687 339.822 Q178.02 341.883 175.034 342.623 Q178.414 343.41 180.289 345.702 Q182.187 347.994 182.187 351.304 Q182.187 356.327 179.108 359.012 Q176.053 361.697 170.335 361.697 Q164.618 361.697 161.539 359.012 Q158.484 356.327 158.484 351.304 Q158.484 347.994 160.382 345.702 Q162.28 343.41 165.659 342.623 M163.946 335.239 Q163.946 337.924 165.613 339.429 Q167.303 340.933 170.335 340.933 Q173.345 340.933 175.034 339.429 Q176.747 337.924 176.747 335.239 Q176.747 332.554 175.034 331.049 Q173.345 329.545 170.335 329.545 Q167.303 329.545 165.613 331.049 Q163.946 332.554 163.946 335.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M125.197 169.89 Q121.586 169.89 119.757 173.455 Q117.951 176.996 117.951 184.126 Q117.951 191.232 119.757 194.797 Q121.586 198.339 125.197 198.339 Q128.831 198.339 130.636 194.797 Q132.465 191.232 132.465 184.126 Q132.465 176.996 130.636 173.455 Q128.831 169.89 125.197 169.89 M125.197 166.186 Q131.007 166.186 134.062 170.793 Q137.141 175.376 137.141 184.126 Q137.141 192.853 134.062 197.459 Q131.007 202.042 125.197 202.042 Q119.386 202.042 116.308 197.459 Q113.252 192.853 113.252 184.126 Q113.252 175.376 116.308 170.793 Q119.386 166.186 125.197 166.186 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M145.359 195.491 L150.243 195.491 L150.243 201.371 L145.359 201.371 L145.359 195.491 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M160.567 200.654 L160.567 196.394 Q162.326 197.228 164.132 197.667 Q165.937 198.107 167.673 198.107 Q172.303 198.107 174.733 195.005 Q177.187 191.88 177.534 185.538 Q176.192 187.529 174.132 188.593 Q172.071 189.658 169.571 189.658 Q164.386 189.658 161.354 186.533 Q158.345 183.385 158.345 177.945 Q158.345 172.621 161.493 169.404 Q164.641 166.186 169.872 166.186 Q175.868 166.186 179.016 170.793 Q182.187 175.376 182.187 184.126 Q182.187 192.297 178.298 197.181 Q174.432 202.042 167.882 202.042 Q166.122 202.042 164.317 201.695 Q162.511 201.348 160.567 200.654 M169.872 186.001 Q173.02 186.001 174.849 183.848 Q176.701 181.695 176.701 177.945 Q176.701 174.218 174.849 172.066 Q173.02 169.89 169.872 169.89 Q166.724 169.89 164.872 172.066 Q163.044 174.218 163.044 177.945 Q163.044 181.695 164.872 183.848 Q166.724 186.001 169.872 186.001 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip292)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,196.565 325.239,377.677 357.991,494.453 390.744,605.214 423.496,706.185 456.249,737.296 489.001,749.871 521.754,797.079 554.507,789.97 \n",
       "  587.259,823.967 652.764,892.758 718.269,885.477 783.775,883.678 849.28,905.42 947.537,933.853 1045.8,952.499 1176.81,999.048 1307.82,940.296 1471.58,919.901 \n",
       "  1668.09,1005.96 1864.61,913.396 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)\n",
    "\n",
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"huge\" (3/3)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10-element Vector{Int64}:\n 2\n 2\n 1\n 1\n 0\n 3\n 7\n 1\n 6\n 5"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬────────────┬────────────┬──────────────────────────────────┐\n",
      "│ a     │ b          │ c          │ d                                │\n",
      "│ Int64 │ Float64    │ Float64    │ CategoricalValue{String, UInt32} │\n",
      "│ Count │ Continuous │ Continuous │ OrderedFactor{2}                 │\n",
      "├───────┼────────────┼────────────┼──────────────────────────────────┤\n",
      "│ 1     │ 0.480318   │ 0.669866   │ male                             │\n",
      "│ 2     │ 0.83416    │ 0.591969   │ female                           │\n",
      "│ 3     │ 0.669467   │ 0.858711   │ female                           │\n",
      "│ 4     │ 0.185419   │ 0.361825   │ male                             │\n",
      "└───────┴────────────┴────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous,\n",
    "                 name -> true);"
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Int64}:\n 1\n 2\n 3\n 4"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┐\n",
      "│ b          │ c          │\n",
      "│ Float64    │ Float64    │\n",
      "│ Continuous │ Continuous │\n",
      "├────────────┼────────────┤\n",
      "│ 0.480318   │ 0.669866   │\n",
      "│ 0.83416    │ 0.591969   │\n",
      "│ 0.669467   │ 0.858711   │\n",
      "│ 0.185419   │ 0.361825   │\n",
      "└────────────┴────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": 54
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"male\"\n \"female\"\n \"female\"\n \"male\""
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬─────────────────────────────────┬──────────────────┐\n│\u001b[22m _.names                 \u001b[0m│\u001b[22m _.types                         \u001b[0m│\u001b[22m _.scitypes       \u001b[0m│\n├─────────────────────────┼─────────────────────────────────┼──────────────────┤\n│ surgery                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n│ age                     │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n│ rectal_temperature      │ Float64                         │ Continuous       │\n│ pulse                   │ Float64                         │ Continuous       │\n│ respiratory_rate        │ Float64                         │ Continuous       │\n│ temperature_extremities │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ mucous_membranes        │ CategoricalValue{Int64, UInt32} │ Multiclass{6}    │\n│ capillary_refill_time   │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    │\n│ pain                    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{5} │\n│ peristalsis             │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ abdominal_distension    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ packed_cell_volume      │ Float64                         │ Continuous       │\n│ total_protein           │ Float64                         │ Continuous       │\n│ outcome                 │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    │\n│ surgical_lesion         │ CategoricalValue{Int64, UInt32} │ OrderedFactor{2} │\n│ cp_data                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n└─────────────────────────┴─────────────────────────────────┴──────────────────┘\n_.nrows = 366\n"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
