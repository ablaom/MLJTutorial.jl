{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "import DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These look fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We can randomize the data at\n",
    "the same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This puts the `:class` column into a vector `y`, and all remaining\n",
    "columns into a table `X`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you already have an idea about the name of the model, you could\n",
    "search by string or regex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "some_models = models(\"LinearRegressor\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not\n",
    "yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "meta = some_models[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(1:length(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: - In MLJ, a model that can predict probabilities (and\n",
    "not just point values) will do so by default.  - For most\n",
    "probabilistic predictors, the predicted object is a\n",
    "`Distributions.Distribution` object or a\n",
    "`CategoricalDistributions.UnivariateFinite` object (the case here)\n",
    "which all support the following methods: `rand`, `pdf`, `logpdf`;\n",
    "and, where appropriate: `mode`, `median` and `mean`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
