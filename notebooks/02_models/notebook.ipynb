{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia, JuliaCon2020"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A workshop introducing the machine learning toolbox\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.6.3\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env/Project.toml`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [List of methods introduced in this tutorial](methods.md)\n",
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n  Marshall \u001b[1mPlease cite\u001b[22m:\n\n  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n  the pattern recognition literature. Fisher's paper is a classic in the field\n  and is referenced frequently to this day. (See Duda & Hart, for example.)\n  The data set contains 3 classes of 50 instances each, where each class\n  refers to a type of iris plant. One class is linearly separable from the\n  other 2; the latter are NOT linearly separable from each other.\n\n  Predicted attribute: class of iris plant. This is an exceedingly simple\n  domain.\n\n\u001b[1m  Attribute Information:\u001b[22m\n\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n\n\u001b[36m  1. sepal length in cm\u001b[39m\n\u001b[36m  2. sepal width in cm\u001b[39m\n\u001b[36m  3. petal length in cm\u001b[39m\n\u001b[36m  4. petal width in cm\u001b[39m\n\u001b[36m  5. class: \u001b[39m\n\u001b[36m     -- Iris Setosa\u001b[39m\n\u001b[36m     -- Iris Versicolour\u001b[39m\n\u001b[36m     -- Iris Virginica\u001b[39m",
      "text/markdown": "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n\n**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n\n### Attribute Information:\n\n```\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n   -- Iris Setosa\n   -- Iris Versicolour\n   -- Iris Virginica\n```\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n─────┼───────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n   4 │         4.6         3.1          1.5         0.2  Iris-setosa",
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "using DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬──────────────────────────────────┬───────────────┐\n│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n├─────────────┼──────────────────────────────────┼───────────────┤\n│ sepallength │ Float64                          │ Continuous    │\n│ sepalwidth  │ Float64                          │ Continuous    │\n│ petallength │ Float64                          │ Continuous    │\n│ petalwidth  │ Float64                          │ Continuous    │\n│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n└─────────────┴──────────────────────────────────┴───────────────┘\n_.nrows = 150\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬──────────────────────────────────┬───────────────┐\n│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n├─────────────┼──────────────────────────────────┼───────────────┤\n│ sepallength │ Float64                          │ Continuous    │\n│ sepalwidth  │ Float64                          │ Continuous    │\n│ petallength │ Float64                          │ Continuous    │\n│ petalwidth  │ Float64                          │ Continuous    │\n│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n└─────────────┴──────────────────────────────────┴───────────────┘\n_.nrows = 150\n"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "coerce!(iris,\n",
    "        Union{Missing,Continuous}=>Continuous,\n",
    "        Union{Missing,Multiclass}=>Multiclass)\n",
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We randomize the data at the\n",
    "same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{ScientificTypesBase.Multiclass{3}} (alias for AbstractArray{ScientificTypesBase.Multiclass{3}, 1})"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), name->true; rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[36m  t1, t2, ...., tk = unpack(table, f1, f2, ... fk;\u001b[39m\n\u001b[36m                           wrap_singles=false,\u001b[39m\n\u001b[36m                           shuffle=false,\u001b[39m\n\u001b[36m                           rng::Union{AbstractRNG,Int,Nothing}=nothing)\u001b[39m\n\n  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables (or\n  vectors) \u001b[36mt1, t2, ..., tk\u001b[39m by making column selections \u001b[1mwithout replacement\u001b[22m by\n  successively applying the columnn name filters \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m, ..., \u001b[36mfk\u001b[39m. A \u001b[4mfilter\u001b[24m is\n  any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column \u001b[36mname::Symbol\u001b[39m\n  of \u001b[36mtable\u001b[39m. For example, use the filter \u001b[36m_ -> true\u001b[39m to pick up all remaining\n  columns of the table.\n\n  Whenever a returned table contains a single column, it is converted to a\n  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n\n  Scientific type conversions can be optionally specified (note semicolon):\n\n\u001b[36m  unpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\u001b[39m\n\n  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n  \u001b[36mshuffle=true\u001b[39m is implicit.\n\n\u001b[1m  Example\u001b[22m\n\u001b[1m  –––––––––\u001b[22m\n\n\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n\u001b[36m  julia> Z, XY = unpack(table, ==(:z), !=(:w);\u001b[39m\n\u001b[36m                 :x=>Continuous, :y=>Multiclass)\u001b[39m\n\u001b[36m  julia> XY\u001b[39m\n\u001b[36m  2×2 DataFrame\u001b[39m\n\u001b[36m  │ Row │ x       │ y            │\u001b[39m\n\u001b[36m  │     │ Float64 │ Categorical… │\u001b[39m\n\u001b[36m  ├─────┼─────────┼──────────────┤\u001b[39m\n\u001b[36m  │ 1   │ 1.0     │ 'a'          │\u001b[39m\n\u001b[36m  │ 2   │ 2.0     │ 'b'          │\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> Z\u001b[39m\n\u001b[36m  2-element Array{Float64,1}:\u001b[39m\n\u001b[36m   10.0\u001b[39m\n\u001b[36m   20.0\u001b[39m",
      "text/markdown": "```\nt1, t2, ...., tk = unpack(table, f1, f2, ... fk;\n                         wrap_singles=false,\n                         shuffle=false,\n                         rng::Union{AbstractRNG,Int,Nothing}=nothing)\n```\n\nHorizontally split any Tables.jl compatible `table` into smaller tables (or vectors) `t1, t2, ..., tk` by making column selections **without replacement** by successively applying the columnn name filters `f1`, `f2`, ..., `fk`. A *filter* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`. For example, use the filter `_ -> true` to pick up all remaining columns of the table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n\nScientific type conversions can be optionally specified (note semicolon):\n\n```\nunpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\n```\n\nIf `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n\n### Example\n\n```\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n```\n"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "183-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = ARDRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n ⋮\n (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )\n (name = XGBoostCount, package_name = XGBoost, ... )\n (name = XGBoostRegressor, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mAEDetector from OutlierDetectionNetworks.jl.\u001b[39m\n\u001b[35m[Documentation](https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl).\u001b[39m\n(name = \"AEDetector\",\n package_name = \"OutlierDetectionNetworks\",\n is_supervised = false,\n abstract_type = MLJModelInterface.UnsupervisedDetector,\n deep_properties = (),\n docstring = \"AEDetector from OutlierDetectionNetworks.jl.\\n[Documentation](https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl).\",\n fit_data_scitype = Tuple{Union{ScientificTypesBase.Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:ScientificTypesBase.Continuous), AbstractMatrix{_s689} where _s689<:ScientificTypesBase.Continuous}},\n hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n hyperparameter_types = (\"Flux.Chain\", \"Flux.Chain\", \"Integer\", \"Integer\", \"Bool\", \"Bool\", \"Any\", \"Function\"),\n hyperparameters = (:encoder, :decoder, :batchsize, :epochs, :shuffle, :partial, :opt, :loss),\n implemented_methods = [:clean!, :fit, :transform],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = nothing,\n load_path = \"OutlierDetectionNetworks.AEDetector\",\n package_license = \"MIT\",\n package_url = \"https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl\",\n package_uuid = \"c7f57e37-4fcb-4a0b-a36c-c2204bc839a7\",\n predict_scitype = ScientificTypesBase.Unknown,\n prediction_type = :unknown,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = false,\n supports_weights = false,\n transform_scitype = AbstractVector{_s689} where _s689<:ScientificTypesBase.Continuous,\n input_scitype = Union{ScientificTypesBase.Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:ScientificTypesBase.Continuous), AbstractMatrix{_s689} where _s689<:ScientificTypesBase.Continuous},\n target_scitype = AbstractVector{_s689} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}},\n output_scitype = AbstractVector{_s689} where _s689<:ScientificTypesBase.Continuous,)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "meta = all_models[3]"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{_s689} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}} (alias for AbstractArray{_s689, 1} where _s689<:Union{Missing, ScientificTypesBase.OrderedFactor{2}})"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "false"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's  find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n ⋮\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all models with \"Classifier\" in `name` (or `docstring`):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "45-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )\n (name = ComplementNBClassifier, package_name = ScikitLearn, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = DummyClassifier, package_name = ScikitLearn, ... )\n ⋮\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "models(\"Classifier\")"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n ⋮\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n    builder = Short(\n            n_hidden = 0,\n            dropout = 0.5,\n            σ = NNlib.σ),\n    finaliser = NNlib.softmax,\n    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n    loss = Flux.Losses.crossentropy,\n    epochs = 10,\n    batch_size = 1,\n    lambda = 0.0,\n    alpha = 0.0,\n    rng = Random._GLOBAL_RNG(),\n    optimiser_changes_trigger_retraining = false,\n    acceleration = ComputationalResources.CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n(name = \"NeuralNetworkClassifier\",\n package_name = \"MLJFlux\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (:optimiser, :builder),\n docstring = \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n fit_data_scitype = Tuple{ScientificTypesBase.Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:ScientificTypesBase.Continuous), AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite},\n hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n hyperparameter_types = (\"MLJFlux.Short\", \"typeof(NNlib.softmax)\", \"Flux.Optimise.ADAM\", \"typeof(Flux.Losses.crossentropy)\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\"),\n hyperparameters = (:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration),\n implemented_methods = Any[],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = :epochs,\n load_path = \"MLJFlux.NeuralNetworkClassifier\",\n package_license = \"MIT\",\n package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n predict_scitype = AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = true,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype = ScientificTypesBase.Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:ScientificTypesBase.Continuous),\n target_scitype = AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite,\n output_scitype = ScientificTypesBase.Unknown,)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n  args: \n    1:\tSource @415 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @576 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.645\n",
      "[ Info: Loss is 1.363\n",
      "[ Info: Loss is 1.232\n",
      "[ Info: Loss is 1.158\n",
      "[ Info: Loss is 1.076\n",
      "[ Info: Loss is 1.058\n",
      "[ Info: Loss is 1.028\n",
      "[ Info: Loss is 0.9847\n",
      "[ Info: Loss is 0.9805\n",
      "[ Info: Loss is 0.9549\n",
      "[ Info: Loss is 0.9395\n",
      "[ Info: Loss is 0.9979\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n  args: \n    1:\tSource @415 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @576 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.323, Iris-versicolor=>0.35, Iris-virginica=>0.327)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.293, Iris-versicolor=>0.36, Iris-virginica=>0.346)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.534, Iris-versicolor=>0.26, Iris-virginica=>0.206)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(training_losses = [1.5933675289883662, 1.6446698405083613, 1.3626403027529719, 1.2323719793765826, 1.158333420417294, 1.0764707564716018, 1.0582327914384302, 1.028055297181189, 0.9847124977292665, 0.9804833169795328, 0.9549258446985405, 0.9395107447503084, 0.9979022014209239],)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.285, Iris-versicolor=>0.364, Iris-virginica=>0.352)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.322, Iris-versicolor=>0.35, Iris-virginica=>0.327)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.293, Iris-versicolor=>0.359, Iris-virginica=>0.348)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net: 15%[===>                     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 38%[=========>               ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 46%[===========>             ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 54%[=============>           ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 62%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 85%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @233 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @290 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9778\n",
      "[ Info: Loss is 0.9875\n",
      "[ Info: Loss is 0.9413\n",
      "[ Info: Loss is 0.9795\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @415 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @576 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9222\n",
      "[ Info: Loss is 0.7776\n",
      "[ Info: Loss is 0.812\n",
      "[ Info: Loss is 0.6807\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n  args: \n    1:\tSource @415 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @576 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.113\n",
      "[ Info: Loss is 1.008\n",
      "[ Info: Loss is 0.9232\n",
      "[ Info: Loss is 0.863\n",
      "[ Info: Loss is 0.888\n",
      "[ Info: Loss is 0.7969\n",
      "[ Info: Loss is 0.8619\n",
      "[ Info: Loss is 0.7553\n",
      "[ Info: Loss is 0.7372\n",
      "[ Info: Loss is 0.7965\n",
      "[ Info: Loss is 0.7005\n",
      "[ Info: Loss is 0.7514\n",
      "[ Info: Loss is 0.6858\n",
      "[ Info: Loss is 0.7612\n",
      "[ Info: Loss is 0.7729\n",
      "[ Info: Loss is 0.7872\n",
      "[ Info: Loss is 0.7477\n",
      "[ Info: Loss is 0.7094\n",
      "[ Info: Loss is 0.7275\n",
      "[ Info: Loss is 0.6967\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n  args: \n    1:\tSource @415 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @576 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 13%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 19%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 26%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 32%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 42%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 48%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 52%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 58%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 68%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 74%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 81%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 87%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 97%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.0724, Iris-versicolor=>0.537, Iris-virginica=>0.39)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":probabilistic"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**:\n",
    "- In MLJ, a model that can predict probabilities (and not just point values) will do so by default.\n",
    "- For most probabilistic predictors, the predicted object is a `Distributions.Distribution` object, supporting the `Distributions.jl` [API](https://juliastats.org/Distributions.jl/latest/extends/#Create-a-Distribution-1) for such objects. In particular, the methods `rand`,  `pdf`, `logpdf`, `mode`, `median` and `mean` will apply, where appropriate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.39037621722728894"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-versicolor\""
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Float64}:\n 0.5372494122460736\n 0.43996401604081015\n 0.15966703802394752\n 0.27185945322514554"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": 37
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-virginica\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-virginica\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4×3 Matrix{Float64}:\n 0.0723744   0.537249  0.390376\n 0.0160674   0.439964  0.543969\n 0.78891     0.159667  0.0514231\n 0.00135705  0.271859  0.726783"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3988116512093229"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.08888888888888889"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "61-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type), T} where T<:Tuple}:\n (name = BrierLoss, instances = [brier_loss], ...)\n (name = BrierScore, instances = [brier_score], ...)\n (name = LPLoss, instances = [l1, l2], ...)\n (name = LogCoshLoss, instances = [log_cosh, log_cosh_loss], ...)\n (name = LogLoss, instances = [log_loss, cross_entropy], ...)\n (name = LogScore, instances = [log_score], ...)\n (name = SphericalScore, instances = [spherical_score], ...)\n (name = Accuracy, instances = [accuracy], ...)\n (name = AreaUnderCurve, instances = [area_under_curve, auc], ...)\n (name = BalancedAccuracy, instances = [balanced_accuracy, bacc, bac], ...)\n ⋮\n (name = SmoothedL1HingeLoss, instances = [smoothed_l1_hinge_loss], ...)\n (name = ZeroOneLoss, instances = [zero_one_loss], ...)\n (name = HuberLoss, instances = [huber_loss], ...)\n (name = L1EpsilonInsLoss, instances = [l1_epsilon_ins_loss], ...)\n (name = L2EpsilonInsLoss, instances = [l2_epsilon_ins_loss], ...)\n (name = LPDistLoss, instances = [lp_dist_loss], ...)\n (name = LogitDistLoss, instances = [logit_dist_loss], ...)\n (name = PeriodicLoss, instances = [periodic_loss], ...)\n (name = QuantileLoss, instances = [quantile_loss], ...)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬──────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold \u001b[0m│\n├────────────────────────────┼─────────────┼───────────┼──────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.399       │ predict   │ [0.399]  │\n│ BrierScore()               │ -0.215      │ predict   │ [-0.215] │\n└────────────────────────────┴─────────────┴───────────┴──────────┘\n"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:13\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:09\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:06\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:17\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n├────────────────────────────┼─────────────┼───────────┼────────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.407       │ predict   │ [0.434, 0.341, 0.428, ⋯\n│ BrierScore()               │ -0.229      │ predict   │ [-0.247, -0.171, -0.2 ⋯\n└────────────────────────────┴─────────────┴───────────┴────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 18 folds:  11%[==>                      ]  ETA: 0:00:47\u001b[K\rEvaluating over 18 folds:  17%[====>                    ]  ETA: 0:00:42\u001b[K\rEvaluating over 18 folds:  22%[=====>                   ]  ETA: 0:00:39\u001b[K\rEvaluating over 18 folds:  28%[======>                  ]  ETA: 0:00:36\u001b[K\rEvaluating over 18 folds:  33%[========>                ]  ETA: 0:00:34\u001b[K\rEvaluating over 18 folds:  39%[=========>               ]  ETA: 0:00:32\u001b[K\rEvaluating over 18 folds:  44%[===========>             ]  ETA: 0:00:30\u001b[K\rEvaluating over 18 folds:  50%[============>            ]  ETA: 0:00:27\u001b[K\rEvaluating over 18 folds:  56%[=============>           ]  ETA: 0:00:24\u001b[K\rEvaluating over 18 folds:  61%[===============>         ]  ETA: 0:00:21\u001b[K\rEvaluating over 18 folds:  67%[================>        ]  ETA: 0:00:18\u001b[K\rEvaluating over 18 folds:  72%[==================>      ]  ETA: 0:00:15\u001b[K\rEvaluating over 18 folds:  78%[===================>     ]  ETA: 0:00:12\u001b[K\rEvaluating over 18 folds:  83%[====================>    ]  ETA: 0:00:09\u001b[K\rEvaluating over 18 folds:  89%[======================>  ]  ETA: 0:00:06\u001b[K\rEvaluating over 18 folds:  94%[=======================> ]  ETA: 0:00:03\u001b[K\rEvaluating over 18 folds: 100%[=========================] Time: 0:00:52\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n├────────────────────────────┼─────────────┼───────────┼────────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.409       │ predict   │ [0.424, 0.349, 0.37,  ⋯\n│ BrierScore()               │ -0.226      │ predict   │ [-0.218, -0.187, -0.1 ⋯\n└────────────────────────────┴─────────────┴───────────┴────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, brier_score])"
   ],
   "metadata": {},
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Creating subsamples from a subset of all rows. \n",
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:08\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:06\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:04\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:11\u001b[K\n",
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  4%[>                        ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net:  8%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 12%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 14%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 16%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 18%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 20%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 22%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 24%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 25%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 27%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 33%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 37%[=========>               ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 41%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 43%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 47%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 49%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 51%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 53%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 57%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 59%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 63%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 67%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 73%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 75%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 76%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 78%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 80%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:02\u001b[K\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5) on log scale"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log)"
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "[ Info: Attempting to evaluate 22 models.\n",
      "\rEvaluating over 22 metamodels:   0%[>                        ]  ETA: N/A\u001b[K\rEvaluating over 22 metamodels:   5%[=>                       ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:   9%[==>                      ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  14%[===>                     ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  18%[====>                    ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  23%[=====>                   ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  27%[======>                  ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  32%[=======>                 ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  36%[=========>               ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  41%[==========>              ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  45%[===========>             ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  50%[============>            ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  55%[=============>           ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  59%[==============>          ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  64%[===============>         ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  68%[=================>       ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  73%[==================>      ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  77%[===================>     ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  82%[====================>    ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  86%[=====================>   ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  95%[=======================> ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels: 100%[=========================] Time: 0:00:02\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Plot{Plots.GRBackend() n=1}",
      "image/png": "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",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip720\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip721\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip722\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,959.437 1912.76,959.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,817.414 1912.76,817.414 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,675.391 1912.76,675.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,533.368 1912.76,533.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,391.345 1912.76,391.345 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,249.322 1912.76,249.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,107.299 1912.76,107.299 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,959.437 230.485,959.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,817.414 230.485,817.414 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,675.391 230.485,675.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,533.368 230.485,533.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,391.345 230.485,391.345 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,249.322 230.485,249.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,107.299 230.485,107.299 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M124.525 945.236 Q120.914 945.236 119.086 948.801 Q117.28 952.342 117.28 959.472 Q117.28 966.578 119.086 970.143 Q120.914 973.685 124.525 973.685 Q128.16 973.685 129.965 970.143 Q131.794 966.578 131.794 959.472 Q131.794 952.342 129.965 948.801 Q128.16 945.236 124.525 945.236 M124.525 941.532 Q130.336 941.532 133.391 946.138 Q136.47 950.722 136.47 959.472 Q136.47 968.199 133.391 972.805 Q130.336 977.388 124.525 977.388 Q118.715 977.388 115.637 972.805 Q112.581 968.199 112.581 959.472 Q112.581 950.722 115.637 946.138 Q118.715 941.532 124.525 941.532 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M144.687 970.837 L149.572 970.837 L149.572 976.717 L144.687 976.717 L144.687 970.837 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M172.604 946.231 L160.798 964.68 L172.604 964.68 L172.604 946.231 M171.377 942.157 L177.257 942.157 L177.257 964.68 L182.187 964.68 L182.187 968.569 L177.257 968.569 L177.257 976.717 L172.604 976.717 L172.604 968.569 L157.002 968.569 L157.002 964.055 L171.377 942.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M126.007 803.213 Q122.396 803.213 120.567 806.778 Q118.761 810.319 118.761 817.449 Q118.761 824.555 120.567 828.12 Q122.396 831.662 126.007 831.662 Q129.641 831.662 131.447 828.12 Q133.275 824.555 133.275 817.449 Q133.275 810.319 131.447 806.778 Q129.641 803.213 126.007 803.213 M126.007 799.509 Q131.817 799.509 134.873 804.116 Q137.951 808.699 137.951 817.449 Q137.951 826.176 134.873 830.782 Q131.817 835.365 126.007 835.365 Q120.197 835.365 117.118 830.782 Q114.062 826.176 114.062 817.449 Q114.062 808.699 117.118 804.116 Q120.197 799.509 126.007 799.509 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M146.169 828.814 L151.053 828.814 L151.053 834.694 L146.169 834.694 L146.169 828.814 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M161.284 800.134 L179.641 800.134 L179.641 804.069 L165.567 804.069 L165.567 812.541 Q166.585 812.194 167.604 812.032 Q168.622 811.847 169.641 811.847 Q175.428 811.847 178.807 815.018 Q182.187 818.19 182.187 823.606 Q182.187 829.185 178.715 832.287 Q175.243 835.365 168.923 835.365 Q166.747 835.365 164.479 834.995 Q162.233 834.625 159.826 833.884 L159.826 829.185 Q161.909 830.319 164.132 830.875 Q166.354 831.43 168.831 831.43 Q172.835 831.43 175.173 829.324 Q177.511 827.217 177.511 823.606 Q177.511 819.995 175.173 817.889 Q172.835 815.782 168.831 815.782 Q166.956 815.782 165.081 816.199 Q163.229 816.615 161.284 817.495 L161.284 800.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M124.849 661.19 Q121.238 661.19 119.41 664.755 Q117.604 668.296 117.604 675.426 Q117.604 682.532 119.41 686.097 Q121.238 689.639 124.849 689.639 Q128.484 689.639 130.289 686.097 Q132.118 682.532 132.118 675.426 Q132.118 668.296 130.289 664.755 Q128.484 661.19 124.849 661.19 M124.849 657.486 Q130.66 657.486 133.715 662.093 Q136.794 666.676 136.794 675.426 Q136.794 684.153 133.715 688.759 Q130.66 693.342 124.849 693.342 Q119.039 693.342 115.961 688.759 Q112.905 684.153 112.905 675.426 Q112.905 666.676 115.961 662.093 Q119.039 657.486 124.849 657.486 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M145.011 686.791 L149.896 686.791 L149.896 692.671 L145.011 692.671 L145.011 686.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M170.659 673.528 Q167.511 673.528 165.659 675.68 Q163.831 677.833 163.831 681.583 Q163.831 685.31 165.659 687.486 Q167.511 689.639 170.659 689.639 Q173.807 689.639 175.636 687.486 Q177.488 685.31 177.488 681.583 Q177.488 677.833 175.636 675.68 Q173.807 673.528 170.659 673.528 M179.942 658.875 L179.942 663.134 Q178.182 662.301 176.377 661.861 Q174.595 661.421 172.835 661.421 Q168.206 661.421 165.752 664.546 Q163.321 667.671 162.974 673.991 Q164.34 671.977 166.4 670.912 Q168.46 669.824 170.937 669.824 Q176.145 669.824 179.155 672.995 Q182.187 676.143 182.187 681.583 Q182.187 686.907 179.039 690.125 Q175.891 693.342 170.659 693.342 Q164.664 693.342 161.493 688.759 Q158.321 684.153 158.321 675.426 Q158.321 667.231 162.21 662.37 Q166.099 657.486 172.65 657.486 Q174.409 657.486 176.192 657.833 Q177.997 658.181 179.942 658.875 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M125.914 519.167 Q122.303 519.167 120.474 522.732 Q118.669 526.273 118.669 533.403 Q118.669 540.509 120.474 544.074 Q122.303 547.616 125.914 547.616 Q129.548 547.616 131.354 544.074 Q133.183 540.509 133.183 533.403 Q133.183 526.273 131.354 522.732 Q129.548 519.167 125.914 519.167 M125.914 515.463 Q131.724 515.463 134.78 520.07 Q137.859 524.653 137.859 533.403 Q137.859 542.13 134.78 546.736 Q131.724 551.319 125.914 551.319 Q120.104 551.319 117.025 546.736 Q113.97 542.13 113.97 533.403 Q113.97 524.653 117.025 520.07 Q120.104 515.463 125.914 515.463 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M146.076 544.769 L150.96 544.769 L150.96 550.648 L146.076 550.648 L146.076 544.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M159.965 516.088 L182.187 516.088 L182.187 518.079 L169.641 550.648 L164.757 550.648 L176.562 520.023 L159.965 520.023 L159.965 516.088 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M125.104 377.144 Q121.493 377.144 119.664 380.709 Q117.859 384.25 117.859 391.38 Q117.859 398.486 119.664 402.051 Q121.493 405.593 125.104 405.593 Q128.738 405.593 130.544 402.051 Q132.373 398.486 132.373 391.38 Q132.373 384.25 130.544 380.709 Q128.738 377.144 125.104 377.144 M125.104 373.44 Q130.914 373.44 133.97 378.047 Q137.048 382.63 137.048 391.38 Q137.048 400.107 133.97 404.713 Q130.914 409.296 125.104 409.296 Q119.294 409.296 116.215 404.713 Q113.16 400.107 113.16 391.38 Q113.16 382.63 116.215 378.047 Q119.294 373.44 125.104 373.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M145.266 402.746 L150.15 402.746 L150.15 408.625 L145.266 408.625 L145.266 402.746 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M170.335 392.213 Q167.002 392.213 165.081 393.996 Q163.183 395.778 163.183 398.903 Q163.183 402.028 165.081 403.81 Q167.002 405.593 170.335 405.593 Q173.669 405.593 175.59 403.81 Q177.511 402.005 177.511 398.903 Q177.511 395.778 175.59 393.996 Q173.692 392.213 170.335 392.213 M165.659 390.222 Q162.65 389.482 160.96 387.422 Q159.294 385.361 159.294 382.398 Q159.294 378.255 162.233 375.848 Q165.196 373.44 170.335 373.44 Q175.497 373.44 178.437 375.848 Q181.377 378.255 181.377 382.398 Q181.377 385.361 179.687 387.422 Q178.02 389.482 175.034 390.222 Q178.414 391.009 180.289 393.301 Q182.187 395.593 182.187 398.903 Q182.187 403.926 179.108 406.611 Q176.053 409.296 170.335 409.296 Q164.618 409.296 161.539 406.611 Q158.484 403.926 158.484 398.903 Q158.484 395.593 160.382 393.301 Q162.28 391.009 165.659 390.222 M163.946 382.838 Q163.946 385.523 165.613 387.028 Q167.303 388.533 170.335 388.533 Q173.345 388.533 175.034 387.028 Q176.747 385.523 176.747 382.838 Q176.747 380.153 175.034 378.648 Q173.345 377.144 170.335 377.144 Q167.303 377.144 165.613 378.648 Q163.946 380.153 163.946 382.838 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M125.197 235.121 Q121.586 235.121 119.757 238.686 Q117.951 242.227 117.951 249.357 Q117.951 256.463 119.757 260.028 Q121.586 263.57 125.197 263.57 Q128.831 263.57 130.636 260.028 Q132.465 256.463 132.465 249.357 Q132.465 242.227 130.636 238.686 Q128.831 235.121 125.197 235.121 M125.197 231.417 Q131.007 231.417 134.062 236.024 Q137.141 240.607 137.141 249.357 Q137.141 258.084 134.062 262.69 Q131.007 267.273 125.197 267.273 Q119.386 267.273 116.308 262.69 Q113.252 258.084 113.252 249.357 Q113.252 240.607 116.308 236.024 Q119.386 231.417 125.197 231.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M145.359 260.723 L150.243 260.723 L150.243 266.602 L145.359 266.602 L145.359 260.723 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M160.567 265.885 L160.567 261.625 Q162.326 262.459 164.132 262.898 Q165.937 263.338 167.673 263.338 Q172.303 263.338 174.733 260.236 Q177.187 257.111 177.534 250.769 Q176.192 252.76 174.132 253.824 Q172.071 254.889 169.571 254.889 Q164.386 254.889 161.354 251.764 Q158.345 248.616 158.345 243.176 Q158.345 237.852 161.493 234.635 Q164.641 231.417 169.872 231.417 Q175.868 231.417 179.016 236.024 Q182.187 240.607 182.187 249.357 Q182.187 257.528 178.298 262.412 Q174.432 267.273 167.882 267.273 Q166.122 267.273 164.317 266.926 Q162.511 266.579 160.567 265.885 M169.872 251.232 Q173.02 251.232 174.849 249.079 Q176.701 246.926 176.701 243.176 Q176.701 239.45 174.849 237.297 Q173.02 235.121 169.872 235.121 Q166.724 235.121 164.872 237.297 Q163.044 239.45 163.044 243.176 Q163.044 246.926 164.872 249.079 Q166.724 251.232 169.872 251.232 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M115.822 120.644 L123.461 120.644 L123.461 94.2784 L115.15 95.9451 L115.15 91.6858 L123.414 90.0192 L128.09 90.0192 L128.09 120.644 L135.729 120.644 L135.729 124.579 L115.822 124.579 L115.822 120.644 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M145.173 118.7 L150.058 118.7 L150.058 124.579 L145.173 124.579 L145.173 118.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M170.243 93.0979 Q166.632 93.0979 164.803 96.6627 Q162.997 100.204 162.997 107.334 Q162.997 114.44 164.803 118.005 Q166.632 121.547 170.243 121.547 Q173.877 121.547 175.682 118.005 Q177.511 114.44 177.511 107.334 Q177.511 100.204 175.682 96.6627 Q173.877 93.0979 170.243 93.0979 M170.243 89.3942 Q176.053 89.3942 179.108 94.0006 Q182.187 98.5839 182.187 107.334 Q182.187 116.061 179.108 120.667 Q176.053 125.25 170.243 125.25 Q164.433 125.25 161.354 120.667 Q158.298 116.061 158.298 107.334 Q158.298 98.5839 161.354 94.0006 Q164.433 89.3942 170.243 89.3942 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip722)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,176.349 325.239,250.207 357.991,368.987 390.744,489.869 423.496,557.906 456.249,622.332 489.001,680.306 521.754,746.197 554.507,764.298 \n",
       "  587.259,780.129 652.764,822.614 718.269,872.587 783.775,872.769 849.28,880.057 947.537,899.672 1045.8,893.944 1176.81,921.502 1307.82,951.388 1471.58,1005.96 \n",
       "  1668.09,924.607 1864.61,944.319 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ],
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip690\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip691\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip692\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,959.437 1912.76,959.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,817.414 1912.76,817.414 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,675.391 1912.76,675.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,533.368 1912.76,533.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,391.345 1912.76,391.345 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,249.322 1912.76,249.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,107.299 1912.76,107.299 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,959.437 230.485,959.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,817.414 230.485,817.414 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,675.391 230.485,675.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,533.368 230.485,533.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,391.345 230.485,391.345 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,249.322 230.485,249.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,107.299 230.485,107.299 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M124.525 945.236 Q120.914 945.236 119.086 948.801 Q117.28 952.342 117.28 959.472 Q117.28 966.578 119.086 970.143 Q120.914 973.685 124.525 973.685 Q128.16 973.685 129.965 970.143 Q131.794 966.578 131.794 959.472 Q131.794 952.342 129.965 948.801 Q128.16 945.236 124.525 945.236 M124.525 941.532 Q130.336 941.532 133.391 946.138 Q136.47 950.722 136.47 959.472 Q136.47 968.199 133.391 972.805 Q130.336 977.388 124.525 977.388 Q118.715 977.388 115.637 972.805 Q112.581 968.199 112.581 959.472 Q112.581 950.722 115.637 946.138 Q118.715 941.532 124.525 941.532 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M144.687 970.837 L149.572 970.837 L149.572 976.717 L144.687 976.717 L144.687 970.837 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M172.604 946.231 L160.798 964.68 L172.604 964.68 L172.604 946.231 M171.377 942.157 L177.257 942.157 L177.257 964.68 L182.187 964.68 L182.187 968.569 L177.257 968.569 L177.257 976.717 L172.604 976.717 L172.604 968.569 L157.002 968.569 L157.002 964.055 L171.377 942.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M126.007 803.213 Q122.396 803.213 120.567 806.778 Q118.761 810.319 118.761 817.449 Q118.761 824.555 120.567 828.12 Q122.396 831.662 126.007 831.662 Q129.641 831.662 131.447 828.12 Q133.275 824.555 133.275 817.449 Q133.275 810.319 131.447 806.778 Q129.641 803.213 126.007 803.213 M126.007 799.509 Q131.817 799.509 134.873 804.116 Q137.951 808.699 137.951 817.449 Q137.951 826.176 134.873 830.782 Q131.817 835.365 126.007 835.365 Q120.197 835.365 117.118 830.782 Q114.062 826.176 114.062 817.449 Q114.062 808.699 117.118 804.116 Q120.197 799.509 126.007 799.509 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M146.169 828.814 L151.053 828.814 L151.053 834.694 L146.169 834.694 L146.169 828.814 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M161.284 800.134 L179.641 800.134 L179.641 804.069 L165.567 804.069 L165.567 812.541 Q166.585 812.194 167.604 812.032 Q168.622 811.847 169.641 811.847 Q175.428 811.847 178.807 815.018 Q182.187 818.19 182.187 823.606 Q182.187 829.185 178.715 832.287 Q175.243 835.365 168.923 835.365 Q166.747 835.365 164.479 834.995 Q162.233 834.625 159.826 833.884 L159.826 829.185 Q161.909 830.319 164.132 830.875 Q166.354 831.43 168.831 831.43 Q172.835 831.43 175.173 829.324 Q177.511 827.217 177.511 823.606 Q177.511 819.995 175.173 817.889 Q172.835 815.782 168.831 815.782 Q166.956 815.782 165.081 816.199 Q163.229 816.615 161.284 817.495 L161.284 800.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M124.849 661.19 Q121.238 661.19 119.41 664.755 Q117.604 668.296 117.604 675.426 Q117.604 682.532 119.41 686.097 Q121.238 689.639 124.849 689.639 Q128.484 689.639 130.289 686.097 Q132.118 682.532 132.118 675.426 Q132.118 668.296 130.289 664.755 Q128.484 661.19 124.849 661.19 M124.849 657.486 Q130.66 657.486 133.715 662.093 Q136.794 666.676 136.794 675.426 Q136.794 684.153 133.715 688.759 Q130.66 693.342 124.849 693.342 Q119.039 693.342 115.961 688.759 Q112.905 684.153 112.905 675.426 Q112.905 666.676 115.961 662.093 Q119.039 657.486 124.849 657.486 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M145.011 686.791 L149.896 686.791 L149.896 692.671 L145.011 692.671 L145.011 686.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M170.659 673.528 Q167.511 673.528 165.659 675.68 Q163.831 677.833 163.831 681.583 Q163.831 685.31 165.659 687.486 Q167.511 689.639 170.659 689.639 Q173.807 689.639 175.636 687.486 Q177.488 685.31 177.488 681.583 Q177.488 677.833 175.636 675.68 Q173.807 673.528 170.659 673.528 M179.942 658.875 L179.942 663.134 Q178.182 662.301 176.377 661.861 Q174.595 661.421 172.835 661.421 Q168.206 661.421 165.752 664.546 Q163.321 667.671 162.974 673.991 Q164.34 671.977 166.4 670.912 Q168.46 669.824 170.937 669.824 Q176.145 669.824 179.155 672.995 Q182.187 676.143 182.187 681.583 Q182.187 686.907 179.039 690.125 Q175.891 693.342 170.659 693.342 Q164.664 693.342 161.493 688.759 Q158.321 684.153 158.321 675.426 Q158.321 667.231 162.21 662.37 Q166.099 657.486 172.65 657.486 Q174.409 657.486 176.192 657.833 Q177.997 658.181 179.942 658.875 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M125.914 519.167 Q122.303 519.167 120.474 522.732 Q118.669 526.273 118.669 533.403 Q118.669 540.509 120.474 544.074 Q122.303 547.616 125.914 547.616 Q129.548 547.616 131.354 544.074 Q133.183 540.509 133.183 533.403 Q133.183 526.273 131.354 522.732 Q129.548 519.167 125.914 519.167 M125.914 515.463 Q131.724 515.463 134.78 520.07 Q137.859 524.653 137.859 533.403 Q137.859 542.13 134.78 546.736 Q131.724 551.319 125.914 551.319 Q120.104 551.319 117.025 546.736 Q113.97 542.13 113.97 533.403 Q113.97 524.653 117.025 520.07 Q120.104 515.463 125.914 515.463 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M146.076 544.769 L150.96 544.769 L150.96 550.648 L146.076 550.648 L146.076 544.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M159.965 516.088 L182.187 516.088 L182.187 518.079 L169.641 550.648 L164.757 550.648 L176.562 520.023 L159.965 520.023 L159.965 516.088 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M125.104 377.144 Q121.493 377.144 119.664 380.709 Q117.859 384.25 117.859 391.38 Q117.859 398.486 119.664 402.051 Q121.493 405.593 125.104 405.593 Q128.738 405.593 130.544 402.051 Q132.373 398.486 132.373 391.38 Q132.373 384.25 130.544 380.709 Q128.738 377.144 125.104 377.144 M125.104 373.44 Q130.914 373.44 133.97 378.047 Q137.048 382.63 137.048 391.38 Q137.048 400.107 133.97 404.713 Q130.914 409.296 125.104 409.296 Q119.294 409.296 116.215 404.713 Q113.16 400.107 113.16 391.38 Q113.16 382.63 116.215 378.047 Q119.294 373.44 125.104 373.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M145.266 402.746 L150.15 402.746 L150.15 408.625 L145.266 408.625 L145.266 402.746 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M170.335 392.213 Q167.002 392.213 165.081 393.996 Q163.183 395.778 163.183 398.903 Q163.183 402.028 165.081 403.81 Q167.002 405.593 170.335 405.593 Q173.669 405.593 175.59 403.81 Q177.511 402.005 177.511 398.903 Q177.511 395.778 175.59 393.996 Q173.692 392.213 170.335 392.213 M165.659 390.222 Q162.65 389.482 160.96 387.422 Q159.294 385.361 159.294 382.398 Q159.294 378.255 162.233 375.848 Q165.196 373.44 170.335 373.44 Q175.497 373.44 178.437 375.848 Q181.377 378.255 181.377 382.398 Q181.377 385.361 179.687 387.422 Q178.02 389.482 175.034 390.222 Q178.414 391.009 180.289 393.301 Q182.187 395.593 182.187 398.903 Q182.187 403.926 179.108 406.611 Q176.053 409.296 170.335 409.296 Q164.618 409.296 161.539 406.611 Q158.484 403.926 158.484 398.903 Q158.484 395.593 160.382 393.301 Q162.28 391.009 165.659 390.222 M163.946 382.838 Q163.946 385.523 165.613 387.028 Q167.303 388.533 170.335 388.533 Q173.345 388.533 175.034 387.028 Q176.747 385.523 176.747 382.838 Q176.747 380.153 175.034 378.648 Q173.345 377.144 170.335 377.144 Q167.303 377.144 165.613 378.648 Q163.946 380.153 163.946 382.838 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M125.197 235.121 Q121.586 235.121 119.757 238.686 Q117.951 242.227 117.951 249.357 Q117.951 256.463 119.757 260.028 Q121.586 263.57 125.197 263.57 Q128.831 263.57 130.636 260.028 Q132.465 256.463 132.465 249.357 Q132.465 242.227 130.636 238.686 Q128.831 235.121 125.197 235.121 M125.197 231.417 Q131.007 231.417 134.062 236.024 Q137.141 240.607 137.141 249.357 Q137.141 258.084 134.062 262.69 Q131.007 267.273 125.197 267.273 Q119.386 267.273 116.308 262.69 Q113.252 258.084 113.252 249.357 Q113.252 240.607 116.308 236.024 Q119.386 231.417 125.197 231.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M145.359 260.723 L150.243 260.723 L150.243 266.602 L145.359 266.602 L145.359 260.723 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M160.567 265.885 L160.567 261.625 Q162.326 262.459 164.132 262.898 Q165.937 263.338 167.673 263.338 Q172.303 263.338 174.733 260.236 Q177.187 257.111 177.534 250.769 Q176.192 252.76 174.132 253.824 Q172.071 254.889 169.571 254.889 Q164.386 254.889 161.354 251.764 Q158.345 248.616 158.345 243.176 Q158.345 237.852 161.493 234.635 Q164.641 231.417 169.872 231.417 Q175.868 231.417 179.016 236.024 Q182.187 240.607 182.187 249.357 Q182.187 257.528 178.298 262.412 Q174.432 267.273 167.882 267.273 Q166.122 267.273 164.317 266.926 Q162.511 266.579 160.567 265.885 M169.872 251.232 Q173.02 251.232 174.849 249.079 Q176.701 246.926 176.701 243.176 Q176.701 239.45 174.849 237.297 Q173.02 235.121 169.872 235.121 Q166.724 235.121 164.872 237.297 Q163.044 239.45 163.044 243.176 Q163.044 246.926 164.872 249.079 Q166.724 251.232 169.872 251.232 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M115.822 120.644 L123.461 120.644 L123.461 94.2784 L115.15 95.9451 L115.15 91.6858 L123.414 90.0192 L128.09 90.0192 L128.09 120.644 L135.729 120.644 L135.729 124.579 L115.822 124.579 L115.822 120.644 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M145.173 118.7 L150.058 118.7 L150.058 124.579 L145.173 124.579 L145.173 118.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M170.243 93.0979 Q166.632 93.0979 164.803 96.6627 Q162.997 100.204 162.997 107.334 Q162.997 114.44 164.803 118.005 Q166.632 121.547 170.243 121.547 Q173.877 121.547 175.682 118.005 Q177.511 114.44 177.511 107.334 Q177.511 100.204 175.682 96.6627 Q173.877 93.0979 170.243 93.0979 M170.243 89.3942 Q176.053 89.3942 179.108 94.0006 Q182.187 98.5839 182.187 107.334 Q182.187 116.061 179.108 120.667 Q176.053 125.25 170.243 125.25 Q164.433 125.25 161.354 120.667 Q158.298 116.061 158.298 107.334 Q158.298 98.5839 161.354 94.0006 Q164.433 89.3942 170.243 89.3942 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip692)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,176.349 325.239,250.207 357.991,368.987 390.744,489.869 423.496,557.906 456.249,622.332 489.001,680.306 521.754,746.197 554.507,764.298 \n",
       "  587.259,780.129 652.764,822.614 718.269,872.587 783.775,872.769 849.28,880.057 947.537,899.672 1045.8,893.944 1176.81,921.502 1307.82,951.388 1471.58,1005.96 \n",
       "  1668.09,924.607 1864.61,944.319 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)\n",
    "\n",
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"big\" (2/3)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": 50
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10-element Vector{Int64}:\n 3\n 1\n 1\n 1\n 5\n 3\n 1\n 1\n 1\n 2"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬────────────┬────────────┬──────────────────────────────────┐\n",
      "│ a     │ b          │ c          │ d                                │\n",
      "│ Int64 │ Float64    │ Float64    │ CategoricalValue{String, UInt32} │\n",
      "│ Count │ Continuous │ Continuous │ OrderedFactor{2}                 │\n",
      "├───────┼────────────┼────────────┼──────────────────────────────────┤\n",
      "│ 1     │ 0.245296   │ 0.683787   │ male                             │\n",
      "│ 2     │ 0.741169   │ 0.51769    │ female                           │\n",
      "│ 3     │ 0.941211   │ 0.696902   │ female                           │\n",
      "│ 4     │ 0.660113   │ 0.939313   │ male                             │\n",
      "└───────┴────────────┴────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous,\n",
    "                 name -> true);"
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Int64}:\n 1\n 2\n 3\n 4"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": 54
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┐\n",
      "│ b          │ c          │\n",
      "│ Float64    │ Float64    │\n",
      "│ Continuous │ Continuous │\n",
      "├────────────┼────────────┤\n",
      "│ 0.245296   │ 0.683787   │\n",
      "│ 0.741169   │ 0.51769    │\n",
      "│ 0.941211   │ 0.696902   │\n",
      "│ 0.660113   │ 0.939313   │\n",
      "└────────────┴────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": 55
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"male\"\n \"female\"\n \"female\"\n \"male\""
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬─────────────────────────────────┬───────────────────\n│\u001b[22m _.names                 \u001b[0m│\u001b[22m _.types                         \u001b[0m│\u001b[22m _.scitypes      \u001b[0m ⋯\n├─────────────────────────┼─────────────────────────────────┼───────────────────\n│ surgery                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    ⋯\n│ age                     │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    ⋯\n│ rectal_temperature      │ Float64                         │ Continuous       ⋯\n│ pulse                   │ Float64                         │ Continuous       ⋯\n│ respiratory_rate        │ Float64                         │ Continuous       ⋯\n│ temperature_extremities │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} ⋯\n│ mucous_membranes        │ CategoricalValue{Int64, UInt32} │ Multiclass{6}    ⋯\n│ capillary_refill_time   │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    ⋯\n│ pain                    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{5} ⋯\n│ peristalsis             │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} ⋯\n│ abdominal_distension    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} ⋯\n│ packed_cell_volume      │ Float64                         │ Continuous       ⋯\n│ total_protein           │ Float64                         │ Continuous       ⋯\n│ outcome                 │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    ⋯\n│ surgical_lesion         │ CategoricalValue{Int64, UInt32} │ OrderedFactor{2} ⋯\n│ cp_data                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    ⋯\n└─────────────────────────┴─────────────────────────────────┴───────────────────\n_.nrows = 366\n"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
