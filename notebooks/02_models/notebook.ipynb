{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.7.1\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env`\n",
      "┌ Warning: The active manifest file is missing a julia version entry. Dependencies may have been resolved with a different julia version.\n",
      "└ @ ~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env/Manifest.toml:0\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n  Marshall \u001b[1mPlease cite\u001b[22m:\n\n  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n  the pattern recognition literature. Fisher's paper is a classic in the field\n  and is referenced frequently to this day. (See Duda & Hart, for example.)\n  The data set contains 3 classes of 50 instances each, where each class\n  refers to a type of iris plant. One class is linearly separable from the\n  other 2; the latter are NOT linearly separable from each other.\n\n  Predicted attribute: class of iris plant. This is an exceedingly simple\n  domain.\n\n\u001b[1m  Attribute Information:\u001b[22m\n\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n\n\u001b[36m  1. sepal length in cm\u001b[39m\n\u001b[36m  2. sepal width in cm\u001b[39m\n\u001b[36m  3. petal length in cm\u001b[39m\n\u001b[36m  4. petal width in cm\u001b[39m\n\u001b[36m  5. class: \u001b[39m\n\u001b[36m     -- Iris Setosa\u001b[39m\n\u001b[36m     -- Iris Versicolour\u001b[39m\n\u001b[36m     -- Iris Virginica\u001b[39m",
      "text/markdown": "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n\n**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n\n### Attribute Information:\n\n```\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n   -- Iris Setosa\n   -- Iris Versicolour\n   -- Iris Virginica\n```\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n─────┼───────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n   4 │         4.6         3.1          1.5         0.2  Iris-setosa",
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "import DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬───────────────┬──────────────────────────────────┐\n│\u001b[22m names       \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                            \u001b[0m│\n├─────────────┼───────────────┼──────────────────────────────────┤\n│ sepallength │ Continuous    │ Float64                          │\n│ sepalwidth  │ Continuous    │ Float64                          │\n│ petallength │ Continuous    │ Float64                          │\n│ petalwidth  │ Continuous    │ Float64                          │\n│ class       │ Multiclass{3} │ CategoricalValue{String, UInt32} │\n└─────────────┴───────────────┴──────────────────────────────────┘\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "These look fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We can randomize the data at\n",
    "the same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{Multiclass{3}} (alias for AbstractArray{ScientificTypesBase.Multiclass{3}, 1})"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This puts the `:class` column into a vector `y`, and all remaining\n",
    "columns into a table `X`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[36m  unpack(table, f1, f2, ... fk;\u001b[39m\n\u001b[36m         wrap_singles=false,\u001b[39m\n\u001b[36m         shuffle=false,\u001b[39m\n\u001b[36m         rng::Union{AbstractRNG,Int,Nothing}=nothing,\u001b[39m\n\u001b[36m         coerce_options...)\u001b[39m\n\n  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables or\n  vectors by making column selections determined by the predicates \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m,\n  ..., \u001b[36mfk\u001b[39m. Selection from the column names is without replacement. A \u001b[4mpredicate\u001b[24m\n  is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column\n  \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m.\n\n  Returns a tuple of tables/vectors with length one greater than the number of\n  supplied predicates, with the last component including all previously\n  unselected columns.\n\n\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n\u001b[36m  2×4 DataFrame\u001b[39m\n\u001b[36m   Row │ x      y     z        w\u001b[39m\n\u001b[36m       │ Int64  Char  Float64  String\u001b[39m\n\u001b[36m  ─────┼──────────────────────────────\u001b[39m\n\u001b[36m     1 │     1  a        10.0  A\u001b[39m\n\u001b[36m     2 │     2  b        20.0  B\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  Z, XY, W = unpack(table, ==(:z), !=(:w))\u001b[39m\n\u001b[36m  julia> Z\u001b[39m\n\u001b[36m  2-element Vector{Float64}:\u001b[39m\n\u001b[36m   10.0\u001b[39m\n\u001b[36m   20.0\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> XY\u001b[39m\n\u001b[36m  2×2 DataFrame\u001b[39m\n\u001b[36m   Row │ x      y\u001b[39m\n\u001b[36m       │ Int64  Char\u001b[39m\n\u001b[36m  ─────┼─────────────\u001b[39m\n\u001b[36m     1 │     1  a\u001b[39m\n\u001b[36m     2 │     2  b\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> W  # the column(s) left over\u001b[39m\n\u001b[36m  2-element Vector{String}:\u001b[39m\n\u001b[36m   \"A\"\u001b[39m\n\u001b[36m   \"B\"\u001b[39m\n\n  Whenever a returned table contains a single column, it is converted to a\n  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n\n  If \u001b[36mcoerce_options\u001b[39m are specified then \u001b[36mtable\u001b[39m is first replaced with\n  \u001b[36mcoerce(table, coerce_options)\u001b[39m. See \u001b[36mScientificTypes.coerce\u001b[39m for details.\n\n  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n  \u001b[36mshuffle=true\u001b[39m is implicit.",
      "text/markdown": "```\nunpack(table, f1, f2, ... fk;\n       wrap_singles=false,\n       shuffle=false,\n       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n       coerce_options...)\n```\n\nHorizontally split any Tables.jl compatible `table` into smaller tables or vectors by making column selections determined by the predicates `f1`, `f2`, ..., `fk`. Selection from the column names is without replacement. A *predicate* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`.\n\nReturns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n\n```\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n2×4 DataFrame\n Row │ x      y     z        w\n     │ Int64  Char  Float64  String\n─────┼──────────────────────────────\n   1 │     1  a        10.0  A\n   2 │     2  b        20.0  B\n\nZ, XY, W = unpack(table, ==(:z), !=(:w))\njulia> Z\n2-element Vector{Float64}:\n 10.0\n 20.0\n\njulia> XY\n2×2 DataFrame\n Row │ x      y\n     │ Int64  Char\n─────┼─────────────\n   1 │     1  a\n   2 │     2  b\n\njulia> W  # the column(s) left over\n2-element Vector{String}:\n \"A\"\n \"B\"\n```\n\nWhenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n\nIf `coerce_options` are specified then `table` is first replaced with `coerce(table, coerce_options)`. See [`ScientificTypes.coerce`](@ref) for details.\n\nIf `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "186-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = ARDRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BM25Transformer, package_name = MLJText, ... )\n ⋮\n (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )\n (name = XGBoostCount, package_name = XGBoost, ... )\n (name = XGBoostRegressor, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you already have an idea about the name of the model, you could\n",
    "search by string or regex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = LinearRegressor, package_name = GLM, ... )\n (name = LinearRegressor, package_name = MLJLinearModels, ... )\n (name = LinearRegressor, package_name = MultivariateStats, ... )\n (name = LinearRegressor, package_name = ScikitLearn, ... )\n (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n (name = SVMLinearRegressor, package_name = ScikitLearn, ... )"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "some_models = models(\"LinearRegressor\")"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not\n",
    "yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mLinear regressor (OLS) with a Normal model.\u001b[39m\n\u001b[35m→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\u001b[39m\n\u001b[35m→ do `@load LinearRegressor pkg=\"GLM\"` to use the model.\u001b[39m\n\u001b[35m→ do `?LinearRegressor` for documentation.\u001b[39m\n(name = \"LinearRegressor\",\n package_name = \"GLM\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (),\n docstring =\n     \"Linear regressor (OLS) with a Normal model.\\n→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\\n→ do `@load LinearRegressor pkg=\\\"GLM\\\"` to use the model.\\n→ do `?LinearRegressor` for documentation.\",\n fit_data_scitype =\n     Tuple{ScientificTypesBase.Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:ScientificTypesBase.Continuous), AbstractVector{ScientificTypesBase.Continuous}},\n hyperparameter_ranges = (nothing, nothing, nothing),\n hyperparameter_types = (\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\"),\n hyperparameters = (:fit_intercept, :allowrankdeficient, :offsetcol),\n implemented_methods = [:fit, :fitted_params, :predict, :predict_mean],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = nothing,\n load_path = \"MLJGLMInterface.LinearRegressor\",\n package_license = \"MIT\",\n package_url = \"https://github.com/JuliaStats/GLM.jl\",\n package_uuid = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\",\n predict_scitype =\n     AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = false,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype =\n     ScientificTypesBase.Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:ScientificTypesBase.Continuous),\n target_scitype = AbstractVector{ScientificTypesBase.Continuous},\n output_scitype = ScientificTypesBase.Unknown)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "meta = some_models[1]"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{Continuous} (alias for AbstractArray{ScientificTypesBase.Continuous, 1})"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "false"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n ⋮\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n ⋮\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n    builder = Short(\n            n_hidden = 0,\n            dropout = 0.5,\n            σ = NNlib.σ),\n    finaliser = NNlib.softmax,\n    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n    loss = Flux.Losses.crossentropy,\n    epochs = 10,\n    batch_size = 1,\n    lambda = 0.0,\n    alpha = 0.0,\n    rng = Random._GLOBAL_RNG(),\n    optimiser_changes_trigger_retraining = false,\n    acceleration = ComputationalResources.CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n(name = \"NeuralNetworkClassifier\",\n package_name = \"MLJFlux\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (:optimiser, :builder),\n docstring =\n     \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n fit_data_scitype =\n     Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}},\n hyperparameter_ranges = (nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing),\n hyperparameter_types = (\"MLJFlux.Short\",\n                         \"typeof(NNlib.softmax)\",\n                         \"Flux.Optimise.ADAM\",\n                         \"typeof(Flux.Losses.crossentropy)\",\n                         \"Int64\",\n                         \"Int64\",\n                         \"Float64\",\n                         \"Float64\",\n                         \"Union{Int64, Random.AbstractRNG}\",\n                         \"Bool\",\n                         \"ComputationalResources.AbstractResource\"),\n hyperparameters = (:builder,\n                    :finaliser,\n                    :optimiser,\n                    :loss,\n                    :epochs,\n                    :batch_size,\n                    :lambda,\n                    :alpha,\n                    :rng,\n                    :optimiser_changes_trigger_retraining,\n                    :acceleration),\n implemented_methods = [],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = :epochs,\n load_path = \"MLJFlux.NeuralNetworkClassifier\",\n package_license = \"MIT\",\n package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n predict_scitype =\n     AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = true,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype =\n     ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}},\n target_scitype = AbstractVector{<:ScientificTypesBase.Finite},\n output_scitype = ScientificTypesBase.Unknown)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @918 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @019 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(1:length(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.175\n",
      "[ Info: Loss is 1.14\n",
      "[ Info: Loss is 1.085\n",
      "[ Info: Loss is 1.112\n",
      "[ Info: Loss is 1.123\n",
      "[ Info: Loss is 1.139\n",
      "[ Info: Loss is 1.079\n",
      "[ Info: Loss is 1.029\n",
      "[ Info: Loss is 1.058\n",
      "[ Info: Loss is 1.045\n",
      "[ Info: Loss is 0.9872\n",
      "[ Info: Loss is 1.019\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @918 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @019 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element CategoricalDistributions.UnivariateFiniteArray{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64, 1}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.333, Iris-versicolor=>0.347, Iris-virginica=>0.32)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.307, Iris-versicolor=>0.352, Iris-virginica=>0.341)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.454, Iris-versicolor=>0.315, Iris-virginica=>0.232)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(training_losses = [1.1354121699899986, 1.1750209970243275, 1.1395791009076957, 1.0851724386546644, 1.1123435129557946, 1.1231463698671609, 1.139074237743253, 1.0794514707358978, 1.0294376901314548, 1.058348515394612, 1.0448884307323718, 0.987213431206338, 1.0191162540412542],)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element CategoricalDistributions.UnivariateFiniteArray{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64, 1}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.299, Iris-versicolor=>0.353, Iris-virginica=>0.348)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.332, Iris-versicolor=>0.348, Iris-virginica=>0.321)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.301, Iris-versicolor=>0.352, Iris-virginica=>0.346)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net: 15%[===>                     ]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @465 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @276 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9662\n",
      "[ Info: Loss is 0.9811\n",
      "[ Info: Loss is 0.9737\n",
      "[ Info: Loss is 0.9632\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @918 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @019 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9415\n",
      "[ Info: Loss is 0.8081\n",
      "[ Info: Loss is 0.8252\n",
      "[ Info: Loss is 0.7347\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @918 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @019 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.157\n",
      "[ Info: Loss is 1.119\n",
      "[ Info: Loss is 1.087\n",
      "[ Info: Loss is 1.037\n",
      "[ Info: Loss is 0.9911\n",
      "[ Info: Loss is 0.8854\n",
      "[ Info: Loss is 0.8448\n",
      "[ Info: Loss is 0.8262\n",
      "[ Info: Loss is 0.7744\n",
      "[ Info: Loss is 0.7203\n",
      "[ Info: Loss is 0.7338\n",
      "[ Info: Loss is 0.6636\n",
      "[ Info: Loss is 0.6267\n",
      "[ Info: Loss is 0.6888\n",
      "[ Info: Loss is 0.6419\n",
      "[ Info: Loss is 0.711\n",
      "[ Info: Loss is 0.5899\n",
      "[ Info: Loss is 0.7157\n",
      "[ Info: Loss is 0.7172\n",
      "[ Info: Loss is 0.6879\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @918 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @019 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 13%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 19%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 26%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 32%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 42%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 48%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 52%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 58%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 68%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 74%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 81%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 87%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 97%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   \u001b[1mUnivariateFinite{ScientificTypesBase.Multiclass{3}}\u001b[22m \n                   \u001b[90m┌                                        ┐\u001b[39m \n       \u001b[0mIris-setosa \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■\u001b[39m\u001b[0m 0.11430662519303098                \u001b[90m \u001b[39m \n   \u001b[0mIris-versicolor \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 0.5503093143454909 \u001b[90m \u001b[39m \n    \u001b[0mIris-virginica \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■■■■■■■■■\u001b[39m\u001b[0m 0.335384060461478          \u001b[90m \u001b[39m \n                   \u001b[90m└                                        ┘\u001b[39m "
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":probabilistic"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: - In MLJ, a model that can predict probabilities (and\n",
    "not just point values) will do so by default.  - For most\n",
    "probabilistic predictors, the predicted object is a\n",
    "`Distributions.Distribution` object or a\n",
    "`CategoricalDistributions.UnivariateFinite` object (the case here)\n",
    "which all support the following methods: `rand`, `pdf`, `logpdf`;\n",
    "and, where appropriate: `mode`, `median` and `mean`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.335384060461478"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-versicolor\""
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Float64}:\n 0.5503093143454909\n 0.4769407860698532\n 0.06452639227143382\n 0.349241729332935"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": 36
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4×3 Matrix{Float64}:\n 0.114307  0.550309   0.335384\n 0.048901  0.476941   0.474158\n 0.934022  0.0645264  0.00145125\n 0.011553  0.349242   0.639205"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.38125389359494777"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.044444444444444446"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "62-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type)}}:\n (name = BrierLoss, instances = [brier_loss], ...)\n (name = BrierScore, instances = [brier_score], ...)\n (name = LPLoss, instances = [l1, l2], ...)\n (name = LogCoshLoss, instances = [log_cosh, log_cosh_loss], ...)\n (name = LogLoss, instances = [log_loss, cross_entropy], ...)\n (name = LogScore, instances = [log_score], ...)\n (name = SphericalScore, instances = [spherical_score], ...)\n (name = Accuracy, instances = [accuracy], ...)\n (name = AreaUnderCurve, instances = [area_under_curve, auc], ...)\n (name = BalancedAccuracy, instances = [balanced_accuracy, bacc, bac], ...)\n ⋮\n (name = SmoothedL1HingeLoss, instances = [smoothed_l1_hinge_loss], ...)\n (name = ZeroOneLoss, instances = [zero_one_loss], ...)\n (name = HuberLoss, instances = [huber_loss], ...)\n (name = L1EpsilonInsLoss, instances = [l1_epsilon_ins_loss], ...)\n (name = L2EpsilonInsLoss, instances = [l2_epsilon_ins_loss], ...)\n (name = LPDistLoss, instances = [lp_dist_loss], ...)\n (name = LogitDistLoss, instances = [logit_dist_loss], ...)\n (name = PeriodicLoss, instances = [periodic_loss], ...)\n (name = QuantileLoss, instances = [quantile_loss], ...)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬──────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold \u001b[0m│\n├────────────────────────────┼─────────────┼──────────────┼──────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.381       │ predict      │ [0.381]  │\n│ MisclassificationRate()    │ 0.0444      │ predict_mode │ [0.0444] │\n│ BrierScore()               │ -0.214      │ predict      │ [-0.214] │\n└────────────────────────────┴─────────────┴──────────────┴──────────┘\n"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:10\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:13\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold          \u001b[0m ⋯\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.413       │ predict      │ [0.411, 0.381, 0.5 ⋯\n│ MisclassificationRate()    │ 0.04        │ predict_mode │ [0.04, 0.0, 0.04,  ⋯\n│ BrierScore()               │ -0.232      │ predict      │ [-0.231, -0.195, - ⋯\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 18 folds:  11%[==>                      ]  ETA: 0:00:37\u001b[K\rEvaluating over 18 folds:  17%[====>                    ]  ETA: 0:00:34\u001b[K\rEvaluating over 18 folds:  22%[=====>                   ]  ETA: 0:00:32\u001b[K\rEvaluating over 18 folds:  28%[======>                  ]  ETA: 0:00:29\u001b[K\rEvaluating over 18 folds:  33%[========>                ]  ETA: 0:00:27\u001b[K\rEvaluating over 18 folds:  39%[=========>               ]  ETA: 0:00:25\u001b[K\rEvaluating over 18 folds:  44%[===========>             ]  ETA: 0:00:23\u001b[K\rEvaluating over 18 folds:  50%[============>            ]  ETA: 0:00:20\u001b[K\rEvaluating over 18 folds:  56%[=============>           ]  ETA: 0:00:18\u001b[K\rEvaluating over 18 folds:  61%[===============>         ]  ETA: 0:00:16\u001b[K\rEvaluating over 18 folds:  67%[================>        ]  ETA: 0:00:13\u001b[K\rEvaluating over 18 folds:  72%[==================>      ]  ETA: 0:00:11\u001b[K\rEvaluating over 18 folds:  78%[===================>     ]  ETA: 0:00:09\u001b[K\rEvaluating over 18 folds:  83%[====================>    ]  ETA: 0:00:07\u001b[K\rEvaluating over 18 folds:  89%[======================>  ]  ETA: 0:00:04\u001b[K\rEvaluating over 18 folds:  94%[=======================> ]  ETA: 0:00:02\u001b[K\rEvaluating over 18 folds: 100%[=========================] Time: 0:00:40\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold          \u001b[0m ⋯\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.394       │ predict      │ [0.301, 0.326, 0.3 ⋯\n│ MisclassificationRate()    │ 0.0356      │ predict_mode │ [0.0, 0.0, 0.0, 0. ⋯\n│ BrierScore()               │ -0.216      │ predict      │ [-0.16, -0.167, -0 ⋯\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Creating subsamples from a subset of all rows. \n",
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:09\u001b[K\n",
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  4%[>                        ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  8%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 12%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 14%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 18%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 20%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 22%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 24%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 25%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 27%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 33%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 37%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 41%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 43%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 47%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 49%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 51%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 53%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 57%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 59%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 63%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 67%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 73%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 75%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 76%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 78%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 80%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5) on log10 scale"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log10)"
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "[ Info: Attempting to evaluate 22 models.\n",
      "\rEvaluating over 22 metamodels:   0%[>                        ]  ETA: N/A\u001b[K\rEvaluating over 22 metamodels:   5%[=>                       ]  ETA: 0:00:21\u001b[K\rEvaluating over 22 metamodels:   9%[==>                      ]  ETA: 0:00:14\u001b[K\rEvaluating over 22 metamodels:  14%[===>                     ]  ETA: 0:00:09\u001b[K\rEvaluating over 22 metamodels:  18%[====>                    ]  ETA: 0:00:07\u001b[K\rEvaluating over 22 metamodels:  23%[=====>                   ]  ETA: 0:00:05\u001b[K\rEvaluating over 22 metamodels:  27%[======>                  ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  32%[=======>                 ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  36%[=========>               ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  41%[==========>              ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  45%[===========>             ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  50%[============>            ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  55%[=============>           ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  59%[==============>          ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  64%[===============>         ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  68%[=================>       ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  73%[==================>      ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  77%[===================>     ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  82%[====================>    ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  86%[=====================>   ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  95%[=======================> ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels: 100%[=========================] Time: 0:00:03\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(parameter_name = \"epochs\",\n parameter_scale = :log10,\n parameter_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 17, 19, 22, 25, 29, 33, 38, 44, 50],\n measurements = [0.9874976446285566, 0.8519401629310261, 0.7244470398086738, 0.6645356554277381, 0.6670583906092185, 0.5952243890927977, 0.6237282863842009, 0.5954393042987541, 0.5695708602028463, 0.5437789777701643  …  0.4530288789617725, 0.4626442203954852, 0.4432925582736485, 0.4434541874565817, 0.42243823223304605, 0.420798962993395, 0.41194355778351455, 0.3891099489137666, 0.3827399705850807, 0.39603525980331045],)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Plot{Plots.GRBackend() n=1}",
      "image/png": "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",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip280\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip281\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip282\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,979.391 1912.76,979.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,825.48 1912.76,825.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,671.569 1912.76,671.569 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,517.658 1912.76,517.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,363.747 1912.76,363.747 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,209.836 1912.76,209.836 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,55.9253 1912.76,55.9253 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,979.391 230.485,979.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,825.48 230.485,825.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,671.569 230.485,671.569 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,517.658 230.485,517.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,363.747 230.485,363.747 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,209.836 230.485,209.836 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,55.9253 230.485,55.9253 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M124.525 965.19 Q120.914 965.19 119.086 968.755 Q117.28 972.297 117.28 979.426 Q117.28 986.533 119.086 990.097 Q120.914 993.639 124.525 993.639 Q128.16 993.639 129.965 990.097 Q131.794 986.533 131.794 979.426 Q131.794 972.297 129.965 968.755 Q128.16 965.19 124.525 965.19 M124.525 961.486 Q130.336 961.486 133.391 966.093 Q136.47 970.676 136.47 979.426 Q136.47 988.153 133.391 992.759 Q130.336 997.343 124.525 997.343 Q118.715 997.343 115.637 992.759 Q112.581 988.153 112.581 979.426 Q112.581 970.676 115.637 966.093 Q118.715 961.486 124.525 961.486 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M144.687 990.792 L149.572 990.792 L149.572 996.671 L144.687 996.671 L144.687 990.792 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M172.604 966.186 L160.798 984.635 L172.604 984.635 L172.604 966.186 M171.377 962.111 L177.257 962.111 L177.257 984.635 L182.187 984.635 L182.187 988.523 L177.257 988.523 L177.257 996.671 L172.604 996.671 L172.604 988.523 L157.002 988.523 L157.002 984.01 L171.377 962.111 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M126.007 811.279 Q122.396 811.279 120.567 814.844 Q118.761 818.386 118.761 825.515 Q118.761 832.622 120.567 836.186 Q122.396 839.728 126.007 839.728 Q129.641 839.728 131.447 836.186 Q133.275 832.622 133.275 825.515 Q133.275 818.386 131.447 814.844 Q129.641 811.279 126.007 811.279 M126.007 807.575 Q131.817 807.575 134.873 812.182 Q137.951 816.765 137.951 825.515 Q137.951 834.242 134.873 838.848 Q131.817 843.432 126.007 843.432 Q120.197 843.432 117.118 838.848 Q114.062 834.242 114.062 825.515 Q114.062 816.765 117.118 812.182 Q120.197 807.575 126.007 807.575 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M146.169 836.881 L151.053 836.881 L151.053 842.76 L146.169 842.76 L146.169 836.881 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M161.284 808.2 L179.641 808.2 L179.641 812.136 L165.567 812.136 L165.567 820.608 Q166.585 820.261 167.604 820.099 Q168.622 819.913 169.641 819.913 Q175.428 819.913 178.807 823.085 Q182.187 826.256 182.187 831.673 Q182.187 837.251 178.715 840.353 Q175.243 843.432 168.923 843.432 Q166.747 843.432 164.479 843.061 Q162.233 842.691 159.826 841.95 L159.826 837.251 Q161.909 838.385 164.132 838.941 Q166.354 839.497 168.831 839.497 Q172.835 839.497 175.173 837.39 Q177.511 835.284 177.511 831.673 Q177.511 828.061 175.173 825.955 Q172.835 823.849 168.831 823.849 Q166.956 823.849 165.081 824.265 Q163.229 824.682 161.284 825.561 L161.284 808.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M124.849 657.368 Q121.238 657.368 119.41 660.933 Q117.604 664.475 117.604 671.604 Q117.604 678.711 119.41 682.275 Q121.238 685.817 124.849 685.817 Q128.484 685.817 130.289 682.275 Q132.118 678.711 132.118 671.604 Q132.118 664.475 130.289 660.933 Q128.484 657.368 124.849 657.368 M124.849 653.664 Q130.66 653.664 133.715 658.271 Q136.794 662.854 136.794 671.604 Q136.794 680.331 133.715 684.937 Q130.66 689.521 124.849 689.521 Q119.039 689.521 115.961 684.937 Q112.905 680.331 112.905 671.604 Q112.905 662.854 115.961 658.271 Q119.039 653.664 124.849 653.664 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M145.011 682.97 L149.896 682.97 L149.896 688.849 L145.011 688.849 L145.011 682.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M170.659 669.706 Q167.511 669.706 165.659 671.859 Q163.831 674.012 163.831 677.762 Q163.831 681.488 165.659 683.664 Q167.511 685.817 170.659 685.817 Q173.807 685.817 175.636 683.664 Q177.488 681.488 177.488 677.762 Q177.488 674.012 175.636 671.859 Q173.807 669.706 170.659 669.706 M179.942 655.053 L179.942 659.313 Q178.182 658.479 176.377 658.039 Q174.595 657.6 172.835 657.6 Q168.206 657.6 165.752 660.725 Q163.321 663.85 162.974 670.169 Q164.34 668.155 166.4 667.09 Q168.46 666.002 170.937 666.002 Q176.145 666.002 179.155 669.174 Q182.187 672.322 182.187 677.762 Q182.187 683.086 179.039 686.303 Q175.891 689.521 170.659 689.521 Q164.664 689.521 161.493 684.937 Q158.321 680.331 158.321 671.604 Q158.321 663.41 162.21 658.549 Q166.099 653.664 172.65 653.664 Q174.409 653.664 176.192 654.012 Q177.997 654.359 179.942 655.053 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M125.914 503.457 Q122.303 503.457 120.474 507.022 Q118.669 510.564 118.669 517.693 Q118.669 524.8 120.474 528.364 Q122.303 531.906 125.914 531.906 Q129.548 531.906 131.354 528.364 Q133.183 524.8 133.183 517.693 Q133.183 510.564 131.354 507.022 Q129.548 503.457 125.914 503.457 M125.914 499.753 Q131.724 499.753 134.78 504.36 Q137.859 508.943 137.859 517.693 Q137.859 526.42 134.78 531.026 Q131.724 535.61 125.914 535.61 Q120.104 535.61 117.025 531.026 Q113.97 526.42 113.97 517.693 Q113.97 508.943 117.025 504.36 Q120.104 499.753 125.914 499.753 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M146.076 529.059 L150.96 529.059 L150.96 534.938 L146.076 534.938 L146.076 529.059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M159.965 500.378 L182.187 500.378 L182.187 502.369 L169.641 534.938 L164.757 534.938 L176.562 504.314 L159.965 504.314 L159.965 500.378 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M125.104 349.546 Q121.493 349.546 119.664 353.111 Q117.859 356.652 117.859 363.782 Q117.859 370.888 119.664 374.453 Q121.493 377.995 125.104 377.995 Q128.738 377.995 130.544 374.453 Q132.373 370.888 132.373 363.782 Q132.373 356.652 130.544 353.111 Q128.738 349.546 125.104 349.546 M125.104 345.842 Q130.914 345.842 133.97 350.449 Q137.048 355.032 137.048 363.782 Q137.048 372.509 133.97 377.115 Q130.914 381.699 125.104 381.699 Q119.294 381.699 116.215 377.115 Q113.16 372.509 113.16 363.782 Q113.16 355.032 116.215 350.449 Q119.294 345.842 125.104 345.842 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M145.266 375.148 L150.15 375.148 L150.15 381.027 L145.266 381.027 L145.266 375.148 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M170.335 364.615 Q167.002 364.615 165.081 366.398 Q163.183 368.18 163.183 371.305 Q163.183 374.43 165.081 376.213 Q167.002 377.995 170.335 377.995 Q173.669 377.995 175.59 376.213 Q177.511 374.407 177.511 371.305 Q177.511 368.18 175.59 366.398 Q173.692 364.615 170.335 364.615 M165.659 362.625 Q162.65 361.884 160.96 359.824 Q159.294 357.764 159.294 354.801 Q159.294 350.657 162.233 348.25 Q165.196 345.842 170.335 345.842 Q175.497 345.842 178.437 348.25 Q181.377 350.657 181.377 354.801 Q181.377 357.764 179.687 359.824 Q178.02 361.884 175.034 362.625 Q178.414 363.412 180.289 365.703 Q182.187 367.995 182.187 371.305 Q182.187 376.328 179.108 379.013 Q176.053 381.699 170.335 381.699 Q164.618 381.699 161.539 379.013 Q158.484 376.328 158.484 371.305 Q158.484 367.995 160.382 365.703 Q162.28 363.412 165.659 362.625 M163.946 355.24 Q163.946 357.926 165.613 359.43 Q167.303 360.935 170.335 360.935 Q173.345 360.935 175.034 359.43 Q176.747 357.926 176.747 355.24 Q176.747 352.555 175.034 351.051 Q173.345 349.546 170.335 349.546 Q167.303 349.546 165.613 351.051 Q163.946 352.555 163.946 355.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M125.197 195.635 Q121.586 195.635 119.757 199.2 Q117.951 202.741 117.951 209.871 Q117.951 216.977 119.757 220.542 Q121.586 224.084 125.197 224.084 Q128.831 224.084 130.636 220.542 Q132.465 216.977 132.465 209.871 Q132.465 202.741 130.636 199.2 Q128.831 195.635 125.197 195.635 M125.197 191.931 Q131.007 191.931 134.062 196.538 Q137.141 201.121 137.141 209.871 Q137.141 218.598 134.062 223.204 Q131.007 227.788 125.197 227.788 Q119.386 227.788 116.308 223.204 Q113.252 218.598 113.252 209.871 Q113.252 201.121 116.308 196.538 Q119.386 191.931 125.197 191.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M145.359 221.237 L150.243 221.237 L150.243 227.116 L145.359 227.116 L145.359 221.237 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M160.567 226.399 L160.567 222.139 Q162.326 222.973 164.132 223.413 Q165.937 223.852 167.673 223.852 Q172.303 223.852 174.733 220.751 Q177.187 217.626 177.534 211.283 Q176.192 213.274 174.132 214.339 Q172.071 215.403 169.571 215.403 Q164.386 215.403 161.354 212.278 Q158.345 209.13 158.345 203.69 Q158.345 198.366 161.493 195.149 Q164.641 191.931 169.872 191.931 Q175.868 191.931 179.016 196.538 Q182.187 201.121 182.187 209.871 Q182.187 218.042 178.298 222.927 Q174.432 227.788 167.882 227.788 Q166.122 227.788 164.317 227.44 Q162.511 227.093 160.567 226.399 M169.872 211.746 Q173.02 211.746 174.849 209.593 Q176.701 207.44 176.701 203.69 Q176.701 199.964 174.849 197.811 Q173.02 195.635 169.872 195.635 Q166.724 195.635 164.872 197.811 Q163.044 199.964 163.044 203.69 Q163.044 207.44 164.872 209.593 Q166.724 211.746 169.872 211.746 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M115.822 69.2701 L123.461 69.2701 L123.461 42.9045 L115.15 44.5711 L115.15 40.3119 L123.414 38.6453 L128.09 38.6453 L128.09 69.2701 L135.729 69.2701 L135.729 73.2053 L115.822 73.2053 L115.822 69.2701 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M145.173 67.3257 L150.058 67.3257 L150.058 73.2053 L145.173 73.2053 L145.173 67.3257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M170.243 41.7239 Q166.632 41.7239 164.803 45.2887 Q162.997 48.8304 162.997 55.96 Q162.997 63.0664 164.803 66.6312 Q166.632 70.1729 170.243 70.1729 Q173.877 70.1729 175.682 66.6312 Q177.511 63.0664 177.511 55.96 Q177.511 48.8304 175.682 45.2887 Q173.877 41.7239 170.243 41.7239 M170.243 38.0203 Q176.053 38.0203 179.108 42.6267 Q182.187 47.21 182.187 55.96 Q182.187 64.6868 179.108 69.2932 Q176.053 73.8765 170.243 73.8765 Q164.433 73.8765 161.354 69.2932 Q158.298 64.6868 158.298 55.96 Q158.298 47.21 161.354 42.6267 Q164.433 38.0203 170.243 38.0203 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip282)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,283.806 325.239,480.032 357.991,572.242 390.744,568.359 423.496,678.92 456.249,635.049 489.001,678.589 521.754,718.403 554.507,758.1 \n",
       "  587.259,774.518 652.764,838.009 718.269,897.774 783.775,882.975 849.28,912.759 947.537,912.511 1045.8,944.857 1176.81,947.38 1307.82,961.009 1471.58,996.152 \n",
       "  1668.09,1005.96 1864.61,985.494 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ],
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip250\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip251\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip252\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,979.391 1912.76,979.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,825.48 1912.76,825.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,671.569 1912.76,671.569 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,517.658 1912.76,517.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,363.747 1912.76,363.747 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,209.836 1912.76,209.836 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,55.9253 1912.76,55.9253 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,979.391 230.485,979.391 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,825.48 230.485,825.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,671.569 230.485,671.569 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,517.658 230.485,517.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,363.747 230.485,363.747 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,209.836 230.485,209.836 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,55.9253 230.485,55.9253 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M124.525 965.19 Q120.914 965.19 119.086 968.755 Q117.28 972.297 117.28 979.426 Q117.28 986.533 119.086 990.097 Q120.914 993.639 124.525 993.639 Q128.16 993.639 129.965 990.097 Q131.794 986.533 131.794 979.426 Q131.794 972.297 129.965 968.755 Q128.16 965.19 124.525 965.19 M124.525 961.486 Q130.336 961.486 133.391 966.093 Q136.47 970.676 136.47 979.426 Q136.47 988.153 133.391 992.759 Q130.336 997.343 124.525 997.343 Q118.715 997.343 115.637 992.759 Q112.581 988.153 112.581 979.426 Q112.581 970.676 115.637 966.093 Q118.715 961.486 124.525 961.486 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M144.687 990.792 L149.572 990.792 L149.572 996.671 L144.687 996.671 L144.687 990.792 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M172.604 966.186 L160.798 984.635 L172.604 984.635 L172.604 966.186 M171.377 962.111 L177.257 962.111 L177.257 984.635 L182.187 984.635 L182.187 988.523 L177.257 988.523 L177.257 996.671 L172.604 996.671 L172.604 988.523 L157.002 988.523 L157.002 984.01 L171.377 962.111 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M126.007 811.279 Q122.396 811.279 120.567 814.844 Q118.761 818.386 118.761 825.515 Q118.761 832.622 120.567 836.186 Q122.396 839.728 126.007 839.728 Q129.641 839.728 131.447 836.186 Q133.275 832.622 133.275 825.515 Q133.275 818.386 131.447 814.844 Q129.641 811.279 126.007 811.279 M126.007 807.575 Q131.817 807.575 134.873 812.182 Q137.951 816.765 137.951 825.515 Q137.951 834.242 134.873 838.848 Q131.817 843.432 126.007 843.432 Q120.197 843.432 117.118 838.848 Q114.062 834.242 114.062 825.515 Q114.062 816.765 117.118 812.182 Q120.197 807.575 126.007 807.575 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M146.169 836.881 L151.053 836.881 L151.053 842.76 L146.169 842.76 L146.169 836.881 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M161.284 808.2 L179.641 808.2 L179.641 812.136 L165.567 812.136 L165.567 820.608 Q166.585 820.261 167.604 820.099 Q168.622 819.913 169.641 819.913 Q175.428 819.913 178.807 823.085 Q182.187 826.256 182.187 831.673 Q182.187 837.251 178.715 840.353 Q175.243 843.432 168.923 843.432 Q166.747 843.432 164.479 843.061 Q162.233 842.691 159.826 841.95 L159.826 837.251 Q161.909 838.385 164.132 838.941 Q166.354 839.497 168.831 839.497 Q172.835 839.497 175.173 837.39 Q177.511 835.284 177.511 831.673 Q177.511 828.061 175.173 825.955 Q172.835 823.849 168.831 823.849 Q166.956 823.849 165.081 824.265 Q163.229 824.682 161.284 825.561 L161.284 808.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M124.849 657.368 Q121.238 657.368 119.41 660.933 Q117.604 664.475 117.604 671.604 Q117.604 678.711 119.41 682.275 Q121.238 685.817 124.849 685.817 Q128.484 685.817 130.289 682.275 Q132.118 678.711 132.118 671.604 Q132.118 664.475 130.289 660.933 Q128.484 657.368 124.849 657.368 M124.849 653.664 Q130.66 653.664 133.715 658.271 Q136.794 662.854 136.794 671.604 Q136.794 680.331 133.715 684.937 Q130.66 689.521 124.849 689.521 Q119.039 689.521 115.961 684.937 Q112.905 680.331 112.905 671.604 Q112.905 662.854 115.961 658.271 Q119.039 653.664 124.849 653.664 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M145.011 682.97 L149.896 682.97 L149.896 688.849 L145.011 688.849 L145.011 682.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M170.659 669.706 Q167.511 669.706 165.659 671.859 Q163.831 674.012 163.831 677.762 Q163.831 681.488 165.659 683.664 Q167.511 685.817 170.659 685.817 Q173.807 685.817 175.636 683.664 Q177.488 681.488 177.488 677.762 Q177.488 674.012 175.636 671.859 Q173.807 669.706 170.659 669.706 M179.942 655.053 L179.942 659.313 Q178.182 658.479 176.377 658.039 Q174.595 657.6 172.835 657.6 Q168.206 657.6 165.752 660.725 Q163.321 663.85 162.974 670.169 Q164.34 668.155 166.4 667.09 Q168.46 666.002 170.937 666.002 Q176.145 666.002 179.155 669.174 Q182.187 672.322 182.187 677.762 Q182.187 683.086 179.039 686.303 Q175.891 689.521 170.659 689.521 Q164.664 689.521 161.493 684.937 Q158.321 680.331 158.321 671.604 Q158.321 663.41 162.21 658.549 Q166.099 653.664 172.65 653.664 Q174.409 653.664 176.192 654.012 Q177.997 654.359 179.942 655.053 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M125.914 503.457 Q122.303 503.457 120.474 507.022 Q118.669 510.564 118.669 517.693 Q118.669 524.8 120.474 528.364 Q122.303 531.906 125.914 531.906 Q129.548 531.906 131.354 528.364 Q133.183 524.8 133.183 517.693 Q133.183 510.564 131.354 507.022 Q129.548 503.457 125.914 503.457 M125.914 499.753 Q131.724 499.753 134.78 504.36 Q137.859 508.943 137.859 517.693 Q137.859 526.42 134.78 531.026 Q131.724 535.61 125.914 535.61 Q120.104 535.61 117.025 531.026 Q113.97 526.42 113.97 517.693 Q113.97 508.943 117.025 504.36 Q120.104 499.753 125.914 499.753 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M146.076 529.059 L150.96 529.059 L150.96 534.938 L146.076 534.938 L146.076 529.059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M159.965 500.378 L182.187 500.378 L182.187 502.369 L169.641 534.938 L164.757 534.938 L176.562 504.314 L159.965 504.314 L159.965 500.378 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M125.104 349.546 Q121.493 349.546 119.664 353.111 Q117.859 356.652 117.859 363.782 Q117.859 370.888 119.664 374.453 Q121.493 377.995 125.104 377.995 Q128.738 377.995 130.544 374.453 Q132.373 370.888 132.373 363.782 Q132.373 356.652 130.544 353.111 Q128.738 349.546 125.104 349.546 M125.104 345.842 Q130.914 345.842 133.97 350.449 Q137.048 355.032 137.048 363.782 Q137.048 372.509 133.97 377.115 Q130.914 381.699 125.104 381.699 Q119.294 381.699 116.215 377.115 Q113.16 372.509 113.16 363.782 Q113.16 355.032 116.215 350.449 Q119.294 345.842 125.104 345.842 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M145.266 375.148 L150.15 375.148 L150.15 381.027 L145.266 381.027 L145.266 375.148 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M170.335 364.615 Q167.002 364.615 165.081 366.398 Q163.183 368.18 163.183 371.305 Q163.183 374.43 165.081 376.213 Q167.002 377.995 170.335 377.995 Q173.669 377.995 175.59 376.213 Q177.511 374.407 177.511 371.305 Q177.511 368.18 175.59 366.398 Q173.692 364.615 170.335 364.615 M165.659 362.625 Q162.65 361.884 160.96 359.824 Q159.294 357.764 159.294 354.801 Q159.294 350.657 162.233 348.25 Q165.196 345.842 170.335 345.842 Q175.497 345.842 178.437 348.25 Q181.377 350.657 181.377 354.801 Q181.377 357.764 179.687 359.824 Q178.02 361.884 175.034 362.625 Q178.414 363.412 180.289 365.703 Q182.187 367.995 182.187 371.305 Q182.187 376.328 179.108 379.013 Q176.053 381.699 170.335 381.699 Q164.618 381.699 161.539 379.013 Q158.484 376.328 158.484 371.305 Q158.484 367.995 160.382 365.703 Q162.28 363.412 165.659 362.625 M163.946 355.24 Q163.946 357.926 165.613 359.43 Q167.303 360.935 170.335 360.935 Q173.345 360.935 175.034 359.43 Q176.747 357.926 176.747 355.24 Q176.747 352.555 175.034 351.051 Q173.345 349.546 170.335 349.546 Q167.303 349.546 165.613 351.051 Q163.946 352.555 163.946 355.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M125.197 195.635 Q121.586 195.635 119.757 199.2 Q117.951 202.741 117.951 209.871 Q117.951 216.977 119.757 220.542 Q121.586 224.084 125.197 224.084 Q128.831 224.084 130.636 220.542 Q132.465 216.977 132.465 209.871 Q132.465 202.741 130.636 199.2 Q128.831 195.635 125.197 195.635 M125.197 191.931 Q131.007 191.931 134.062 196.538 Q137.141 201.121 137.141 209.871 Q137.141 218.598 134.062 223.204 Q131.007 227.788 125.197 227.788 Q119.386 227.788 116.308 223.204 Q113.252 218.598 113.252 209.871 Q113.252 201.121 116.308 196.538 Q119.386 191.931 125.197 191.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M145.359 221.237 L150.243 221.237 L150.243 227.116 L145.359 227.116 L145.359 221.237 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M160.567 226.399 L160.567 222.139 Q162.326 222.973 164.132 223.413 Q165.937 223.852 167.673 223.852 Q172.303 223.852 174.733 220.751 Q177.187 217.626 177.534 211.283 Q176.192 213.274 174.132 214.339 Q172.071 215.403 169.571 215.403 Q164.386 215.403 161.354 212.278 Q158.345 209.13 158.345 203.69 Q158.345 198.366 161.493 195.149 Q164.641 191.931 169.872 191.931 Q175.868 191.931 179.016 196.538 Q182.187 201.121 182.187 209.871 Q182.187 218.042 178.298 222.927 Q174.432 227.788 167.882 227.788 Q166.122 227.788 164.317 227.44 Q162.511 227.093 160.567 226.399 M169.872 211.746 Q173.02 211.746 174.849 209.593 Q176.701 207.44 176.701 203.69 Q176.701 199.964 174.849 197.811 Q173.02 195.635 169.872 195.635 Q166.724 195.635 164.872 197.811 Q163.044 199.964 163.044 203.69 Q163.044 207.44 164.872 209.593 Q166.724 211.746 169.872 211.746 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M115.822 69.2701 L123.461 69.2701 L123.461 42.9045 L115.15 44.5711 L115.15 40.3119 L123.414 38.6453 L128.09 38.6453 L128.09 69.2701 L135.729 69.2701 L135.729 73.2053 L115.822 73.2053 L115.822 69.2701 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M145.173 67.3257 L150.058 67.3257 L150.058 73.2053 L145.173 73.2053 L145.173 67.3257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M170.243 41.7239 Q166.632 41.7239 164.803 45.2887 Q162.997 48.8304 162.997 55.96 Q162.997 63.0664 164.803 66.6312 Q166.632 70.1729 170.243 70.1729 Q173.877 70.1729 175.682 66.6312 Q177.511 63.0664 177.511 55.96 Q177.511 48.8304 175.682 45.2887 Q173.877 41.7239 170.243 41.7239 M170.243 38.0203 Q176.053 38.0203 179.108 42.6267 Q182.187 47.21 182.187 55.96 Q182.187 64.6868 179.108 69.2932 Q176.053 73.8765 170.243 73.8765 Q164.433 73.8765 161.354 69.2932 Q158.298 64.6868 158.298 55.96 Q158.298 47.21 161.354 42.6267 Q164.433 38.0203 170.243 38.0203 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip252)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,283.806 325.239,480.032 357.991,572.242 390.744,568.359 423.496,678.92 456.249,635.049 489.001,678.589 521.754,718.403 554.507,758.1 \n",
       "  587.259,774.518 652.764,838.009 718.269,897.774 783.775,882.975 849.28,912.759 947.537,912.511 1045.8,944.857 1176.81,947.38 1307.82,961.009 1471.58,996.152 \n",
       "  1668.09,1005.96 1864.61,985.494 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"big\" (2/3)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": 50
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10-element Vector{Int64}:\n 8\n 4\n 2\n 1\n 1\n 2\n 2\n 4\n 1\n 2"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬────────────┬────────────┬──────────────────────────────────┐\n",
      "│ a     │ b          │ c          │ d                                │\n",
      "│ Int64 │ Float64    │ Float64    │ CategoricalValue{String, UInt32} │\n",
      "│ Count │ Continuous │ Continuous │ OrderedFactor{2}                 │\n",
      "├───────┼────────────┼────────────┼──────────────────────────────────┤\n",
      "│ 1     │ 0.131905   │ 0.711407   │ male                             │\n",
      "│ 2     │ 0.587952   │ 0.017719   │ female                           │\n",
      "│ 3     │ 0.28426    │ 0.351781   │ female                           │\n",
      "│ 4     │ 0.858723   │ 0.570438   │ male                             │\n",
      "└───────┴────────────┴────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous);"
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Int64}:\n 1\n 2\n 3\n 4"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": 54
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┐\n",
      "│ b          │ c          │\n",
      "│ Float64    │ Float64    │\n",
      "│ Continuous │ Continuous │\n",
      "├────────────┼────────────┤\n",
      "│ 0.131905   │ 0.711407   │\n",
      "│ 0.587952   │ 0.017719   │\n",
      "│ 0.28426    │ 0.351781   │\n",
      "│ 0.858723   │ 0.570438   │\n",
      "└────────────┴────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": 55
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"male\"\n \"female\"\n \"female\"\n \"male\""
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬──────────────────┬──────────────────────────────────\n│\u001b[22m names                   \u001b[0m│\u001b[22m scitypes         \u001b[0m│\u001b[22m types                          \u001b[0m ⋯\n├─────────────────────────┼──────────────────┼──────────────────────────────────\n│ surgery                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ age                     │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ rectal_temperature      │ Continuous       │ Float64                         ⋯\n│ pulse                   │ Continuous       │ Float64                         ⋯\n│ respiratory_rate        │ Continuous       │ Float64                         ⋯\n│ temperature_extremities │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ mucous_membranes        │ Multiclass{6}    │ CategoricalValue{Int64, UInt32} ⋯\n│ capillary_refill_time   │ Multiclass{3}    │ CategoricalValue{Int64, UInt32} ⋯\n│ pain                    │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} ⋯\n│ peristalsis             │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ abdominal_distension    │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ packed_cell_volume      │ Continuous       │ Float64                         ⋯\n│ total_protein           │ Continuous       │ Float64                         ⋯\n│ outcome                 │ Multiclass{3}    │ CategoricalValue{Int64, UInt32} ⋯\n│ surgical_lesion         │ OrderedFactor{2} │ CategoricalValue{Int64, UInt32} ⋯\n│ cp_data                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n└─────────────────────────┴──────────────────┴──────────────────────────────────\n"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
