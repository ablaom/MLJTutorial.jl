{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.7** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3 - Transformers and Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unsupervised models, which receive no target `y` during training,\n",
    "always have a `transform` operation. They sometimes also support an\n",
    "`inverse_transform` operation, with obvious meaning, and sometimes\n",
    "support a `predict` operation (see the clustering example discussed\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Transformers-that-also-predict-1)).\n",
    "Otherwise, they are handled much like supervised models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a simple standardization example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "\n",
    "x = rand(100);\n",
    "@show mean(x) std(x);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Standardizer() # a built-in model\n",
    "mach = machine(model, x)\n",
    "fit!(mach)\n",
    "xhat = transform(mach, x);\n",
    "@show mean(xhat) std(xhat);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This particular model has an `inverse_transform`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "inverse_transform(mach, xhat) â‰ˆ x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Re-encoding the King County House data as continuous"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For further illustrations of transformers, let's re-encode *all* of the\n",
    "King County House input features (see [Ex\n",
    "3](#exercise-3-fixing-scitypes-in-a-table)) into a set of `Continuous`\n",
    "features. We do this with the `ContinuousEncoder` model, which, by\n",
    "default, will:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- one-hot encode all `Multiclass` features\n",
    "- coerce all `OrderedFactor` features to `Continuous` ones\n",
    "- coerce all `Count` features to `Continuous` ones (there aren't any)\n",
    "- drop any remaining non-Continuous features (none of these either)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we reload the data and fix the scitypes (Exercise 3):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "import DataFrames\n",
    "house_csv = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                        \"MachineLearningInJulia2020/for-MLJ-version-0.16/\"*\n",
    "                        \"data/house.csv\");\n",
    "house = DataFrames.DataFrame(house_csv)\n",
    "coerce!(house, autotype(house_csv));\n",
    "coerce!(house, Count => Continuous, :zipcode => Multiclass);\n",
    "schema(house)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(house, ==(:price), rng=123);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate the unsupervised model (transformer):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "encoder = ContinuousEncoder() # a built-in model; no need to @load it"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bind the model to the data and fit!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(encoder, X) |> fit!;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform and inspect the result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xcont = transform(mach, X);\n",
    "schema(Xcont)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to list all of MLJ's unsupervised models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "models(m->!m.is_supervised)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some commonly used ones are built-in (do not require `@load`ing):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model type                  | does what?\n",
    "----------------------------|----------------------------------------------\n",
    "ContinuousEncoder | transform input table to a table of `Continuous` features (see above)\n",
    "FeatureSelector | retain or dump selected features\n",
    "FillImputer | impute missing values\n",
    "OneHotEncoder | one-hot encoder `Multiclass` (and optionally `OrderedFactor`) features\n",
    "Standardizer | standardize (whiten) a vector or all `Continuous` features of a table\n",
    "UnivariateBoxCoxTransformer | apply a learned Box-Cox transformation to a vector\n",
    "UnivariateDiscretizer | discretize a `Continuous` vector, and hence render its elscitypw `OrderedFactor`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to \"dynamic\" transformers (ones that learn something\n",
    "from the data and must be `fit!`) users can wrap ordinary functions\n",
    "as transformers, and such *static* transformers can depend on\n",
    "parameters, like the dynamic ones. See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Static-transformers-1)\n",
    "for how to define your own static transformers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipelines"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(schema(Xcont).names)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's suppose that additionally we'd like to reduce the dimension of\n",
    "our data.  A model that will do this is `PCA` from\n",
    "`MultivariateStats.jl`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "PCA = @load PCA\n",
    "reducer = PCA()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, rather simply repeating the work-flow above, applying the new\n",
    "transformation to `Xcont`, we can combine both the encoding and the\n",
    "dimension-reducing models into a single model, known as a\n",
    "*pipeline*. While MLJ offers a powerful interface for composing\n",
    "models in a variety of ways, we'll stick to these simplest class of\n",
    "composite models for now. The simplest way to construct a pipeline\n",
    "is using the Julia's `|>` syntax:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe = encoder |> reducer"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the model `pipe` has other models as hyperparameters\n",
    "(with names automatically generated based on the mode type\n",
    "name). The hyperparameters of the component models are are now\n",
    "*nested*, but we can still access them:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show pipe.pca.pratio\n",
    "pipe.pca.pratio = 0.85"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline model behaves like any other transformer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(pipe, X)\n",
    "fit!(mach)\n",
    "Xsmall = transform(mach, X)\n",
    "schema(Xsmall)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Want to combine this pre-processing with ridge regression?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels\n",
    "rgs = RidgeRegressor()\n",
    "pipe2 = pipe |> rgs"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now our pipeline is a supervised model, instead of a transformer,\n",
    "whose performance we can evaluate:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(pipe2, X, y)\n",
    "evaluate!(mach, measure=mae, resampling=Holdout()) # CV(nfolds=6) is default"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training of composite models is \"smart\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now notice what happens if we train on all the data, then change a\n",
    "regressor hyper-parameter and retrain:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe2.ridge_regressor.lambda = 0.1\n",
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second time only the ridge regressor is retrained!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mutate a hyper-parameter of the `PCA` model and every model except\n",
    "the `ContinuousEncoder` (which comes before it will be retrained):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe2.pca.pratio = 0.9999\n",
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspecting composite models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dot syntax used above to change the values of *nested*\n",
    "hyper-parameters is also useful when inspecting the learned\n",
    "parameters and report generated when training a composite model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fitted_params(mach).ridge_regressor"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "report(mach).pca"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incorporating target transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, suppose that instead of using the raw `:price` as the training\n",
    "target, we want to use the log-price (a common practice in dealing\n",
    "with house price data). However, suppose that we still want to\n",
    "report final *predictions* on the original linear scale (and use\n",
    "these for evaluation purposes). Then we wrap our supervised model\n",
    "using `TransformedTargetModel`, which has to key-word arguments\n",
    "`target` and `inverse`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we'll overload `log` and `exp` for broadcasting:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Base.log(v::AbstractArray) = log.(v)\n",
    "Base.exp(v::AbstractArray) = exp.(v)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for the new pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rgs_log = TransformedTargetModel(rgs, target=log, inverse=exp)\n",
    "\n",
    "pipe3 = pipe |> rgs_log\n",
    "mach = machine(pipe3, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLJ will also allow you to insert *learned* target\n",
    "transformations. For example, we might want to apply\n",
    "`Standardizer()` to the target, to standardize it, or\n",
    "`UnivariateBoxCoxTransformer()` to make it look Gaussian. Then\n",
    "instead of specifying a *function* for `target`, we specify a\n",
    "unsupervised *model* (or model type). One does not specify `inverse`\n",
    "because only models implementing `inverse_transform` are\n",
    "allowed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see which of these two options results in a better outcome:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "box = UnivariateBoxCoxTransformer(n=20)\n",
    "stand = Standardizer()\n",
    "\n",
    "rgs_box = TransformedTargetModel(rgs, target=box)\n",
    "pipe4 = pipe |> rgs_box\n",
    "mach = machine(pipe4, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe4.transformed_target_model_deterministic.target = stand\n",
    "evaluate!(mach, measure=mae)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Transformers and other unsupervised models](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/)\n",
    "    - [Linear pipelines](https://alan-turing-institute.github.io/MLJ.jl/dev/linear_pipelines/#Linear-Pipelines)\n",
    "- From Data Science Tutorials:\n",
    "    - [Composing models](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/composing-models/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider again the Horse Colic classification problem considered in\n",
    "Exercise 6, but with all features, `Finite` and `Infinite`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "\n",
    "y, X = unpack(horse, ==(:outcome));\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Define a pipeline that:\n",
    "- uses `Standardizer` to ensure that features that are already\n",
    "  continuous are centered at zero and have unit variance\n",
    "- re-encodes the full set of features as `Continuous`, using\n",
    "  `ContinuousEncoder`\n",
    "- uses the `KMeans` clustering model from `Clustering.jl`\n",
    "  to reduce the dimension of the feature space to `k=10`.\n",
    "- trains a `EvoTreeClassifier` (a gradient tree boosting\n",
    "  algorithm in `EvoTrees.jl`) on the reduced data, using\n",
    "  `nrounds=50` and default values for the other\n",
    "   hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Evaluate the pipeline on all data, using 6-fold cross-validation\n",
    "and `cross_entropy` loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "&star;(c) Plot a learning curve which examines the effect on this loss\n",
    "as the tree booster parameter `max_depth` varies from 2 to 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-4-tuning-hyper-parameters'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
