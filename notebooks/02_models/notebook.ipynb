{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.6.5\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env/Project.toml`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n  Marshall \u001b[1mPlease cite\u001b[22m:\n\n  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n  the pattern recognition literature. Fisher's paper is a classic in the field\n  and is referenced frequently to this day. (See Duda & Hart, for example.)\n  The data set contains 3 classes of 50 instances each, where each class\n  refers to a type of iris plant. One class is linearly separable from the\n  other 2; the latter are NOT linearly separable from each other.\n\n  Predicted attribute: class of iris plant. This is an exceedingly simple\n  domain.\n\n\u001b[1m  Attribute Information:\u001b[22m\n\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n\n\u001b[36m  1. sepal length in cm\u001b[39m\n\u001b[36m  2. sepal width in cm\u001b[39m\n\u001b[36m  3. petal length in cm\u001b[39m\n\u001b[36m  4. petal width in cm\u001b[39m\n\u001b[36m  5. class: \u001b[39m\n\u001b[36m     -- Iris Setosa\u001b[39m\n\u001b[36m     -- Iris Versicolour\u001b[39m\n\u001b[36m     -- Iris Virginica\u001b[39m",
      "text/markdown": "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n\n**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n\n### Attribute Information:\n\n```\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n   -- Iris Setosa\n   -- Iris Versicolour\n   -- Iris Virginica\n```\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n─────┼───────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n   4 │         4.6         3.1          1.5         0.2  Iris-setosa",
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "import DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬───────────────┬──────────────────────────────────┐\n│\u001b[22m names       \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                            \u001b[0m│\n├─────────────┼───────────────┼──────────────────────────────────┤\n│ sepallength │ Continuous    │ Float64                          │\n│ sepalwidth  │ Continuous    │ Float64                          │\n│ petallength │ Continuous    │ Float64                          │\n│ petalwidth  │ Continuous    │ Float64                          │\n│ class       │ Multiclass{3} │ CategoricalValue{String, UInt32} │\n└─────────────┴───────────────┴──────────────────────────────────┘\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "These look fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We can randomize the data at\n",
    "the same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{ScientificTypesBase.Multiclass{3}} (alias for AbstractArray{ScientificTypesBase.Multiclass{3}, 1})"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This puts the `:class` column into a vector `y`, and all remaining\n",
    "columns into a table `X`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[36m  unpack(table, f1, f2, ... fk;\u001b[39m\n\u001b[36m         wrap_singles=false,\u001b[39m\n\u001b[36m         shuffle=false,\u001b[39m\n\u001b[36m         rng::Union{AbstractRNG,Int,Nothing}=nothing,\u001b[39m\n\u001b[36m         coerce_options...)\u001b[39m\n\n  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables or\n  vectors by making column selections determined by the predicates \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m,\n  ..., \u001b[36mfk\u001b[39m. Selection from the column names is without replacement. A \u001b[4mpredicate\u001b[24m\n  is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column\n  \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m.\n\n  Returns a tuple of tables/vectors with length one greater than the number of\n  supplied predicates, with the last component including all previously\n  unselected columns.\n\n\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n\u001b[36m  2×4 DataFrame\u001b[39m\n\u001b[36m   Row │ x      y     z        w\u001b[39m\n\u001b[36m       │ Int64  Char  Float64  String\u001b[39m\n\u001b[36m  ─────┼──────────────────────────────\u001b[39m\n\u001b[36m     1 │     1  a        10.0  A\u001b[39m\n\u001b[36m     2 │     2  b        20.0  B\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  Z, XY, W = unpack(table, ==(:z), !=(:w))\u001b[39m\n\u001b[36m  julia> Z\u001b[39m\n\u001b[36m  2-element Vector{Float64}:\u001b[39m\n\u001b[36m   10.0\u001b[39m\n\u001b[36m   20.0\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> XY\u001b[39m\n\u001b[36m  2×2 DataFrame\u001b[39m\n\u001b[36m   Row │ x      y\u001b[39m\n\u001b[36m       │ Int64  Char\u001b[39m\n\u001b[36m  ─────┼─────────────\u001b[39m\n\u001b[36m     1 │     1  a\u001b[39m\n\u001b[36m     2 │     2  b\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> W  # the column(s) left over\u001b[39m\n\u001b[36m  2-element Vector{String}:\u001b[39m\n\u001b[36m   \"A\"\u001b[39m\n\u001b[36m   \"B\"\u001b[39m\n\n  Whenever a returned table contains a single column, it is converted to a\n  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n\n  If \u001b[36mcoerce_options\u001b[39m are specified then \u001b[36mtable\u001b[39m is first replaced with\n  \u001b[36mcoerce(table, coerce_options)\u001b[39m. See \u001b[36mScientificTypes.coerce\u001b[39m for details.\n\n  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n  \u001b[36mshuffle=true\u001b[39m is implicit.",
      "text/markdown": "```\nunpack(table, f1, f2, ... fk;\n       wrap_singles=false,\n       shuffle=false,\n       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n       coerce_options...)\n```\n\nHorizontally split any Tables.jl compatible `table` into smaller tables or vectors by making column selections determined by the predicates `f1`, `f2`, ..., `fk`. Selection from the column names is without replacement. A *predicate* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`.\n\nReturns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n\n```\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n2×4 DataFrame\n Row │ x      y     z        w\n     │ Int64  Char  Float64  String\n─────┼──────────────────────────────\n   1 │     1  a        10.0  A\n   2 │     2  b        20.0  B\n\nZ, XY, W = unpack(table, ==(:z), !=(:w))\njulia> Z\n2-element Vector{Float64}:\n 10.0\n 20.0\n\njulia> XY\n2×2 DataFrame\n Row │ x      y\n     │ Int64  Char\n─────┼─────────────\n   1 │     1  a\n   2 │     2  b\n\njulia> W  # the column(s) left over\n2-element Vector{String}:\n \"A\"\n \"B\"\n```\n\nWhenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n\nIf `coerce_options` are specified then `table` is first replaced with `coerce(table, coerce_options)`. See [`ScientificTypes.coerce`](@ref) for details.\n\nIf `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "186-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = ARDRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BM25Transformer, package_name = MLJText, ... )\n ⋮\n (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )\n (name = XGBoostCount, package_name = XGBoost, ... )\n (name = XGBoostRegressor, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you already have an idea about the name of the model, you could\n",
    "search by string or regex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = LinearRegressor, package_name = GLM, ... )\n (name = LinearRegressor, package_name = MLJLinearModels, ... )\n (name = LinearRegressor, package_name = MultivariateStats, ... )\n (name = LinearRegressor, package_name = ScikitLearn, ... )\n (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n (name = SVMLinearRegressor, package_name = ScikitLearn, ... )"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "some_models = models(\"LinearRegressor\")"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not\n",
    "yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mLinear regressor (OLS) with a Normal model.\u001b[39m\n\u001b[35m→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\u001b[39m\n\u001b[35m→ do `@load LinearRegressor pkg=\"GLM\"` to use the model.\u001b[39m\n\u001b[35m→ do `?LinearRegressor` for documentation.\u001b[39m\n(name = \"LinearRegressor\",\n package_name = \"GLM\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (),\n docstring =\n     \"Linear regressor (OLS) with a Normal model.\\n→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\\n→ do `@load LinearRegressor pkg=\\\"GLM\\\"` to use the model.\\n→ do `?LinearRegressor` for documentation.\",\n fit_data_scitype =\n     Tuple{ScientificTypesBase.Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:ScientificTypesBase.Continuous), AbstractVector{ScientificTypesBase.Continuous}},\n hyperparameter_ranges = (nothing, nothing, nothing),\n hyperparameter_types = (\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\"),\n hyperparameters = (:fit_intercept, :allowrankdeficient, :offsetcol),\n implemented_methods = [:fit, :fitted_params, :predict, :predict_mean],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = nothing,\n load_path = \"MLJGLMInterface.LinearRegressor\",\n package_license = \"MIT\",\n package_url = \"https://github.com/JuliaStats/GLM.jl\",\n package_uuid = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\",\n predict_scitype =\n     AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = false,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype =\n     ScientificTypesBase.Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:ScientificTypesBase.Continuous),\n target_scitype = AbstractVector{ScientificTypesBase.Continuous},\n output_scitype = ScientificTypesBase.Unknown)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "meta = some_models[1]"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{ScientificTypesBase.Continuous} (alias for AbstractArray{ScientificTypesBase.Continuous, 1})"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "false"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n ⋮\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n ⋮\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n    builder = Short(\n            n_hidden = 0,\n            dropout = 0.5,\n            σ = NNlib.σ),\n    finaliser = NNlib.softmax,\n    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n    loss = Flux.Losses.crossentropy,\n    epochs = 10,\n    batch_size = 1,\n    lambda = 0.0,\n    alpha = 0.0,\n    rng = Random._GLOBAL_RNG(),\n    optimiser_changes_trigger_retraining = false,\n    acceleration = ComputationalResources.CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n(name = \"NeuralNetworkClassifier\",\n package_name = \"MLJFlux\",\n is_supervised = true,\n abstract_type = MLJModelInterface.Probabilistic,\n deep_properties = (:optimiser, :builder),\n docstring =\n     \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n fit_data_scitype =\n     Tuple{ScientificTypesBase.Table{var\"#s28\"} where var\"#s28\"<:(AbstractVector{var\"#s29\"} where var\"#s29\"<:ScientificTypesBase.Continuous), AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite},\n hyperparameter_ranges = (nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing,\n                          nothing),\n hyperparameter_types = (\"MLJFlux.Short\",\n                         \"typeof(NNlib.softmax)\",\n                         \"Flux.Optimise.ADAM\",\n                         \"typeof(Flux.Losses.crossentropy)\",\n                         \"Int64\",\n                         \"Int64\",\n                         \"Float64\",\n                         \"Float64\",\n                         \"Union{Int64, Random.AbstractRNG}\",\n                         \"Bool\",\n                         \"ComputationalResources.AbstractResource\"),\n hyperparameters = (:builder,\n                    :finaliser,\n                    :optimiser,\n                    :loss,\n                    :epochs,\n                    :batch_size,\n                    :lambda,\n                    :alpha,\n                    :rng,\n                    :optimiser_changes_trigger_retraining,\n                    :acceleration),\n implemented_methods = [],\n inverse_transform_scitype = ScientificTypesBase.Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = :epochs,\n load_path = \"MLJFlux.NeuralNetworkClassifier\",\n package_license = \"MIT\",\n package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n predict_scitype =\n     AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = true,\n supports_weights = false,\n transform_scitype = ScientificTypesBase.Unknown,\n input_scitype =\n     ScientificTypesBase.Table{var\"#s28\"} where var\"#s28\"<:(AbstractVector{var\"#s29\"} where var\"#s29\"<:ScientificTypesBase.Continuous),\n target_scitype =\n     AbstractVector{var\"#s76\"} where var\"#s76\"<:ScientificTypesBase.Finite,\n output_scitype = ScientificTypesBase.Unknown)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @956 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @938 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(1:length(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.321\n",
      "[ Info: Loss is 1.301\n",
      "[ Info: Loss is 1.206\n",
      "[ Info: Loss is 1.15\n",
      "[ Info: Loss is 1.172\n",
      "[ Info: Loss is 1.131\n",
      "[ Info: Loss is 1.081\n",
      "[ Info: Loss is 1.096\n",
      "[ Info: Loss is 1.048\n",
      "[ Info: Loss is 1.043\n",
      "[ Info: Loss is 1.013\n",
      "[ Info: Loss is 0.9736\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @956 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @938 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element CategoricalDistributions.UnivariateFiniteArray{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64, 1}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.317, Iris-versicolor=>0.334, Iris-virginica=>0.349)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.288, Iris-versicolor=>0.336, Iris-virginica=>0.376)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.476, Iris-versicolor=>0.278, Iris-virginica=>0.246)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(training_losses = [1.3186817199364722, 1.3207108587466092, 1.3005753086031402, 1.205958742363388, 1.149801000499821, 1.172247503303348, 1.131396706025703, 1.0812020942600453, 1.0962750422948127, 1.0484586809735577, 1.0433204511887757, 1.0133012208995973, 0.9736099094546508],)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element CategoricalDistributions.UnivariateFiniteArray{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64, 1}:\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.278, Iris-versicolor=>0.34, Iris-virginica=>0.381)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.317, Iris-versicolor=>0.332, Iris-virginica=>0.351)\n UnivariateFinite{ScientificTypesBase.Multiclass{3}}(Iris-setosa=>0.285, Iris-versicolor=>0.334, Iris-virginica=>0.381)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net: 15%[===>                     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @969 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @146 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9744\n",
      "[ Info: Loss is 0.9294\n",
      "[ Info: Loss is 0.9764\n",
      "[ Info: Loss is 0.9147\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @956 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @938 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.8294\n",
      "[ Info: Loss is 0.8432\n",
      "[ Info: Loss is 0.7488\n",
      "[ Info: Loss is 0.7968\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @956 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @938 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.311\n",
      "[ Info: Loss is 1.187\n",
      "[ Info: Loss is 1.051\n",
      "[ Info: Loss is 0.9918\n",
      "[ Info: Loss is 1.003\n",
      "[ Info: Loss is 0.9559\n",
      "[ Info: Loss is 0.9413\n",
      "[ Info: Loss is 0.8889\n",
      "[ Info: Loss is 0.9215\n",
      "[ Info: Loss is 0.8469\n",
      "[ Info: Loss is 0.7858\n",
      "[ Info: Loss is 0.7537\n",
      "[ Info: Loss is 0.7544\n",
      "[ Info: Loss is 0.7388\n",
      "[ Info: Loss is 0.6828\n",
      "[ Info: Loss is 0.7419\n",
      "[ Info: Loss is 0.6675\n",
      "[ Info: Loss is 0.7611\n",
      "[ Info: Loss is 0.7981\n",
      "[ Info: Loss is 0.7338\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n  args: \n    1:\tSource @956 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @938 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 13%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 19%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 26%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 32%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 42%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 48%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 52%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 58%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 68%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 74%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 81%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 87%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 97%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   \u001b[1mUnivariateFinite{ScientificTypesBase.Multiclass{3}}\u001b[22m \n                   \u001b[90m┌                                        ┐\u001b[39m \n       \u001b[0mIris-setosa \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■■■\u001b[39m\u001b[0m 0.1604818119326164               \u001b[90m \u001b[39m \n   \u001b[0mIris-versicolor \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■■■■■■■■■■■■■■■■■\u001b[39m\u001b[0m 0.5460173449779772 \u001b[90m \u001b[39m \n    \u001b[0mIris-virginica \u001b[90m┤\u001b[39m\u001b[38;5;2m■■■■■■■■■■■\u001b[39m\u001b[0m 0.29350084308940644         \u001b[90m \u001b[39m \n                   \u001b[90m└                                        ┘\u001b[39m "
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":probabilistic"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: - In MLJ, a model that can predict probabilities (and\n",
    "not just point values) will do so by default.  - For most\n",
    "probabilistic predictors, the predicted object is a\n",
    "`Distributions.Distribution` object or a\n",
    "`CategoricalDistributions.UnivariateFinite` object (the case here)\n",
    "which all support the following methods: `rand`, `pdf`, `logpdf`;\n",
    "and, where appropriate: `mode`, `median` and `mean`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.29350084308940644"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-versicolor\""
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Float64}:\n 0.5460173449779772\n 0.5491752002635047\n 0.15305640888835056\n 0.41492792474967766"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": 36
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-versicolor\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4×3 Matrix{Float64}:\n 0.160482    0.546017  0.293501\n 0.0609427   0.549175  0.389882\n 0.792046    0.153056  0.0548975\n 0.00501513  0.414928  0.580057"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.446375792287265"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.022222222222222223"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "62-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type), T} where T<:Tuple}:\n (name = BrierLoss, instances = [brier_loss], ...)\n (name = BrierScore, instances = [brier_score], ...)\n (name = LPLoss, instances = [l1, l2], ...)\n (name = LogCoshLoss, instances = [log_cosh, log_cosh_loss], ...)\n (name = LogLoss, instances = [log_loss, cross_entropy], ...)\n (name = LogScore, instances = [log_score], ...)\n (name = SphericalScore, instances = [spherical_score], ...)\n (name = Accuracy, instances = [accuracy], ...)\n (name = AreaUnderCurve, instances = [area_under_curve, auc], ...)\n (name = BalancedAccuracy, instances = [balanced_accuracy, bacc, bac], ...)\n ⋮\n (name = SmoothedL1HingeLoss, instances = [smoothed_l1_hinge_loss], ...)\n (name = ZeroOneLoss, instances = [zero_one_loss], ...)\n (name = HuberLoss, instances = [huber_loss], ...)\n (name = L1EpsilonInsLoss, instances = [l1_epsilon_ins_loss], ...)\n (name = L2EpsilonInsLoss, instances = [l2_epsilon_ins_loss], ...)\n (name = LPDistLoss, instances = [lp_dist_loss], ...)\n (name = LogitDistLoss, instances = [logit_dist_loss], ...)\n (name = PeriodicLoss, instances = [periodic_loss], ...)\n (name = QuantileLoss, instances = [quantile_loss], ...)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬──────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold \u001b[0m│\n├────────────────────────────┼─────────────┼──────────────┼──────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.446       │ predict      │ [0.446]  │\n│ MisclassificationRate()    │ 0.0222      │ predict_mode │ [0.0222] │\n│ BrierScore()               │ -0.245      │ predict      │ [-0.245] │\n└────────────────────────────┴─────────────┴──────────────┴──────────┘\n"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:13\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:10\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:19\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold          \u001b[0m ⋯\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.409       │ predict      │ [0.405, 0.389, 0.4 ⋯\n│ MisclassificationRate()    │ 0.0333      │ predict_mode │ [0.04, 0.0, 0.04,  ⋯\n│ BrierScore()               │ -0.222      │ predict      │ [-0.227, -0.195, - ⋯\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 18 folds:  11%[==>                      ]  ETA: 0:00:52\u001b[K\rEvaluating over 18 folds:  17%[====>                    ]  ETA: 0:00:49\u001b[K\rEvaluating over 18 folds:  22%[=====>                   ]  ETA: 0:00:45\u001b[K\rEvaluating over 18 folds:  28%[======>                  ]  ETA: 0:00:42\u001b[K\rEvaluating over 18 folds:  33%[========>                ]  ETA: 0:00:39\u001b[K\rEvaluating over 18 folds:  39%[=========>               ]  ETA: 0:00:36\u001b[K\rEvaluating over 18 folds:  44%[===========>             ]  ETA: 0:00:32\u001b[K\rEvaluating over 18 folds:  50%[============>            ]  ETA: 0:00:29\u001b[K\rEvaluating over 18 folds:  56%[=============>           ]  ETA: 0:00:26\u001b[K\rEvaluating over 18 folds:  61%[===============>         ]  ETA: 0:00:23\u001b[K\rEvaluating over 18 folds:  67%[================>        ]  ETA: 0:00:19\u001b[K\rEvaluating over 18 folds:  72%[==================>      ]  ETA: 0:00:16\u001b[K\rEvaluating over 18 folds:  78%[===================>     ]  ETA: 0:00:13\u001b[K\rEvaluating over 18 folds:  83%[====================>    ]  ETA: 0:00:10\u001b[K\rEvaluating over 18 folds:  89%[======================>  ]  ETA: 0:00:06\u001b[K\rEvaluating over 18 folds:  94%[=======================> ]  ETA: 0:00:03\u001b[K\rEvaluating over 18 folds: 100%[=========================] Time: 0:00:58\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold          \u001b[0m ⋯\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.408       │ predict      │ [0.275, 0.352, 0.4 ⋯\n│ MisclassificationRate()    │ 0.0422      │ predict_mode │ [0.0, 0.0, 0.04, 0 ⋯\n│ BrierScore()               │ -0.229      │ predict      │ [-0.15, -0.187, -0 ⋯\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Creating subsamples from a subset of all rows. \n",
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:10\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:13\u001b[K\n",
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  4%[>                        ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net:  8%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 12%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 14%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 16%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 18%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 20%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 22%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 24%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 25%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 27%[======>                  ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 33%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 37%[=========>               ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 41%[==========>              ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 43%[==========>              ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 47%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 49%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 51%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 53%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 57%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 59%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 63%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 67%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 73%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 75%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 76%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 78%[===================>     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 80%[====================>    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:02\u001b[K\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5) on log10 scale"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log10)"
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "[ Info: Attempting to evaluate 22 models.\n",
      "\rEvaluating over 22 metamodels:   0%[>                        ]  ETA: N/A\u001b[K\rEvaluating over 22 metamodels:   5%[=>                       ]  ETA: 0:00:29\u001b[K\rEvaluating over 22 metamodels:   9%[==>                      ]  ETA: 0:00:21\u001b[K\rEvaluating over 22 metamodels:  14%[===>                     ]  ETA: 0:00:14\u001b[K\rEvaluating over 22 metamodels:  18%[====>                    ]  ETA: 0:00:10\u001b[K\rEvaluating over 22 metamodels:  23%[=====>                   ]  ETA: 0:00:08\u001b[K\rEvaluating over 22 metamodels:  27%[======>                  ]  ETA: 0:00:06\u001b[K\rEvaluating over 22 metamodels:  32%[=======>                 ]  ETA: 0:00:05\u001b[K\rEvaluating over 22 metamodels:  36%[=========>               ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  41%[==========>              ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  45%[===========>             ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  50%[============>            ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  55%[=============>           ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  59%[==============>          ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  64%[===============>         ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  68%[=================>       ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  73%[==================>      ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  77%[===================>     ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  82%[====================>    ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  86%[=====================>   ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  95%[=======================> ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels: 100%[=========================] Time: 0:00:04\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(parameter_name = \"epochs\",\n parameter_scale = :log10,\n parameter_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 17, 19, 22, 25, 29, 33, 38, 44, 50],\n measurements = [0.9988656690309341, 0.8973176543690112, 0.798917795431131, 0.6605465383551822, 0.6209922843935195, 0.5952099684287824, 0.5466142680953909, 0.5361523985562114, 0.5453282676327983, 0.5014204768584094  …  0.5044350747730423, 0.4831511755045931, 0.4657872759171208, 0.43647724757973805, 0.40526253866309037, 0.45896035485529685, 0.41389802582770646, 0.44506870060849946, 0.3780966524300838, 0.4446183646173391],)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Plot{Plots.GRBackend() n=1}",
      "image/png": "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",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip860\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip860)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip861\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip860)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip862\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip860)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,973.114 1912.76,973.114 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,823.173 1912.76,823.173 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,673.232 1912.76,673.232 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,523.291 1912.76,523.291 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,373.349 1912.76,373.349 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,223.408 1912.76,223.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip862)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,73.4669 1912.76,73.4669 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,973.114 230.485,973.114 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,823.173 230.485,823.173 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,673.232 230.485,673.232 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,523.291 230.485,523.291 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,373.349 230.485,373.349 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,223.408 230.485,223.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,73.4669 230.485,73.4669 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip860)\" d=\"M124.525 958.913 Q120.914 958.913 119.086 962.478 Q117.28 966.02 117.28 973.149 Q117.28 980.256 119.086 983.82 Q120.914 987.362 124.525 987.362 Q128.16 987.362 129.965 983.82 Q131.794 980.256 131.794 973.149 Q131.794 966.02 129.965 962.478 Q128.16 958.913 124.525 958.913 M124.525 955.209 Q130.336 955.209 133.391 959.816 Q136.47 964.399 136.47 973.149 Q136.47 981.876 133.391 986.482 Q130.336 991.066 124.525 991.066 Q118.715 991.066 115.637 986.482 Q112.581 981.876 112.581 973.149 Q112.581 964.399 115.637 959.816 Q118.715 955.209 124.525 955.209 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M144.687 984.515 L149.572 984.515 L149.572 990.394 L144.687 990.394 L144.687 984.515 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M172.604 959.908 L160.798 978.357 L172.604 978.357 L172.604 959.908 M171.377 955.834 L177.257 955.834 L177.257 978.357 L182.187 978.357 L182.187 982.246 L177.257 982.246 L177.257 990.394 L172.604 990.394 L172.604 982.246 L157.002 982.246 L157.002 977.732 L171.377 955.834 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M126.007 808.972 Q122.396 808.972 120.567 812.537 Q118.761 816.078 118.761 823.208 Q118.761 830.314 120.567 833.879 Q122.396 837.421 126.007 837.421 Q129.641 837.421 131.447 833.879 Q133.275 830.314 133.275 823.208 Q133.275 816.078 131.447 812.537 Q129.641 808.972 126.007 808.972 M126.007 805.268 Q131.817 805.268 134.873 809.875 Q137.951 814.458 137.951 823.208 Q137.951 831.935 134.873 836.541 Q131.817 841.124 126.007 841.124 Q120.197 841.124 117.118 836.541 Q114.062 831.935 114.062 823.208 Q114.062 814.458 117.118 809.875 Q120.197 805.268 126.007 805.268 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M146.169 834.574 L151.053 834.574 L151.053 840.453 L146.169 840.453 L146.169 834.574 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M161.284 805.893 L179.641 805.893 L179.641 809.828 L165.567 809.828 L165.567 818.301 Q166.585 817.953 167.604 817.791 Q168.622 817.606 169.641 817.606 Q175.428 817.606 178.807 820.777 Q182.187 823.949 182.187 829.365 Q182.187 834.944 178.715 838.046 Q175.243 841.124 168.923 841.124 Q166.747 841.124 164.479 840.754 Q162.233 840.384 159.826 839.643 L159.826 834.944 Q161.909 836.078 164.132 836.634 Q166.354 837.189 168.831 837.189 Q172.835 837.189 175.173 835.083 Q177.511 832.976 177.511 829.365 Q177.511 825.754 175.173 823.648 Q172.835 821.541 168.831 821.541 Q166.956 821.541 165.081 821.958 Q163.229 822.375 161.284 823.254 L161.284 805.893 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M124.849 659.031 Q121.238 659.031 119.41 662.595 Q117.604 666.137 117.604 673.267 Q117.604 680.373 119.41 683.938 Q121.238 687.48 124.849 687.48 Q128.484 687.48 130.289 683.938 Q132.118 680.373 132.118 673.267 Q132.118 666.137 130.289 662.595 Q128.484 659.031 124.849 659.031 M124.849 655.327 Q130.66 655.327 133.715 659.933 Q136.794 664.517 136.794 673.267 Q136.794 681.993 133.715 686.6 Q130.66 691.183 124.849 691.183 Q119.039 691.183 115.961 686.6 Q112.905 681.993 112.905 673.267 Q112.905 664.517 115.961 659.933 Q119.039 655.327 124.849 655.327 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M145.011 684.632 L149.896 684.632 L149.896 690.512 L145.011 690.512 L145.011 684.632 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M170.659 671.369 Q167.511 671.369 165.659 673.521 Q163.831 675.674 163.831 679.424 Q163.831 683.151 165.659 685.327 Q167.511 687.48 170.659 687.48 Q173.807 687.48 175.636 685.327 Q177.488 683.151 177.488 679.424 Q177.488 675.674 175.636 673.521 Q173.807 671.369 170.659 671.369 M179.942 656.716 L179.942 660.975 Q178.182 660.142 176.377 659.702 Q174.595 659.262 172.835 659.262 Q168.206 659.262 165.752 662.387 Q163.321 665.512 162.974 671.831 Q164.34 669.818 166.4 668.753 Q168.46 667.665 170.937 667.665 Q176.145 667.665 179.155 670.836 Q182.187 673.984 182.187 679.424 Q182.187 684.748 179.039 687.966 Q175.891 691.183 170.659 691.183 Q164.664 691.183 161.493 686.6 Q158.321 681.993 158.321 673.267 Q158.321 665.072 162.21 660.211 Q166.099 655.327 172.65 655.327 Q174.409 655.327 176.192 655.674 Q177.997 656.021 179.942 656.716 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M125.914 509.089 Q122.303 509.089 120.474 512.654 Q118.669 516.196 118.669 523.325 Q118.669 530.432 120.474 533.997 Q122.303 537.538 125.914 537.538 Q129.548 537.538 131.354 533.997 Q133.183 530.432 133.183 523.325 Q133.183 516.196 131.354 512.654 Q129.548 509.089 125.914 509.089 M125.914 505.386 Q131.724 505.386 134.78 509.992 Q137.859 514.575 137.859 523.325 Q137.859 532.052 134.78 536.659 Q131.724 541.242 125.914 541.242 Q120.104 541.242 117.025 536.659 Q113.97 532.052 113.97 523.325 Q113.97 514.575 117.025 509.992 Q120.104 505.386 125.914 505.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M146.076 534.691 L150.96 534.691 L150.96 540.571 L146.076 540.571 L146.076 534.691 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M159.965 506.011 L182.187 506.011 L182.187 508.001 L169.641 540.571 L164.757 540.571 L176.562 509.946 L159.965 509.946 L159.965 506.011 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M125.104 359.148 Q121.493 359.148 119.664 362.713 Q117.859 366.255 117.859 373.384 Q117.859 380.491 119.664 384.055 Q121.493 387.597 125.104 387.597 Q128.738 387.597 130.544 384.055 Q132.373 380.491 132.373 373.384 Q132.373 366.255 130.544 362.713 Q128.738 359.148 125.104 359.148 M125.104 355.444 Q130.914 355.444 133.97 360.051 Q137.048 364.634 137.048 373.384 Q137.048 382.111 133.97 386.717 Q130.914 391.301 125.104 391.301 Q119.294 391.301 116.215 386.717 Q113.16 382.111 113.16 373.384 Q113.16 364.634 116.215 360.051 Q119.294 355.444 125.104 355.444 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M145.266 384.75 L150.15 384.75 L150.15 390.629 L145.266 390.629 L145.266 384.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M170.335 374.217 Q167.002 374.217 165.081 376 Q163.183 377.782 163.183 380.907 Q163.183 384.032 165.081 385.815 Q167.002 387.597 170.335 387.597 Q173.669 387.597 175.59 385.815 Q177.511 384.009 177.511 380.907 Q177.511 377.782 175.59 376 Q173.692 374.217 170.335 374.217 M165.659 372.227 Q162.65 371.486 160.96 369.426 Q159.294 367.366 159.294 364.403 Q159.294 360.259 162.233 357.852 Q165.196 355.444 170.335 355.444 Q175.497 355.444 178.437 357.852 Q181.377 360.259 181.377 364.403 Q181.377 367.366 179.687 369.426 Q178.02 371.486 175.034 372.227 Q178.414 373.014 180.289 375.305 Q182.187 377.597 182.187 380.907 Q182.187 385.93 179.108 388.616 Q176.053 391.301 170.335 391.301 Q164.618 391.301 161.539 388.616 Q158.484 385.93 158.484 380.907 Q158.484 377.597 160.382 375.305 Q162.28 373.014 165.659 372.227 M163.946 364.843 Q163.946 367.528 165.613 369.032 Q167.303 370.537 170.335 370.537 Q173.345 370.537 175.034 369.032 Q176.747 367.528 176.747 364.843 Q176.747 362.157 175.034 360.653 Q173.345 359.148 170.335 359.148 Q167.303 359.148 165.613 360.653 Q163.946 362.157 163.946 364.843 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M125.197 209.207 Q121.586 209.207 119.757 212.772 Q117.951 216.313 117.951 223.443 Q117.951 230.549 119.757 234.114 Q121.586 237.656 125.197 237.656 Q128.831 237.656 130.636 234.114 Q132.465 230.549 132.465 223.443 Q132.465 216.313 130.636 212.772 Q128.831 209.207 125.197 209.207 M125.197 205.503 Q131.007 205.503 134.062 210.11 Q137.141 214.693 137.141 223.443 Q137.141 232.17 134.062 236.776 Q131.007 241.359 125.197 241.359 Q119.386 241.359 116.308 236.776 Q113.252 232.17 113.252 223.443 Q113.252 214.693 116.308 210.11 Q119.386 205.503 125.197 205.503 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M145.359 234.809 L150.243 234.809 L150.243 240.688 L145.359 240.688 L145.359 234.809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M160.567 239.971 L160.567 235.711 Q162.326 236.545 164.132 236.984 Q165.937 237.424 167.673 237.424 Q172.303 237.424 174.733 234.322 Q177.187 231.197 177.534 224.855 Q176.192 226.846 174.132 227.91 Q172.071 228.975 169.571 228.975 Q164.386 228.975 161.354 225.85 Q158.345 222.702 158.345 217.262 Q158.345 211.938 161.493 208.721 Q164.641 205.503 169.872 205.503 Q175.868 205.503 179.016 210.11 Q182.187 214.693 182.187 223.443 Q182.187 231.614 178.298 236.498 Q174.432 241.359 167.882 241.359 Q166.122 241.359 164.317 241.012 Q162.511 240.665 160.567 239.971 M169.872 225.318 Q173.02 225.318 174.849 223.165 Q176.701 221.012 176.701 217.262 Q176.701 213.536 174.849 211.383 Q173.02 209.207 169.872 209.207 Q166.724 209.207 164.872 211.383 Q163.044 213.536 163.044 217.262 Q163.044 221.012 164.872 223.165 Q166.724 225.318 169.872 225.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M115.822 86.8118 L123.461 86.8118 L123.461 60.4462 L115.15 62.1128 L115.15 57.8536 L123.414 56.1869 L128.09 56.1869 L128.09 86.8118 L135.729 86.8118 L135.729 90.7469 L115.822 90.7469 L115.822 86.8118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M145.173 84.8673 L150.058 84.8673 L150.058 90.7469 L145.173 90.7469 L145.173 84.8673 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M170.243 59.2656 Q166.632 59.2656 164.803 62.8304 Q162.997 66.3721 162.997 73.5017 Q162.997 80.6081 164.803 84.1729 Q166.632 87.7145 170.243 87.7145 Q173.877 87.7145 175.682 84.1729 Q177.511 80.6081 177.511 73.5017 Q177.511 66.3721 175.682 62.8304 Q173.877 59.2656 170.243 59.2656 M170.243 55.5619 Q176.053 55.5619 179.108 60.1684 Q182.187 64.7517 182.187 73.5017 Q182.187 82.2285 179.108 86.8349 Q176.053 91.4182 170.243 91.4182 Q164.433 91.4182 161.354 86.8349 Q158.298 82.2285 158.298 73.5017 Q158.298 64.7517 161.354 60.1684 Q164.433 55.5619 170.243 55.5619 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip862)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,227.43 325.239,374.972 357.991,582.448 390.744,641.756 423.496,680.414 456.249,753.279 489.001,768.966 521.754,755.207 554.507,821.043 \n",
       "  587.259,817.901 652.764,855.716 718.269,816.523 783.775,848.437 849.28,874.472 947.537,918.42 1045.8,965.224 1176.81,884.709 1307.82,952.276 1471.58,905.538 \n",
       "  1668.09,1005.96 1864.61,906.213 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip860)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip860)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip860)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip860)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ],
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip830\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip831\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip832\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,973.114 1912.76,973.114 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,823.173 1912.76,823.173 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,673.232 1912.76,673.232 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,523.291 1912.76,523.291 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,373.349 1912.76,373.349 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,223.408 1912.76,223.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,73.4669 1912.76,73.4669 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,973.114 230.485,973.114 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,823.173 230.485,823.173 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,673.232 230.485,673.232 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,523.291 230.485,523.291 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,373.349 230.485,373.349 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,223.408 230.485,223.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,73.4669 230.485,73.4669 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M124.525 958.913 Q120.914 958.913 119.086 962.478 Q117.28 966.02 117.28 973.149 Q117.28 980.256 119.086 983.82 Q120.914 987.362 124.525 987.362 Q128.16 987.362 129.965 983.82 Q131.794 980.256 131.794 973.149 Q131.794 966.02 129.965 962.478 Q128.16 958.913 124.525 958.913 M124.525 955.209 Q130.336 955.209 133.391 959.816 Q136.47 964.399 136.47 973.149 Q136.47 981.876 133.391 986.482 Q130.336 991.066 124.525 991.066 Q118.715 991.066 115.637 986.482 Q112.581 981.876 112.581 973.149 Q112.581 964.399 115.637 959.816 Q118.715 955.209 124.525 955.209 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M144.687 984.515 L149.572 984.515 L149.572 990.394 L144.687 990.394 L144.687 984.515 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M172.604 959.908 L160.798 978.357 L172.604 978.357 L172.604 959.908 M171.377 955.834 L177.257 955.834 L177.257 978.357 L182.187 978.357 L182.187 982.246 L177.257 982.246 L177.257 990.394 L172.604 990.394 L172.604 982.246 L157.002 982.246 L157.002 977.732 L171.377 955.834 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M126.007 808.972 Q122.396 808.972 120.567 812.537 Q118.761 816.078 118.761 823.208 Q118.761 830.314 120.567 833.879 Q122.396 837.421 126.007 837.421 Q129.641 837.421 131.447 833.879 Q133.275 830.314 133.275 823.208 Q133.275 816.078 131.447 812.537 Q129.641 808.972 126.007 808.972 M126.007 805.268 Q131.817 805.268 134.873 809.875 Q137.951 814.458 137.951 823.208 Q137.951 831.935 134.873 836.541 Q131.817 841.124 126.007 841.124 Q120.197 841.124 117.118 836.541 Q114.062 831.935 114.062 823.208 Q114.062 814.458 117.118 809.875 Q120.197 805.268 126.007 805.268 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M146.169 834.574 L151.053 834.574 L151.053 840.453 L146.169 840.453 L146.169 834.574 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M161.284 805.893 L179.641 805.893 L179.641 809.828 L165.567 809.828 L165.567 818.301 Q166.585 817.953 167.604 817.791 Q168.622 817.606 169.641 817.606 Q175.428 817.606 178.807 820.777 Q182.187 823.949 182.187 829.365 Q182.187 834.944 178.715 838.046 Q175.243 841.124 168.923 841.124 Q166.747 841.124 164.479 840.754 Q162.233 840.384 159.826 839.643 L159.826 834.944 Q161.909 836.078 164.132 836.634 Q166.354 837.189 168.831 837.189 Q172.835 837.189 175.173 835.083 Q177.511 832.976 177.511 829.365 Q177.511 825.754 175.173 823.648 Q172.835 821.541 168.831 821.541 Q166.956 821.541 165.081 821.958 Q163.229 822.375 161.284 823.254 L161.284 805.893 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M124.849 659.031 Q121.238 659.031 119.41 662.595 Q117.604 666.137 117.604 673.267 Q117.604 680.373 119.41 683.938 Q121.238 687.48 124.849 687.48 Q128.484 687.48 130.289 683.938 Q132.118 680.373 132.118 673.267 Q132.118 666.137 130.289 662.595 Q128.484 659.031 124.849 659.031 M124.849 655.327 Q130.66 655.327 133.715 659.933 Q136.794 664.517 136.794 673.267 Q136.794 681.993 133.715 686.6 Q130.66 691.183 124.849 691.183 Q119.039 691.183 115.961 686.6 Q112.905 681.993 112.905 673.267 Q112.905 664.517 115.961 659.933 Q119.039 655.327 124.849 655.327 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M145.011 684.632 L149.896 684.632 L149.896 690.512 L145.011 690.512 L145.011 684.632 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M170.659 671.369 Q167.511 671.369 165.659 673.521 Q163.831 675.674 163.831 679.424 Q163.831 683.151 165.659 685.327 Q167.511 687.48 170.659 687.48 Q173.807 687.48 175.636 685.327 Q177.488 683.151 177.488 679.424 Q177.488 675.674 175.636 673.521 Q173.807 671.369 170.659 671.369 M179.942 656.716 L179.942 660.975 Q178.182 660.142 176.377 659.702 Q174.595 659.262 172.835 659.262 Q168.206 659.262 165.752 662.387 Q163.321 665.512 162.974 671.831 Q164.34 669.818 166.4 668.753 Q168.46 667.665 170.937 667.665 Q176.145 667.665 179.155 670.836 Q182.187 673.984 182.187 679.424 Q182.187 684.748 179.039 687.966 Q175.891 691.183 170.659 691.183 Q164.664 691.183 161.493 686.6 Q158.321 681.993 158.321 673.267 Q158.321 665.072 162.21 660.211 Q166.099 655.327 172.65 655.327 Q174.409 655.327 176.192 655.674 Q177.997 656.021 179.942 656.716 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M125.914 509.089 Q122.303 509.089 120.474 512.654 Q118.669 516.196 118.669 523.325 Q118.669 530.432 120.474 533.997 Q122.303 537.538 125.914 537.538 Q129.548 537.538 131.354 533.997 Q133.183 530.432 133.183 523.325 Q133.183 516.196 131.354 512.654 Q129.548 509.089 125.914 509.089 M125.914 505.386 Q131.724 505.386 134.78 509.992 Q137.859 514.575 137.859 523.325 Q137.859 532.052 134.78 536.659 Q131.724 541.242 125.914 541.242 Q120.104 541.242 117.025 536.659 Q113.97 532.052 113.97 523.325 Q113.97 514.575 117.025 509.992 Q120.104 505.386 125.914 505.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M146.076 534.691 L150.96 534.691 L150.96 540.571 L146.076 540.571 L146.076 534.691 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M159.965 506.011 L182.187 506.011 L182.187 508.001 L169.641 540.571 L164.757 540.571 L176.562 509.946 L159.965 509.946 L159.965 506.011 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M125.104 359.148 Q121.493 359.148 119.664 362.713 Q117.859 366.255 117.859 373.384 Q117.859 380.491 119.664 384.055 Q121.493 387.597 125.104 387.597 Q128.738 387.597 130.544 384.055 Q132.373 380.491 132.373 373.384 Q132.373 366.255 130.544 362.713 Q128.738 359.148 125.104 359.148 M125.104 355.444 Q130.914 355.444 133.97 360.051 Q137.048 364.634 137.048 373.384 Q137.048 382.111 133.97 386.717 Q130.914 391.301 125.104 391.301 Q119.294 391.301 116.215 386.717 Q113.16 382.111 113.16 373.384 Q113.16 364.634 116.215 360.051 Q119.294 355.444 125.104 355.444 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M145.266 384.75 L150.15 384.75 L150.15 390.629 L145.266 390.629 L145.266 384.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M170.335 374.217 Q167.002 374.217 165.081 376 Q163.183 377.782 163.183 380.907 Q163.183 384.032 165.081 385.815 Q167.002 387.597 170.335 387.597 Q173.669 387.597 175.59 385.815 Q177.511 384.009 177.511 380.907 Q177.511 377.782 175.59 376 Q173.692 374.217 170.335 374.217 M165.659 372.227 Q162.65 371.486 160.96 369.426 Q159.294 367.366 159.294 364.403 Q159.294 360.259 162.233 357.852 Q165.196 355.444 170.335 355.444 Q175.497 355.444 178.437 357.852 Q181.377 360.259 181.377 364.403 Q181.377 367.366 179.687 369.426 Q178.02 371.486 175.034 372.227 Q178.414 373.014 180.289 375.305 Q182.187 377.597 182.187 380.907 Q182.187 385.93 179.108 388.616 Q176.053 391.301 170.335 391.301 Q164.618 391.301 161.539 388.616 Q158.484 385.93 158.484 380.907 Q158.484 377.597 160.382 375.305 Q162.28 373.014 165.659 372.227 M163.946 364.843 Q163.946 367.528 165.613 369.032 Q167.303 370.537 170.335 370.537 Q173.345 370.537 175.034 369.032 Q176.747 367.528 176.747 364.843 Q176.747 362.157 175.034 360.653 Q173.345 359.148 170.335 359.148 Q167.303 359.148 165.613 360.653 Q163.946 362.157 163.946 364.843 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M125.197 209.207 Q121.586 209.207 119.757 212.772 Q117.951 216.313 117.951 223.443 Q117.951 230.549 119.757 234.114 Q121.586 237.656 125.197 237.656 Q128.831 237.656 130.636 234.114 Q132.465 230.549 132.465 223.443 Q132.465 216.313 130.636 212.772 Q128.831 209.207 125.197 209.207 M125.197 205.503 Q131.007 205.503 134.062 210.11 Q137.141 214.693 137.141 223.443 Q137.141 232.17 134.062 236.776 Q131.007 241.359 125.197 241.359 Q119.386 241.359 116.308 236.776 Q113.252 232.17 113.252 223.443 Q113.252 214.693 116.308 210.11 Q119.386 205.503 125.197 205.503 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M145.359 234.809 L150.243 234.809 L150.243 240.688 L145.359 240.688 L145.359 234.809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M160.567 239.971 L160.567 235.711 Q162.326 236.545 164.132 236.984 Q165.937 237.424 167.673 237.424 Q172.303 237.424 174.733 234.322 Q177.187 231.197 177.534 224.855 Q176.192 226.846 174.132 227.91 Q172.071 228.975 169.571 228.975 Q164.386 228.975 161.354 225.85 Q158.345 222.702 158.345 217.262 Q158.345 211.938 161.493 208.721 Q164.641 205.503 169.872 205.503 Q175.868 205.503 179.016 210.11 Q182.187 214.693 182.187 223.443 Q182.187 231.614 178.298 236.498 Q174.432 241.359 167.882 241.359 Q166.122 241.359 164.317 241.012 Q162.511 240.665 160.567 239.971 M169.872 225.318 Q173.02 225.318 174.849 223.165 Q176.701 221.012 176.701 217.262 Q176.701 213.536 174.849 211.383 Q173.02 209.207 169.872 209.207 Q166.724 209.207 164.872 211.383 Q163.044 213.536 163.044 217.262 Q163.044 221.012 164.872 223.165 Q166.724 225.318 169.872 225.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M115.822 86.8118 L123.461 86.8118 L123.461 60.4462 L115.15 62.1128 L115.15 57.8536 L123.414 56.1869 L128.09 56.1869 L128.09 86.8118 L135.729 86.8118 L135.729 90.7469 L115.822 90.7469 L115.822 86.8118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M145.173 84.8673 L150.058 84.8673 L150.058 90.7469 L145.173 90.7469 L145.173 84.8673 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M170.243 59.2656 Q166.632 59.2656 164.803 62.8304 Q162.997 66.3721 162.997 73.5017 Q162.997 80.6081 164.803 84.1729 Q166.632 87.7145 170.243 87.7145 Q173.877 87.7145 175.682 84.1729 Q177.511 80.6081 177.511 73.5017 Q177.511 66.3721 175.682 62.8304 Q173.877 59.2656 170.243 59.2656 M170.243 55.5619 Q176.053 55.5619 179.108 60.1684 Q182.187 64.7517 182.187 73.5017 Q182.187 82.2285 179.108 86.8349 Q176.053 91.4182 170.243 91.4182 Q164.433 91.4182 161.354 86.8349 Q158.298 82.2285 158.298 73.5017 Q158.298 64.7517 161.354 60.1684 Q164.433 55.5619 170.243 55.5619 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip832)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,227.43 325.239,374.972 357.991,582.448 390.744,641.756 423.496,680.414 456.249,753.279 489.001,768.966 521.754,755.207 554.507,821.043 \n",
       "  587.259,817.901 652.764,855.716 718.269,816.523 783.775,848.437 849.28,874.472 947.537,918.42 1045.8,965.224 1176.81,884.709 1307.82,952.276 1471.58,905.538 \n",
       "  1668.09,1005.96 1864.61,906.213 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"small\" (1/3)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": 50
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10-element Vector{Int64}:\n 5\n 1\n 6\n 1\n 1\n 4\n 1\n 5\n 3\n 3"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬────────────┬────────────┬──────────────────────────────────┐\n",
      "│ a     │ b          │ c          │ d                                │\n",
      "│ Int64 │ Float64    │ Float64    │ CategoricalValue{String, UInt32} │\n",
      "│ Count │ Continuous │ Continuous │ OrderedFactor{2}                 │\n",
      "├───────┼────────────┼────────────┼──────────────────────────────────┤\n",
      "│ 1     │ 0.0476572  │ 0.813061   │ male                             │\n",
      "│ 2     │ 0.515695   │ 0.565139   │ female                           │\n",
      "│ 3     │ 0.771497   │ 0.966822   │ female                           │\n",
      "│ 4     │ 0.100942   │ 0.406338   │ male                             │\n",
      "└───────┴────────────┴────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous);"
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Int64}:\n 1\n 2\n 3\n 4"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": 54
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┐\n",
      "│ b          │ c          │\n",
      "│ Float64    │ Float64    │\n",
      "│ Continuous │ Continuous │\n",
      "├────────────┼────────────┤\n",
      "│ 0.0476572  │ 0.813061   │\n",
      "│ 0.515695   │ 0.565139   │\n",
      "│ 0.771497   │ 0.966822   │\n",
      "│ 0.100942   │ 0.406338   │\n",
      "└────────────┴────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": 55
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"male\"\n \"female\"\n \"female\"\n \"male\""
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬──────────────────┬──────────────────────────────────\n│\u001b[22m names                   \u001b[0m│\u001b[22m scitypes         \u001b[0m│\u001b[22m types                          \u001b[0m ⋯\n├─────────────────────────┼──────────────────┼──────────────────────────────────\n│ surgery                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ age                     │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n│ rectal_temperature      │ Continuous       │ Float64                         ⋯\n│ pulse                   │ Continuous       │ Float64                         ⋯\n│ respiratory_rate        │ Continuous       │ Float64                         ⋯\n│ temperature_extremities │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ mucous_membranes        │ Multiclass{6}    │ CategoricalValue{Int64, UInt32} ⋯\n│ capillary_refill_time   │ Multiclass{3}    │ CategoricalValue{Int64, UInt32} ⋯\n│ pain                    │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} ⋯\n│ peristalsis             │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ abdominal_distension    │ OrderedFactor{4} │ CategoricalValue{Int64, UInt32} ⋯\n│ packed_cell_volume      │ Continuous       │ Float64                         ⋯\n│ total_protein           │ Continuous       │ Float64                         ⋯\n│ outcome                 │ Multiclass{3}    │ CategoricalValue{Int64, UInt32} ⋯\n│ surgical_lesion         │ OrderedFactor{2} │ CategoricalValue{Int64, UInt32} ⋯\n│ cp_data                 │ Multiclass{2}    │ CategoricalValue{Int64, UInt32} ⋯\n└─────────────────────────┴──────────────────┴──────────────────────────────────\n"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}
