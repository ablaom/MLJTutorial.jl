{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.6.3\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/GoogleDrive/Julia/MLJ/MLJTutorial/notebooks/02_models/env/Project.toml`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Selecting, Training and Evaluating Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` work-flow.\n",
    "> 3. Inspect the outcomes of training and save these to a file.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall \u001b[1mPlease cite\u001b[22m:\n\n  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to\n  this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from\n  the other 2; the latter are NOT linearly separable from each other.\n\n  Predicted attribute: class of iris plant. This is an exceedingly simple domain.\n\n\u001b[1m  Attribute Information:\u001b[22m\n\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n\n\u001b[36m  1. sepal length in cm\u001b[39m\n\u001b[36m  2. sepal width in cm\u001b[39m\n\u001b[36m  3. petal length in cm\u001b[39m\n\u001b[36m  4. petal width in cm\u001b[39m\n\u001b[36m  5. class: \u001b[39m\n\u001b[36m     -- Iris Setosa\u001b[39m\n\u001b[36m     -- Iris Versicolour\u001b[39m\n\u001b[36m     -- Iris Virginica\u001b[39m",
      "text/markdown": "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n\n**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n\n### Attribute Information:\n\n```\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n   -- Iris Setosa\n   -- Iris Versicolour\n   -- Iris Virginica\n```\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "OpenML.describe_dataset(61)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n─────┼───────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n   4 │         4.6         3.1          1.5         0.2  Iris-setosa",
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = OpenML.load(61); # a column dictionary table\n",
    "\n",
    "import DataFrames\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────┬──────────────────────────────────┬───────────────┐\n│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n├─────────────┼──────────────────────────────────┼───────────────┤\n│ sepallength │ Float64                          │ Continuous    │\n│ sepalwidth  │ Float64                          │ Continuous    │\n│ petallength │ Float64                          │ Continuous    │\n│ petalwidth  │ Float64                          │ Continuous    │\n│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n└─────────────┴──────────────────────────────────┴───────────────┘\n_.nrows = 150\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "schema(iris)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "These look fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Split data into input and target parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We randomize the data at the\n",
    "same time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{Multiclass{3}} (alias for AbstractArray{Multiclass{3}, 1})"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "y, X = unpack(iris, ==(:class), name->true; rng=123);\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one way to access the documentation (at the REPL, `?unpack`\n",
    "also works):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[36m  t1, t2, ...., tk = unpack(table, f1, f2, ... fk;\u001b[39m\n\u001b[36m                           wrap_singles=false,\u001b[39m\n\u001b[36m                           shuffle=false,\u001b[39m\n\u001b[36m                           rng::Union{AbstractRNG,Int,Nothing}=nothing)\u001b[39m\n\n  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables (or vectors) \u001b[36mt1, t2, ..., tk\u001b[39m by making column selections \u001b[1mwithout replacement\u001b[22m by successively applying the columnn\n  name filters \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m, ..., \u001b[36mfk\u001b[39m. A \u001b[4mfilter\u001b[24m is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m. For example, use the filter \u001b[36m_ -> true\u001b[39m to pick up all\n  remaining columns of the table.\n\n  Whenever a returned table contains a single column, it is converted to a vector unless \u001b[36mwrap_singles=true\u001b[39m.\n\n  Scientific type conversions can be optionally specified (note semicolon):\n\n\u001b[36m  unpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\u001b[39m\n\n  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of an automatically generated\n  Mersenne twister. If \u001b[36mrng\u001b[39m is specified then \u001b[36mshuffle=true\u001b[39m is implicit.\n\n\u001b[1m  Example\u001b[22m\n\u001b[1m  –––––––––\u001b[22m\n\n\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n\u001b[36m  julia> Z, XY = unpack(table, ==(:z), !=(:w);\u001b[39m\n\u001b[36m                 :x=>Continuous, :y=>Multiclass)\u001b[39m\n\u001b[36m  julia> XY\u001b[39m\n\u001b[36m  2×2 DataFrame\u001b[39m\n\u001b[36m  │ Row │ x       │ y            │\u001b[39m\n\u001b[36m  │     │ Float64 │ Categorical… │\u001b[39m\n\u001b[36m  ├─────┼─────────┼──────────────┤\u001b[39m\n\u001b[36m  │ 1   │ 1.0     │ 'a'          │\u001b[39m\n\u001b[36m  │ 2   │ 2.0     │ 'b'          │\u001b[39m\n\u001b[36m  \u001b[39m\n\u001b[36m  julia> Z\u001b[39m\n\u001b[36m  2-element Array{Float64,1}:\u001b[39m\n\u001b[36m   10.0\u001b[39m\n\u001b[36m   20.0\u001b[39m",
      "text/markdown": "```\nt1, t2, ...., tk = unpack(table, f1, f2, ... fk;\n                         wrap_singles=false,\n                         shuffle=false,\n                         rng::Union{AbstractRNG,Int,Nothing}=nothing)\n```\n\nHorizontally split any Tables.jl compatible `table` into smaller tables (or vectors) `t1, t2, ..., tk` by making column selections **without replacement** by successively applying the columnn name filters `f1`, `f2`, ..., `fk`. A *filter* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`. For example, use the filter `_ -> true` to pick up all remaining columns of the table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n\nScientific type conversions can be optionally specified (note semicolon):\n\n```\nunpack(table, t...; col1=>scitype1, col2=>scitype2, ... )\n```\n\nIf `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n\n### Example\n\n```\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n```\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "@doc unpack"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On searching for a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "183-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n (name = ARDRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = AffinityPropagation, package_name = ScikitLearn, ... )\n (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BaggingRegressor, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )\n (name = Birch, package_name = ScikitLearn, ... )\n (name = CBLOFDetector, package_name = OutlierDetectionPython, ... )\n (name = COFDetector, package_name = OutlierDetectionNeighbors, ... )\n (name = COFDetector, package_name = OutlierDetectionPython, ... )\n (name = COPODDetector, package_name = OutlierDetectionPython, ... )\n (name = ComplementNBClassifier, package_name = ScikitLearn, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = ConstantRegressor, package_name = MLJModels, ... )\n (name = ContinuousEncoder, package_name = MLJModels, ... )\n ⋮\n (name = RidgeRegressor, package_name = ScikitLearn, ... )\n (name = RobustRegressor, package_name = MLJLinearModels, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SGDRegressor, package_name = ScikitLearn, ... )\n (name = SODDetector, package_name = OutlierDetectionPython, ... )\n (name = SOSDetector, package_name = OutlierDetectionPython, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearRegressor, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuRegressor, package_name = ScikitLearn, ... )\n (name = SVMRegressor, package_name = ScikitLearn, ... )\n (name = SpectralClustering, package_name = ScikitLearn, ... )\n (name = Standardizer, package_name = MLJModels, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = TSVDTransformer, package_name = TSVD, ... )\n (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n (name = UnivariateFillImputer, package_name = MLJModels, ... )\n (name = UnivariateStandardizer, package_name = MLJModels, ... )\n (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )\n (name = XGBoostCount, package_name = XGBoost, ... )\n (name = XGBoostRegressor, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "all_models = models()"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you already have an idea about the name of the model, you could\n",
    "search by string or regex:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = LinearRegressor, package_name = GLM, ... )\n (name = LinearRegressor, package_name = MLJLinearModels, ... )\n (name = LinearRegressor, package_name = MultivariateStats, ... )\n (name = LinearRegressor, package_name = ScikitLearn, ... )\n (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n (name = SVMLinearRegressor, package_name = ScikitLearn, ... )"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "some_models = models(\"LinearRegressor\") # sic"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each entry contains metadata for a model whose defining code is not\n",
    "yet loaded:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mLinear regressor (OLS) with a Normal model.\u001b[39m\n\u001b[35m→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\u001b[39m\n\u001b[35m→ do `@load LinearRegressor pkg=\"GLM\"` to use the model.\u001b[39m\n\u001b[35m→ do `?LinearRegressor` for documentation.\u001b[39m\n(name = \"LinearRegressor\",\n package_name = \"GLM\",\n is_supervised = true,\n abstract_type = Probabilistic,\n deep_properties = (),\n docstring = \"Linear regressor (OLS) with a Normal model.\\n→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\\n→ do `@load LinearRegressor pkg=\\\"GLM\\\"` to use the model.\\n→ do `?LinearRegressor` for documentation.\",\n fit_data_scitype = Tuple{Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:Continuous), AbstractVector{Continuous}},\n hyperparameter_ranges = (nothing, nothing, nothing),\n hyperparameter_types = (\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\"),\n hyperparameters = (:fit_intercept, :allowrankdeficient, :offsetcol),\n implemented_methods = [:predict, :fit, :fitted_params, :predict_mean],\n inverse_transform_scitype = Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = nothing,\n load_path = \"MLJGLMInterface.LinearRegressor\",\n package_license = \"MIT\",\n package_url = \"https://github.com/JuliaStats/GLM.jl\",\n package_uuid = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\",\n predict_scitype = AbstractVector{ScientificTypesBase.Density{Continuous}},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = false,\n supports_weights = false,\n transform_scitype = Unknown,\n input_scitype = Table{_s52} where _s52<:(AbstractVector{_s51} where _s51<:Continuous),\n target_scitype = AbstractVector{Continuous},\n output_scitype = Unknown,)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "meta = some_models[1]"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AbstractVector{Continuous} (alias for AbstractArray{Continuous, 1})"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "targetscitype = meta.target_scitype"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "false"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "scitype(y) <: targetscitype"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this model won't do. Let's  find all pure julia classifiers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n (name = LDA, package_name = MultivariateStats, ... )\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "filter_julia_classifiers(meta) =\n",
    "    AbstractVector{Finite} <: meta.target_scitype &&\n",
    "    meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find all (supervised) models that match my data!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n (name = BaggingClassifier, package_name = ScikitLearn, ... )\n (name = BayesianLDA, package_name = MultivariateStats, ... )\n (name = BayesianLDA, package_name = ScikitLearn, ... )\n (name = BayesianQDA, package_name = ScikitLearn, ... )\n (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n (name = ConstantClassifier, package_name = MLJModels, ... )\n (name = DecisionTreeClassifier, package_name = BetaML, ... )\n (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n (name = DummyClassifier, package_name = ScikitLearn, ... )\n (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n (name = LDA, package_name = MultivariateStats, ... )\n (name = LGBMClassifier, package_name = LightGBM, ... )\n (name = LinearSVC, package_name = LIBSVM, ... )\n (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n (name = LogisticClassifier, package_name = ScikitLearn, ... )\n (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n (name = NuSVC, package_name = LIBSVM, ... )\n (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n (name = PegasosClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = BetaML, ... )\n (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n (name = RandomForestClassifier, package_name = BetaML, ... )\n (name = RandomForestClassifier, package_name = DecisionTree, ... )\n (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n (name = RidgeClassifier, package_name = ScikitLearn, ... )\n (name = SGDClassifier, package_name = ScikitLearn, ... )\n (name = SVC, package_name = LIBSVM, ... )\n (name = SVMClassifier, package_name = ScikitLearn, ... )\n (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n (name = SubspaceLDA, package_name = MultivariateStats, ... )\n (name = XGBoostClassifier, package_name = XGBoost, ... )"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "models(matching(X, y))"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Select and instantiate a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the code defining a new model type we use the `@load` macro:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other ways to load model code are described\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/loading_model_code/#Loading-Model-Code)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll instantiate this type with default values for the\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n    builder = Short(\n            n_hidden = 0,\n            dropout = 0.5,\n            σ = NNlib.σ),\n    finaliser = NNlib.softmax,\n    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n    loss = Flux.Losses.crossentropy,\n    epochs = 10,\n    batch_size = 1,\n    lambda = 0.0,\n    alpha = 0.0,\n    rng = Random._GLOBAL_RNG(),\n    optimiser_changes_trigger_retraining = false,\n    acceleration = CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "model = NeuralNetworkClassifier()"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n(name = \"NeuralNetworkClassifier\",\n package_name = \"MLJFlux\",\n is_supervised = true,\n abstract_type = Probabilistic,\n deep_properties = (:optimiser, :builder),\n docstring = \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n fit_data_scitype = Tuple{Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:Continuous), AbstractVector{var\"#s76\"} where var\"#s76\"<:Finite},\n hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n hyperparameter_types = (\"MLJFlux.Short\", \"typeof(NNlib.softmax)\", \"Flux.Optimise.ADAM\", \"typeof(Flux.Losses.crossentropy)\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\"),\n hyperparameters = (:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration),\n implemented_methods = Any[],\n inverse_transform_scitype = Unknown,\n is_pure_julia = true,\n is_wrapper = false,\n iteration_parameter = :epochs,\n load_path = \"MLJFlux.NeuralNetworkClassifier\",\n package_license = \"MIT\",\n package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n predict_scitype = AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:Finite},\n prediction_type = :probabilistic,\n supports_class_weights = false,\n supports_online = false,\n supports_training_losses = true,\n supports_weights = false,\n transform_scitype = Unknown,\n input_scitype = Table{var\"#s53\"} where var\"#s53\"<:(AbstractVector{var\"#s52\"} where var\"#s52\"<:Continuous),\n target_scitype = AbstractVector{var\"#s76\"} where var\"#s76\"<:Finite,\n output_scitype = Unknown,)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model)"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 12"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "true"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On fitting, predicting, and inspecting models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n  args: \n    1:\tSource @287 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @682 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(1:length(y), 0.7)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can `fit!`..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.214\n",
      "[ Info: Loss is 1.085\n",
      "[ Info: Loss is 1.026\n",
      "[ Info: Loss is 1.021\n",
      "[ Info: Loss is 1.002\n",
      "[ Info: Loss is 1.039\n",
      "[ Info: Loss is 1.017\n",
      "[ Info: Loss is 1.03\n",
      "[ Info: Loss is 1.021\n",
      "[ Info: Loss is 0.9236\n",
      "[ Info: Loss is 0.9759\n",
      "[ Info: Loss is 0.9731\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n  args: \n    1:\tSource @287 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @682 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.351, Iris-versicolor=>0.346, Iris-virginica=>0.304)\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.311, Iris-versicolor=>0.353, Iris-virginica=>0.336)\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.537, Iris-versicolor=>0.289, Iris-virginica=>0.174)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, rows=test);  # or `predict(mach, Xnew)`\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll have more to say on the form of this prediction shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, one can inspect the learned parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(training_losses = [1.1753411994162573, 1.2138400225812176, 1.0851206408615486, 1.025815075265201, 1.0212714029222927, 1.0021125501295292, 1.0393110399932841, 1.017371901143682, 1.0297530361067428, 1.020795145051376, 0.9236077243272449, 0.9759016657834062, 0.9731015347600622],)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "You save a machine like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ],
   "metadata": {},
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "And retrieve it like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element MLJBase.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.307, Iris-versicolor=>0.352, Iris-virginica=>0.341)\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.347, Iris-versicolor=>0.347, Iris-virginica=>0.306)\n UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.303, Iris-versicolor=>0.353, Iris-virginica=>0.343)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "cell_type": "code",
   "source": [
    "mach2 = machine(\"neural_net.jlso\")\n",
    "yhat = predict(mach2, X);\n",
    "yhat[1:3]"
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to fit a retrieved model, you will need to bind some data to it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net: 15%[===>                     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 38%[=========>               ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 46%[===========>             ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 54%[=============>           ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 62%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 85%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @184 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @260 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "cell_type": "code",
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ],
   "metadata": {},
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.9118\n",
      "[ Info: Loss is 0.9126\n",
      "[ Info: Loss is 0.9227\n",
      "[ Info: Loss is 0.8941\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n  args: \n    1:\tSource @287 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @682 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this particular model we can also increase `:learning_rate`\n",
    "without triggering a cold restart:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 0.8488\n",
      "[ Info: Loss is 0.7126\n",
      "[ Info: Loss is 0.7314\n",
      "[ Info: Loss is 0.6958\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n  args: \n    1:\tSource @287 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @682 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "[ Info: Loss is 1.032\n",
      "[ Info: Loss is 0.8485\n",
      "[ Info: Loss is 0.7205\n",
      "[ Info: Loss is 0.6837\n",
      "[ Info: Loss is 0.7004\n",
      "[ Info: Loss is 0.7524\n",
      "[ Info: Loss is 0.6868\n",
      "[ Info: Loss is 0.6993\n",
      "[ Info: Loss is 0.71\n",
      "[ Info: Loss is 0.6339\n",
      "[ Info: Loss is 0.668\n",
      "[ Info: Loss is 0.754\n",
      "[ Info: Loss is 0.7984\n",
      "[ Info: Loss is 0.6757\n",
      "[ Info: Loss is 0.679\n",
      "[ Info: Loss is 0.5716\n",
      "[ Info: Loss is 0.6311\n",
      "[ Info: Loss is 0.6827\n",
      "[ Info: Loss is 0.7312\n",
      "[ Info: Loss is 0.5859\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n  args: \n    1:\tSource @287 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @682 ⏎ `AbstractVector{Multiclass{3}}`\n"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "cell_type": "code",
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ],
   "metadata": {},
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterative models that implement warm-restart for training can be\n",
    "controlled externally (eg, using an out-of-sample stopping\n",
    "criterion). See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)\n",
    "for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a\n",
    "prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 13%[===>                     ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 16%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 19%[====>                    ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 23%[=====>                   ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 26%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 32%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 42%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 48%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 52%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 58%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 68%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 74%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 77%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 81%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 87%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 97%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.12, Iris-versicolor=>0.543, Iris-virginica=>0.337)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "cell_type": "code",
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "What's going on here?"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":probabilistic"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "cell_type": "code",
   "source": [
    "info(model).prediction_type"
   ],
   "metadata": {},
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**:\n",
    "- In MLJ, a model that can predict probabilities (and not just point values) will do so by default.\n",
    "- For most probabilistic predictors, the predicted object is a `Distributions.Distribution` object, supporting the `Distributions.jl` [API](https://juliastats.org/Distributions.jl/latest/extends/#Create-a-Distribution-1) for such objects. In particular, the methods `rand`,  `pdf`, `logpdf`, `mode`, `median` and `mean` will apply, where appropriate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3369624172532687"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "cell_type": "code",
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ],
   "metadata": {},
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the most likely observation, we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-versicolor\""
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "cell_type": "code",
   "source": [
    "mode(yhat[1])"
   ],
   "metadata": {},
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Float64}:\n 0.5430493924397207\n 0.4485391765456344\n 0.027240907423242117\n 0.4010589088010355"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "cell_type": "code",
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ],
   "metadata": {},
   "execution_count": 36
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-virginica\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "cell_type": "code",
   "source": [
    "mode.(yhat[1:4])"
   ],
   "metadata": {},
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"Iris-versicolor\"\n \"Iris-virginica\"\n \"Iris-setosa\"\n \"Iris-virginica\""
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "cell_type": "code",
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ],
   "metadata": {},
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4×3 Matrix{Float64}:\n 0.119988   0.543049   0.336962\n 0.0681956  0.448539   0.483265\n 0.972751   0.0272409  8.37117e-6\n 0.0519507  0.401059   0.54699"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "cell_type": "code",
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ],
   "metadata": {},
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a typical MLJ work-flow, this is not as useful as you\n",
    "might imagine. In particular, all probabilistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.43557525538577857"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "cell_type": "code",
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ],
   "metadata": {},
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.044444444444444446"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "cell_type": "code",
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ],
   "metadata": {},
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "61-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type), T} where T<:Tuple}:\n (name = BrierLoss, instances = [brier_loss], ...)\n (name = BrierScore, instances = [brier_score], ...)\n (name = LPLoss, instances = [l1, l2], ...)\n (name = LogCoshLoss, instances = [log_cosh, log_cosh_loss], ...)\n (name = LogLoss, instances = [log_loss, cross_entropy], ...)\n (name = LogScore, instances = [log_score], ...)\n (name = SphericalScore, instances = [spherical_score], ...)\n (name = Accuracy, instances = [accuracy], ...)\n (name = AreaUnderCurve, instances = [area_under_curve, auc], ...)\n (name = BalancedAccuracy, instances = [balanced_accuracy, bacc, bac], ...)\n (name = ConfusionMatrix, instances = [confusion_matrix, confmat], ...)\n (name = FScore, instances = [f1score], ...)\n (name = FalseDiscoveryRate, instances = [false_discovery_rate, falsediscovery_rate, fdr], ...)\n (name = FalseNegative, instances = [false_negative, falsenegative], ...)\n (name = FalseNegativeRate, instances = [false_negative_rate, falsenegative_rate, fnr, miss_rate], ...)\n (name = FalsePositive, instances = [false_positive, falsepositive], ...)\n (name = FalsePositiveRate, instances = [false_positive_rate, falsepositive_rate, fpr, fallout], ...)\n (name = MatthewsCorrelation, instances = [matthews_correlation, mcc], ...)\n (name = MeanAbsoluteError, instances = [mae, mav, mean_absolute_error, mean_absolute_value], ...)\n (name = MeanAbsoluteProportionalError, instances = [mape], ...)\n (name = MisclassificationRate, instances = [misclassification_rate, mcr], ...)\n (name = MulticlassFScore, instances = [macro_f1score, micro_f1score, multiclass_f1score], ...)\n (name = MulticlassFalseDiscoveryRate, instances = [multiclass_falsediscovery_rate, multiclass_fdr], ...)\n (name = MulticlassFalseNegative, instances = [multiclass_false_negative, multiclass_falsenegative], ...)\n (name = MulticlassFalseNegativeRate, instances = [multiclass_false_negative_rate, multiclass_fnr, multiclass_miss_rate, multiclass_falsenegative_rate], ...)\n (name = MulticlassFalsePositive, instances = [multiclass_false_positive, multiclass_falsepositive], ...)\n ⋮\n (name = RootMeanSquaredError, instances = [rms, rmse, root_mean_squared_error], ...)\n (name = RootMeanSquaredLogError, instances = [rmsl, rmsle, root_mean_squared_log_error], ...)\n (name = RootMeanSquaredLogProportionalError, instances = [rmslp1], ...)\n (name = RootMeanSquaredProportionalError, instances = [rmsp], ...)\n (name = TrueNegative, instances = [true_negative, truenegative], ...)\n (name = TrueNegativeRate, instances = [true_negative_rate, truenegative_rate, tnr, specificity, selectivity], ...)\n (name = TruePositive, instances = [true_positive, truepositive], ...)\n (name = TruePositiveRate, instances = [true_positive_rate, truepositive_rate, tpr, sensitivity, recall, hit_rate], ...)\n (name = DWDMarginLoss, instances = [dwd_margin_loss], ...)\n (name = ExpLoss, instances = [exp_loss], ...)\n (name = L1HingeLoss, instances = [l1_hinge_loss], ...)\n (name = L2HingeLoss, instances = [l2_hinge_loss], ...)\n (name = L2MarginLoss, instances = [l2_margin_loss], ...)\n (name = LogitMarginLoss, instances = [logit_margin_loss], ...)\n (name = ModifiedHuberLoss, instances = [modified_huber_loss], ...)\n (name = PerceptronLoss, instances = [perceptron_loss], ...)\n (name = SigmoidLoss, instances = [sigmoid_loss], ...)\n (name = SmoothedL1HingeLoss, instances = [smoothed_l1_hinge_loss], ...)\n (name = ZeroOneLoss, instances = [zero_one_loss], ...)\n (name = HuberLoss, instances = [huber_loss], ...)\n (name = L1EpsilonInsLoss, instances = [l1_epsilon_ins_loss], ...)\n (name = L2EpsilonInsLoss, instances = [l2_epsilon_ins_loss], ...)\n (name = LPDistLoss, instances = [lp_dist_loss], ...)\n (name = LogitDistLoss, instances = [logit_dist_loss], ...)\n (name = PeriodicLoss, instances = [periodic_loss], ...)\n (name = QuantileLoss, instances = [quantile_loss], ...)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "cell_type": "code",
   "source": [
    "measures()"
   ],
   "metadata": {},
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Evaluate the model performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬──────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold \u001b[0m│\n├────────────────────────────┼─────────────┼──────────────┼──────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.436       │ predict      │ [0.436]  │\n│ MisclassificationRate()    │ 0.0444      │ predict_mode │ [0.0444] │\n│ BrierScore()               │ -0.26       │ predict      │ [-0.26]  │\n└────────────────────────────┴─────────────┴──────────────┴──────────┘\n"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or applying cross-validation instead:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:10\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:14\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────────────────────────────────┐\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold                                        \u001b[0m│\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────────────────────────────────┤\n│ LogLoss(tol = 2.22045e-16) │ 0.387       │ predict      │ [0.394, 0.283, 0.46, 0.394, 0.414, 0.377]       │\n│ MisclassificationRate()    │ 0.04        │ predict_mode │ [0.04, 0.0, 0.04, 0.04, 0.04, 0.08]             │\n│ BrierScore()               │ -0.214      │ predict      │ [-0.206, -0.149, -0.261, -0.22, -0.232, -0.213] │\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────────────────────────────────┘\n"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, Monte Carlo cross-validation (cross-validation repeated\n",
    "randomized folds)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rEvaluating over 18 folds:  11%[==>                      ]  ETA: 0:00:37\u001b[K\rEvaluating over 18 folds:  17%[====>                    ]  ETA: 0:00:35\u001b[K\rEvaluating over 18 folds:  22%[=====>                   ]  ETA: 0:00:32\u001b[K\rEvaluating over 18 folds:  28%[======>                  ]  ETA: 0:00:30\u001b[K\rEvaluating over 18 folds:  33%[========>                ]  ETA: 0:00:28\u001b[K\rEvaluating over 18 folds:  39%[=========>               ]  ETA: 0:00:26\u001b[K\rEvaluating over 18 folds:  44%[===========>             ]  ETA: 0:00:23\u001b[K\rEvaluating over 18 folds:  50%[============>            ]  ETA: 0:00:21\u001b[K\rEvaluating over 18 folds:  56%[=============>           ]  ETA: 0:00:19\u001b[K\rEvaluating over 18 folds:  61%[===============>         ]  ETA: 0:00:16\u001b[K\rEvaluating over 18 folds:  67%[================>        ]  ETA: 0:00:14\u001b[K\rEvaluating over 18 folds:  72%[==================>      ]  ETA: 0:00:12\u001b[K\rEvaluating over 18 folds:  78%[===================>     ]  ETA: 0:00:09\u001b[K\rEvaluating over 18 folds:  83%[====================>    ]  ETA: 0:00:07\u001b[K\rEvaluating over 18 folds:  89%[======================>  ]  ETA: 0:00:05\u001b[K\rEvaluating over 18 folds:  94%[=======================> ]  ETA: 0:00:02\u001b[K\rEvaluating over 18 folds: 100%[=========================] Time: 0:00:42\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌────────────────────────────┬─────────────┬──────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m per_fold                                                                                                                      \u001b[0m ⋯\n├────────────────────────────┼─────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ LogLoss(tol = 2.22045e-16) │ 0.404       │ predict      │ [0.304, 0.388, 0.41, 0.52, 0.445, 0.464, 0.449, 0.363, 0.439, 0.372, 0.388, 0.397, 0.422, 0.348, 0.448, 0.355, 0.329, 0.431]   ⋯\n│ MisclassificationRate()    │ 0.0511      │ predict_mode │ [0.04, 0.0, 0.0, 0.12, 0.08, 0.12, 0.0, 0.04, 0.04, 0.04, 0.08, 0.0, 0.08, 0.08, 0.12, 0.0, 0.0, 0.08]                         ⋯\n│ BrierScore()               │ -0.226      │ predict      │ [-0.159, -0.224, -0.241, -0.316, -0.249, -0.27, -0.262, -0.191, -0.244, -0.193, -0.206, -0.22, -0.231, -0.19, -0.255, -0.193,  ⋯\n└────────────────────────────┴─────────────┴──────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\u001b[36m                                                                                                                                                                            1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "cell_type": "code",
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, misclassification_rate, brier_score])"
   ],
   "metadata": {},
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a work-flow like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Creating subsamples from a subset of all rows. \n",
      "\rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:06\u001b[K\rEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:05\u001b[K\rEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:03\u001b[K\rEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[K\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:09\u001b[K\n",
      "[ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "\rOptimising neural net:  4%[>                        ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  6%[=>                       ]  ETA: 0:00:01\u001b[K\rOptimising neural net:  8%[=>                       ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 10%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 12%[==>                      ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 14%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 16%[===>                     ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 18%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 20%[====>                    ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 22%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 24%[=====>                   ]  ETA: 0:00:02\u001b[K\rOptimising neural net: 25%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 27%[======>                  ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 29%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 31%[=======>                 ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 33%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 35%[========>                ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 37%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 39%[=========>               ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 41%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 43%[==========>              ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 45%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 47%[===========>             ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 49%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 51%[============>            ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 53%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 55%[=============>           ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 57%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 59%[==============>          ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 61%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 63%[===============>         ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 65%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 67%[================>        ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 69%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 71%[=================>       ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 73%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 75%[==================>      ]  ETA: 0:00:01\u001b[K\rOptimising neural net: 76%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 78%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 80%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:100%[=========================] Time: 0:00:01\u001b[K\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ],
   "metadata": {},
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5) on log10 scale"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "cell_type": "code",
   "source": [
    "r = range(model, :epochs, lower=1, upper=50, scale=:log10)"
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "[ Info: Attempting to evaluate 22 models.\n",
      "\rEvaluating over 22 metamodels:   0%[>                        ]  ETA: N/A\u001b[K\rEvaluating over 22 metamodels:   5%[=>                       ]  ETA: 0:00:07\u001b[K\rEvaluating over 22 metamodels:   9%[==>                      ]  ETA: 0:00:04\u001b[K\rEvaluating over 22 metamodels:  14%[===>                     ]  ETA: 0:00:03\u001b[K\rEvaluating over 22 metamodels:  18%[====>                    ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  23%[=====>                   ]  ETA: 0:00:02\u001b[K\rEvaluating over 22 metamodels:  27%[======>                  ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  32%[=======>                 ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  36%[=========>               ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  41%[==========>              ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  45%[===========>             ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  50%[============>            ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  55%[=============>           ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  59%[==============>          ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  64%[===============>         ]  ETA: 0:00:01\u001b[K\rEvaluating over 22 metamodels:  68%[=================>       ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  73%[==================>      ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  77%[===================>     ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  82%[====================>    ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  86%[=====================>   ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels:  95%[=======================> ]  ETA: 0:00:00\u001b[K\rEvaluating over 22 metamodels: 100%[=========================] Time: 0:00:02\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(parameter_name = \"epochs\",\n parameter_scale = :log10,\n parameter_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 17, 19, 22, 25, 29, 33, 38, 44, 50],\n measurements = [0.9236549248617245, 0.7752832533007669, 0.7144584401554172, 0.6385425127903884, 0.5880112084098996, 0.5648094144238905, 0.5431366149525997, 0.49788858816068315, 0.5019780547125848, 0.5043507373656169  …  0.4835507155874713, 0.43667548246628995, 0.436980379331002, 0.4174409802900054, 0.4280265099583725, 0.4056735509731523, 0.3941022017859865, 0.38445291439755036, 0.3912852900461887, 0.4157815901737606],)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Plot{Plots.GRBackend() n=1}",
      "image/png": "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",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip590\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip590)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip591\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip590)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip592\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip590)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,979.119 1912.76,979.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,806.495 1912.76,806.495 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,633.872 1912.76,633.872 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,461.248 1912.76,461.248 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,288.625 1912.76,288.625 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,116.002 1912.76,116.002 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,979.119 230.485,979.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,806.495 230.485,806.495 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,633.872 230.485,633.872 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,461.248 230.485,461.248 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,288.625 230.485,288.625 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,116.002 230.485,116.002 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip590)\" d=\"M124.525 964.917 Q120.914 964.917 119.086 968.482 Q117.28 972.024 117.28 979.153 Q117.28 986.26 119.086 989.825 Q120.914 993.366 124.525 993.366 Q128.16 993.366 129.965 989.825 Q131.794 986.26 131.794 979.153 Q131.794 972.024 129.965 968.482 Q128.16 964.917 124.525 964.917 M124.525 961.214 Q130.336 961.214 133.391 965.82 Q136.47 970.403 136.47 979.153 Q136.47 987.88 133.391 992.487 Q130.336 997.07 124.525 997.07 Q118.715 997.07 115.637 992.487 Q112.581 987.88 112.581 979.153 Q112.581 970.403 115.637 965.82 Q118.715 961.214 124.525 961.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M144.687 990.519 L149.572 990.519 L149.572 996.399 L144.687 996.399 L144.687 990.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M172.604 965.913 L160.798 984.362 L172.604 984.362 L172.604 965.913 M171.377 961.839 L177.257 961.839 L177.257 984.362 L182.187 984.362 L182.187 988.251 L177.257 988.251 L177.257 996.399 L172.604 996.399 L172.604 988.251 L157.002 988.251 L157.002 983.737 L171.377 961.839 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M126.007 792.294 Q122.396 792.294 120.567 795.859 Q118.761 799.4 118.761 806.53 Q118.761 813.636 120.567 817.201 Q122.396 820.743 126.007 820.743 Q129.641 820.743 131.447 817.201 Q133.275 813.636 133.275 806.53 Q133.275 799.4 131.447 795.859 Q129.641 792.294 126.007 792.294 M126.007 788.59 Q131.817 788.59 134.873 793.197 Q137.951 797.78 137.951 806.53 Q137.951 815.257 134.873 819.863 Q131.817 824.447 126.007 824.447 Q120.197 824.447 117.118 819.863 Q114.062 815.257 114.062 806.53 Q114.062 797.78 117.118 793.197 Q120.197 788.59 126.007 788.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M146.169 817.896 L151.053 817.896 L151.053 823.775 L146.169 823.775 L146.169 817.896 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M161.284 789.215 L179.641 789.215 L179.641 793.15 L165.567 793.15 L165.567 801.623 Q166.585 801.275 167.604 801.113 Q168.622 800.928 169.641 800.928 Q175.428 800.928 178.807 804.099 Q182.187 807.271 182.187 812.687 Q182.187 818.266 178.715 821.368 Q175.243 824.447 168.923 824.447 Q166.747 824.447 164.479 824.076 Q162.233 823.706 159.826 822.965 L159.826 818.266 Q161.909 819.4 164.132 819.956 Q166.354 820.511 168.831 820.511 Q172.835 820.511 175.173 818.405 Q177.511 816.298 177.511 812.687 Q177.511 809.076 175.173 806.97 Q172.835 804.863 168.831 804.863 Q166.956 804.863 165.081 805.28 Q163.229 805.697 161.284 806.576 L161.284 789.215 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M124.849 619.671 Q121.238 619.671 119.41 623.235 Q117.604 626.777 117.604 633.907 Q117.604 641.013 119.41 644.578 Q121.238 648.119 124.849 648.119 Q128.484 648.119 130.289 644.578 Q132.118 641.013 132.118 633.907 Q132.118 626.777 130.289 623.235 Q128.484 619.671 124.849 619.671 M124.849 615.967 Q130.66 615.967 133.715 620.573 Q136.794 625.157 136.794 633.907 Q136.794 642.633 133.715 647.24 Q130.66 651.823 124.849 651.823 Q119.039 651.823 115.961 647.24 Q112.905 642.633 112.905 633.907 Q112.905 625.157 115.961 620.573 Q119.039 615.967 124.849 615.967 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M145.011 645.272 L149.896 645.272 L149.896 651.152 L145.011 651.152 L145.011 645.272 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M170.659 632.008 Q167.511 632.008 165.659 634.161 Q163.831 636.314 163.831 640.064 Q163.831 643.791 165.659 645.967 Q167.511 648.119 170.659 648.119 Q173.807 648.119 175.636 645.967 Q177.488 643.791 177.488 640.064 Q177.488 636.314 175.636 634.161 Q173.807 632.008 170.659 632.008 M179.942 617.356 L179.942 621.615 Q178.182 620.782 176.377 620.342 Q174.595 619.902 172.835 619.902 Q168.206 619.902 165.752 623.027 Q163.321 626.152 162.974 632.471 Q164.34 630.458 166.4 629.393 Q168.46 628.305 170.937 628.305 Q176.145 628.305 179.155 631.476 Q182.187 634.624 182.187 640.064 Q182.187 645.388 179.039 648.606 Q175.891 651.823 170.659 651.823 Q164.664 651.823 161.493 647.24 Q158.321 642.633 158.321 633.907 Q158.321 625.712 162.21 620.851 Q166.099 615.967 172.65 615.967 Q174.409 615.967 176.192 616.314 Q177.997 616.661 179.942 617.356 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M125.914 447.047 Q122.303 447.047 120.474 450.612 Q118.669 454.154 118.669 461.283 Q118.669 468.39 120.474 471.954 Q122.303 475.496 125.914 475.496 Q129.548 475.496 131.354 471.954 Q133.183 468.39 133.183 461.283 Q133.183 454.154 131.354 450.612 Q129.548 447.047 125.914 447.047 M125.914 443.343 Q131.724 443.343 134.78 447.95 Q137.859 452.533 137.859 461.283 Q137.859 470.01 134.78 474.616 Q131.724 479.2 125.914 479.2 Q120.104 479.2 117.025 474.616 Q113.97 470.01 113.97 461.283 Q113.97 452.533 117.025 447.95 Q120.104 443.343 125.914 443.343 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M146.076 472.649 L150.96 472.649 L150.96 478.528 L146.076 478.528 L146.076 472.649 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M159.965 443.968 L182.187 443.968 L182.187 445.959 L169.641 478.528 L164.757 478.528 L176.562 447.904 L159.965 447.904 L159.965 443.968 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M125.104 274.424 Q121.493 274.424 119.664 277.989 Q117.859 281.53 117.859 288.66 Q117.859 295.766 119.664 299.331 Q121.493 302.873 125.104 302.873 Q128.738 302.873 130.544 299.331 Q132.373 295.766 132.373 288.66 Q132.373 281.53 130.544 277.989 Q128.738 274.424 125.104 274.424 M125.104 270.72 Q130.914 270.72 133.97 275.327 Q137.048 279.91 137.048 288.66 Q137.048 297.387 133.97 301.993 Q130.914 306.576 125.104 306.576 Q119.294 306.576 116.215 301.993 Q113.16 297.387 113.16 288.66 Q113.16 279.91 116.215 275.327 Q119.294 270.72 125.104 270.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M145.266 300.025 L150.15 300.025 L150.15 305.905 L145.266 305.905 L145.266 300.025 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M170.335 289.493 Q167.002 289.493 165.081 291.276 Q163.183 293.058 163.183 296.183 Q163.183 299.308 165.081 301.09 Q167.002 302.873 170.335 302.873 Q173.669 302.873 175.59 301.09 Q177.511 299.285 177.511 296.183 Q177.511 293.058 175.59 291.276 Q173.692 289.493 170.335 289.493 M165.659 287.502 Q162.65 286.762 160.96 284.702 Q159.294 282.641 159.294 279.678 Q159.294 275.535 162.233 273.127 Q165.196 270.72 170.335 270.72 Q175.497 270.72 178.437 273.127 Q181.377 275.535 181.377 279.678 Q181.377 282.641 179.687 284.702 Q178.02 286.762 175.034 287.502 Q178.414 288.289 180.289 290.581 Q182.187 292.873 182.187 296.183 Q182.187 301.206 179.108 303.891 Q176.053 306.576 170.335 306.576 Q164.618 306.576 161.539 303.891 Q158.484 301.206 158.484 296.183 Q158.484 292.873 160.382 290.581 Q162.28 288.289 165.659 287.502 M163.946 280.118 Q163.946 282.803 165.613 284.308 Q167.303 285.813 170.335 285.813 Q173.345 285.813 175.034 284.308 Q176.747 282.803 176.747 280.118 Q176.747 277.433 175.034 275.928 Q173.345 274.424 170.335 274.424 Q167.303 274.424 165.613 275.928 Q163.946 277.433 163.946 280.118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M125.197 101.8 Q121.586 101.8 119.757 105.365 Q117.951 108.907 117.951 116.036 Q117.951 123.143 119.757 126.708 Q121.586 130.249 125.197 130.249 Q128.831 130.249 130.636 126.708 Q132.465 123.143 132.465 116.036 Q132.465 108.907 130.636 105.365 Q128.831 101.8 125.197 101.8 M125.197 98.0967 Q131.007 98.0967 134.062 102.703 Q137.141 107.286 137.141 116.036 Q137.141 124.763 134.062 129.37 Q131.007 133.953 125.197 133.953 Q119.386 133.953 116.308 129.37 Q113.252 124.763 113.252 116.036 Q113.252 107.286 116.308 102.703 Q119.386 98.0967 125.197 98.0967 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M145.359 127.402 L150.243 127.402 L150.243 133.282 L145.359 133.282 L145.359 127.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M160.567 132.564 L160.567 128.305 Q162.326 129.138 164.132 129.578 Q165.937 130.018 167.673 130.018 Q172.303 130.018 174.733 126.916 Q177.187 123.791 177.534 117.448 Q176.192 119.439 174.132 120.504 Q172.071 121.569 169.571 121.569 Q164.386 121.569 161.354 118.444 Q158.345 115.296 158.345 109.856 Q158.345 104.532 161.493 101.314 Q164.641 98.0967 169.872 98.0967 Q175.868 98.0967 179.016 102.703 Q182.187 107.286 182.187 116.036 Q182.187 124.208 178.298 129.092 Q174.432 133.953 167.882 133.953 Q166.122 133.953 164.317 133.606 Q162.511 133.259 160.567 132.564 M169.872 117.911 Q173.02 117.911 174.849 115.759 Q176.701 113.606 176.701 109.856 Q176.701 106.129 174.849 103.976 Q173.02 101.8 169.872 101.8 Q166.724 101.8 164.872 103.976 Q163.044 106.129 163.044 109.856 Q163.044 113.606 164.872 115.759 Q166.724 117.911 169.872 117.911 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip592)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,331.292 325.239,436.29 357.991,567.338 390.744,654.567 423.496,694.619 456.249,732.031 489.001,810.14 521.754,803.081 554.507,798.985 \n",
       "  587.259,840.697 652.764,880.775 718.269,834.891 783.775,915.808 849.28,915.282 947.537,949.011 1045.8,930.738 1176.81,969.325 1307.82,989.3 1471.58,1005.96 \n",
       "  1668.09,994.162 1864.61,951.876 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip590)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip590)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip590)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ],
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"490\" height=\"300\" viewBox=\"0 0 1960 1200\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip560\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1960\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip560)\" d=\"\n",
       "M0 1200 L1960 1200 L1960 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip561\">\n",
       "    <rect x=\"392\" y=\"0\" width=\"1373\" height=\"1200\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip560)\" d=\"\n",
       "M211.587 1033.88 L1912.76 1033.88 L1912.76 47.2441 L211.587 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip562\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"1702\" height=\"988\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 1912.76,1033.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.981,1033.88 226.981,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  554.507,1033.88 554.507,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  882.032,1033.88 882.032,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1209.56,1033.88 1209.56,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1537.08,1033.88 1537.08,1014.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1864.61,1033.88 1864.61,1014.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M226.981 1061.28 Q223.37 1061.28 221.541 1064.84 Q219.735 1068.39 219.735 1075.51 Q219.735 1082.62 221.541 1086.19 Q223.37 1089.73 226.981 1089.73 Q230.615 1089.73 232.421 1086.19 Q234.249 1082.62 234.249 1075.51 Q234.249 1068.39 232.421 1064.84 Q230.615 1061.28 226.981 1061.28 M226.981 1057.58 Q232.791 1057.58 235.846 1062.18 Q238.925 1066.77 238.925 1075.51 Q238.925 1084.24 235.846 1088.85 Q232.791 1093.43 226.981 1093.43 Q221.171 1093.43 218.092 1088.85 Q215.036 1084.24 215.036 1075.51 Q215.036 1066.77 218.092 1062.18 Q221.171 1057.58 226.981 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M529.194 1088.83 L536.833 1088.83 L536.833 1062.46 L528.523 1064.13 L528.523 1059.87 L536.787 1058.2 L541.463 1058.2 L541.463 1088.83 L549.101 1088.83 L549.101 1092.76 L529.194 1092.76 L529.194 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M568.546 1061.28 Q564.935 1061.28 563.106 1064.84 Q561.301 1068.39 561.301 1075.51 Q561.301 1082.62 563.106 1086.19 Q564.935 1089.73 568.546 1089.73 Q572.18 1089.73 573.986 1086.19 Q575.814 1082.62 575.814 1075.51 Q575.814 1068.39 573.986 1064.84 Q572.18 1061.28 568.546 1061.28 M568.546 1057.58 Q574.356 1057.58 577.412 1062.18 Q580.49 1066.77 580.49 1075.51 Q580.49 1084.24 577.412 1088.85 Q574.356 1093.43 568.546 1093.43 Q562.736 1093.43 559.657 1088.85 Q556.601 1084.24 556.601 1075.51 Q556.601 1066.77 559.657 1062.18 Q562.736 1057.58 568.546 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M860.806 1088.83 L877.125 1088.83 L877.125 1092.76 L855.181 1092.76 L855.181 1088.83 Q857.843 1086.07 862.426 1081.44 Q867.032 1076.79 868.213 1075.45 Q870.458 1072.92 871.338 1071.19 Q872.241 1069.43 872.241 1067.74 Q872.241 1064.98 870.296 1063.25 Q868.375 1061.51 865.273 1061.51 Q863.074 1061.51 860.62 1062.27 Q858.19 1063.04 855.412 1064.59 L855.412 1059.87 Q858.236 1058.73 860.69 1058.15 Q863.144 1057.58 865.181 1057.58 Q870.551 1057.58 873.745 1060.26 Q876.94 1062.95 876.94 1067.44 Q876.94 1069.57 876.13 1071.49 Q875.343 1073.39 873.236 1075.98 Q872.657 1076.65 869.556 1079.87 Q866.454 1083.06 860.806 1088.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M896.94 1061.28 Q893.329 1061.28 891.5 1064.84 Q889.694 1068.39 889.694 1075.51 Q889.694 1082.62 891.5 1086.19 Q893.329 1089.73 896.94 1089.73 Q900.574 1089.73 902.379 1086.19 Q904.208 1082.62 904.208 1075.51 Q904.208 1068.39 902.379 1064.84 Q900.574 1061.28 896.94 1061.28 M896.94 1057.58 Q902.75 1057.58 905.805 1062.18 Q908.884 1066.77 908.884 1075.51 Q908.884 1084.24 905.805 1088.85 Q902.75 1093.43 896.94 1093.43 Q891.129 1093.43 888.051 1088.85 Q884.995 1084.24 884.995 1075.51 Q884.995 1066.77 888.051 1062.18 Q891.129 1057.58 896.94 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1198.4 1074.13 Q1201.76 1074.84 1203.63 1077.11 Q1205.53 1079.38 1205.53 1082.71 Q1205.53 1087.83 1202.01 1090.63 Q1198.49 1093.43 1192.01 1093.43 Q1189.84 1093.43 1187.52 1092.99 Q1185.23 1092.58 1182.78 1091.72 L1182.78 1087.2 Q1184.72 1088.34 1187.04 1088.92 Q1189.35 1089.5 1191.87 1089.5 Q1196.27 1089.5 1198.56 1087.76 Q1200.88 1086.02 1200.88 1082.71 Q1200.88 1079.66 1198.72 1077.95 Q1196.6 1076.21 1192.78 1076.21 L1188.75 1076.21 L1188.75 1072.37 L1192.96 1072.37 Q1196.41 1072.37 1198.24 1071 Q1200.07 1069.61 1200.07 1067.02 Q1200.07 1064.36 1198.17 1062.95 Q1196.29 1061.51 1192.78 1061.51 Q1190.85 1061.51 1188.66 1061.93 Q1186.46 1062.34 1183.82 1063.22 L1183.82 1059.06 Q1186.48 1058.32 1188.79 1057.95 Q1191.13 1057.58 1193.19 1057.58 Q1198.52 1057.58 1201.62 1060.01 Q1204.72 1062.41 1204.72 1066.53 Q1204.72 1069.4 1203.08 1071.39 Q1201.43 1073.36 1198.4 1074.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1224.4 1061.28 Q1220.78 1061.28 1218.96 1064.84 Q1217.15 1068.39 1217.15 1075.51 Q1217.15 1082.62 1218.96 1086.19 Q1220.78 1089.73 1224.4 1089.73 Q1228.03 1089.73 1229.84 1086.19 Q1231.66 1082.62 1231.66 1075.51 Q1231.66 1068.39 1229.84 1064.84 Q1228.03 1061.28 1224.4 1061.28 M1224.4 1057.58 Q1230.21 1057.58 1233.26 1062.18 Q1236.34 1066.77 1236.34 1075.51 Q1236.34 1084.24 1233.26 1088.85 Q1230.21 1093.43 1224.4 1093.43 Q1218.59 1093.43 1215.51 1088.85 Q1212.45 1084.24 1212.45 1075.51 Q1212.45 1066.77 1215.51 1062.18 Q1218.59 1057.58 1224.4 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1525.26 1062.27 L1513.45 1080.72 L1525.26 1080.72 L1525.26 1062.27 M1524.03 1058.2 L1529.91 1058.2 L1529.91 1080.72 L1534.84 1080.72 L1534.84 1084.61 L1529.91 1084.61 L1529.91 1092.76 L1525.26 1092.76 L1525.26 1084.61 L1509.65 1084.61 L1509.65 1080.1 L1524.03 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1552.57 1061.28 Q1548.96 1061.28 1547.13 1064.84 Q1545.32 1068.39 1545.32 1075.51 Q1545.32 1082.62 1547.13 1086.19 Q1548.96 1089.73 1552.57 1089.73 Q1556.2 1089.73 1558.01 1086.19 Q1559.84 1082.62 1559.84 1075.51 Q1559.84 1068.39 1558.01 1064.84 Q1556.2 1061.28 1552.57 1061.28 M1552.57 1057.58 Q1558.38 1057.58 1561.44 1062.18 Q1564.51 1066.77 1564.51 1075.51 Q1564.51 1084.24 1561.44 1088.85 Q1558.38 1093.43 1552.57 1093.43 Q1546.76 1093.43 1543.68 1088.85 Q1540.63 1084.24 1540.63 1075.51 Q1540.63 1066.77 1543.68 1062.18 Q1546.76 1057.58 1552.57 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1839.31 1058.2 L1857.67 1058.2 L1857.67 1062.14 L1843.59 1062.14 L1843.59 1070.61 Q1844.61 1070.26 1845.63 1070.1 Q1846.65 1069.91 1847.67 1069.91 Q1853.45 1069.91 1856.83 1073.08 Q1860.21 1076.26 1860.21 1081.67 Q1860.21 1087.25 1856.74 1090.35 Q1853.27 1093.43 1846.95 1093.43 Q1844.77 1093.43 1842.5 1093.06 Q1840.26 1092.69 1837.85 1091.95 L1837.85 1087.25 Q1839.93 1088.39 1842.16 1088.94 Q1844.38 1089.5 1846.86 1089.5 Q1850.86 1089.5 1853.2 1087.39 Q1855.54 1085.28 1855.54 1081.67 Q1855.54 1078.06 1853.2 1075.95 Q1850.86 1073.85 1846.86 1073.85 Q1844.98 1073.85 1843.11 1074.26 Q1841.25 1074.68 1839.31 1075.56 L1839.31 1058.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1879.42 1061.28 Q1875.81 1061.28 1873.98 1064.84 Q1872.18 1068.39 1872.18 1075.51 Q1872.18 1082.62 1873.98 1086.19 Q1875.81 1089.73 1879.42 1089.73 Q1883.06 1089.73 1884.86 1086.19 Q1886.69 1082.62 1886.69 1075.51 Q1886.69 1068.39 1884.86 1064.84 Q1883.06 1061.28 1879.42 1061.28 M1879.42 1057.58 Q1885.23 1057.58 1888.29 1062.18 Q1891.37 1066.77 1891.37 1075.51 Q1891.37 1084.24 1888.29 1088.85 Q1885.23 1093.43 1879.42 1093.43 Q1873.61 1093.43 1870.54 1088.85 Q1867.48 1084.24 1867.48 1075.51 Q1867.48 1066.77 1870.54 1062.18 Q1873.61 1057.58 1879.42 1057.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M982.361 1152.86 L982.361 1155.72 L955.435 1155.72 Q955.816 1161.77 959.063 1164.95 Q962.341 1168.1 968.166 1168.1 Q971.54 1168.1 974.691 1167.27 Q977.874 1166.45 980.993 1164.79 L980.993 1170.33 Q977.842 1171.67 974.532 1172.37 Q971.221 1173.07 967.816 1173.07 Q959.286 1173.07 954.289 1168.1 Q949.323 1163.14 949.323 1154.67 Q949.323 1145.92 954.034 1140.79 Q958.777 1135.64 966.797 1135.64 Q973.991 1135.64 978.16 1140.28 Q982.361 1144.9 982.361 1152.86 M976.505 1151.14 Q976.441 1146.33 973.8 1143.47 Q971.19 1140.6 966.861 1140.6 Q961.959 1140.6 958.999 1143.37 Q956.071 1146.14 955.625 1151.17 L976.505 1151.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M997.639 1166.8 L997.639 1185.7 L991.751 1185.7 L991.751 1136.5 L997.639 1136.5 L997.639 1141.91 Q999.485 1138.72 1002.29 1137.2 Q1005.12 1135.64 1009.03 1135.64 Q1015.53 1135.64 1019.57 1140.79 Q1023.64 1145.95 1023.64 1154.35 Q1023.64 1162.75 1019.57 1167.91 Q1015.53 1173.07 1009.03 1173.07 Q1005.12 1173.07 1002.29 1171.54 Q999.485 1169.98 997.639 1166.8 M1017.56 1154.35 Q1017.56 1147.89 1014.89 1144.23 Q1012.25 1140.54 1007.6 1140.54 Q1002.95 1140.54 1000.28 1144.23 Q997.639 1147.89 997.639 1154.35 Q997.639 1160.81 1000.28 1164.51 Q1002.95 1168.17 1007.6 1168.17 Q1012.25 1168.17 1014.89 1164.51 Q1017.56 1160.81 1017.56 1154.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1047.16 1140.6 Q1042.45 1140.6 1039.72 1144.29 Q1036.98 1147.95 1036.98 1154.35 Q1036.98 1160.75 1039.68 1164.44 Q1042.42 1168.1 1047.16 1168.1 Q1051.84 1168.1 1054.58 1164.41 Q1057.32 1160.72 1057.32 1154.35 Q1057.32 1148.02 1054.58 1144.33 Q1051.84 1140.6 1047.16 1140.6 M1047.16 1135.64 Q1054.8 1135.64 1059.16 1140.6 Q1063.52 1145.57 1063.52 1154.35 Q1063.52 1163.1 1059.16 1168.1 Q1054.8 1173.07 1047.16 1173.07 Q1039.49 1173.07 1035.13 1168.1 Q1030.8 1163.1 1030.8 1154.35 Q1030.8 1145.57 1035.13 1140.6 Q1039.49 1135.64 1047.16 1135.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1098.89 1137.86 L1098.89 1143.34 Q1096.4 1141.97 1093.89 1141.3 Q1091.41 1140.6 1088.86 1140.6 Q1083.16 1140.6 1080.01 1144.23 Q1076.86 1147.83 1076.86 1154.35 Q1076.86 1160.88 1080.01 1164.51 Q1083.16 1168.1 1088.86 1168.1 Q1091.41 1168.1 1093.89 1167.43 Q1096.4 1166.73 1098.89 1165.36 L1098.89 1170.78 Q1096.43 1171.92 1093.79 1172.49 Q1091.18 1173.07 1088.22 1173.07 Q1080.17 1173.07 1075.43 1168.01 Q1070.69 1162.95 1070.69 1154.35 Q1070.69 1145.63 1075.46 1140.63 Q1080.27 1135.64 1088.61 1135.64 Q1091.31 1135.64 1093.89 1136.21 Q1096.47 1136.75 1098.89 1137.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1138.7 1150.63 L1138.7 1172.14 L1132.85 1172.14 L1132.85 1150.82 Q1132.85 1145.76 1130.87 1143.24 Q1128.9 1140.73 1124.95 1140.73 Q1120.21 1140.73 1117.47 1143.75 Q1114.74 1146.78 1114.74 1152 L1114.74 1172.14 L1108.85 1172.14 L1108.85 1122.62 L1114.74 1122.62 L1114.74 1142.03 Q1116.84 1138.82 1119.67 1137.23 Q1122.53 1135.64 1126.26 1135.64 Q1132.4 1135.64 1135.55 1139.46 Q1138.7 1143.24 1138.7 1150.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1173.11 1137.55 L1173.11 1143.08 Q1170.63 1141.81 1167.95 1141.18 Q1165.28 1140.54 1162.42 1140.54 Q1158.05 1140.54 1155.86 1141.88 Q1153.69 1143.21 1153.69 1145.89 Q1153.69 1147.92 1155.25 1149.1 Q1156.81 1150.25 1161.52 1151.3 L1163.53 1151.74 Q1169.77 1153.08 1172.38 1155.53 Q1175.02 1157.95 1175.02 1162.31 Q1175.02 1167.27 1171.07 1170.17 Q1167.16 1173.07 1160.28 1173.07 Q1157.42 1173.07 1154.3 1172.49 Q1151.21 1171.95 1147.77 1170.84 L1147.77 1164.79 Q1151.02 1166.48 1154.17 1167.34 Q1157.32 1168.17 1160.41 1168.17 Q1164.55 1168.17 1166.78 1166.77 Q1169 1165.33 1169 1162.75 Q1169 1160.37 1167.38 1159.09 Q1165.79 1157.82 1160.35 1156.64 L1158.31 1156.17 Q1152.87 1155.02 1150.45 1152.67 Q1148.03 1150.28 1148.03 1146.14 Q1148.03 1141.11 1151.59 1138.37 Q1155.16 1135.64 1161.72 1135.64 Q1164.96 1135.64 1167.83 1136.11 Q1170.69 1136.59 1173.11 1137.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,979.119 1912.76,979.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,806.495 1912.76,806.495 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,633.872 1912.76,633.872 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,461.248 1912.76,461.248 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,288.625 1912.76,288.625 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.587,116.002 1912.76,116.002 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,1033.88 211.587,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,979.119 230.485,979.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,806.495 230.485,806.495 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,633.872 230.485,633.872 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,461.248 230.485,461.248 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,288.625 230.485,288.625 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.587,116.002 230.485,116.002 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M124.525 964.917 Q120.914 964.917 119.086 968.482 Q117.28 972.024 117.28 979.153 Q117.28 986.26 119.086 989.825 Q120.914 993.366 124.525 993.366 Q128.16 993.366 129.965 989.825 Q131.794 986.26 131.794 979.153 Q131.794 972.024 129.965 968.482 Q128.16 964.917 124.525 964.917 M124.525 961.214 Q130.336 961.214 133.391 965.82 Q136.47 970.403 136.47 979.153 Q136.47 987.88 133.391 992.487 Q130.336 997.07 124.525 997.07 Q118.715 997.07 115.637 992.487 Q112.581 987.88 112.581 979.153 Q112.581 970.403 115.637 965.82 Q118.715 961.214 124.525 961.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M144.687 990.519 L149.572 990.519 L149.572 996.399 L144.687 996.399 L144.687 990.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M172.604 965.913 L160.798 984.362 L172.604 984.362 L172.604 965.913 M171.377 961.839 L177.257 961.839 L177.257 984.362 L182.187 984.362 L182.187 988.251 L177.257 988.251 L177.257 996.399 L172.604 996.399 L172.604 988.251 L157.002 988.251 L157.002 983.737 L171.377 961.839 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M126.007 792.294 Q122.396 792.294 120.567 795.859 Q118.761 799.4 118.761 806.53 Q118.761 813.636 120.567 817.201 Q122.396 820.743 126.007 820.743 Q129.641 820.743 131.447 817.201 Q133.275 813.636 133.275 806.53 Q133.275 799.4 131.447 795.859 Q129.641 792.294 126.007 792.294 M126.007 788.59 Q131.817 788.59 134.873 793.197 Q137.951 797.78 137.951 806.53 Q137.951 815.257 134.873 819.863 Q131.817 824.447 126.007 824.447 Q120.197 824.447 117.118 819.863 Q114.062 815.257 114.062 806.53 Q114.062 797.78 117.118 793.197 Q120.197 788.59 126.007 788.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M146.169 817.896 L151.053 817.896 L151.053 823.775 L146.169 823.775 L146.169 817.896 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M161.284 789.215 L179.641 789.215 L179.641 793.15 L165.567 793.15 L165.567 801.623 Q166.585 801.275 167.604 801.113 Q168.622 800.928 169.641 800.928 Q175.428 800.928 178.807 804.099 Q182.187 807.271 182.187 812.687 Q182.187 818.266 178.715 821.368 Q175.243 824.447 168.923 824.447 Q166.747 824.447 164.479 824.076 Q162.233 823.706 159.826 822.965 L159.826 818.266 Q161.909 819.4 164.132 819.956 Q166.354 820.511 168.831 820.511 Q172.835 820.511 175.173 818.405 Q177.511 816.298 177.511 812.687 Q177.511 809.076 175.173 806.97 Q172.835 804.863 168.831 804.863 Q166.956 804.863 165.081 805.28 Q163.229 805.697 161.284 806.576 L161.284 789.215 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M124.849 619.671 Q121.238 619.671 119.41 623.235 Q117.604 626.777 117.604 633.907 Q117.604 641.013 119.41 644.578 Q121.238 648.119 124.849 648.119 Q128.484 648.119 130.289 644.578 Q132.118 641.013 132.118 633.907 Q132.118 626.777 130.289 623.235 Q128.484 619.671 124.849 619.671 M124.849 615.967 Q130.66 615.967 133.715 620.573 Q136.794 625.157 136.794 633.907 Q136.794 642.633 133.715 647.24 Q130.66 651.823 124.849 651.823 Q119.039 651.823 115.961 647.24 Q112.905 642.633 112.905 633.907 Q112.905 625.157 115.961 620.573 Q119.039 615.967 124.849 615.967 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M145.011 645.272 L149.896 645.272 L149.896 651.152 L145.011 651.152 L145.011 645.272 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M170.659 632.008 Q167.511 632.008 165.659 634.161 Q163.831 636.314 163.831 640.064 Q163.831 643.791 165.659 645.967 Q167.511 648.119 170.659 648.119 Q173.807 648.119 175.636 645.967 Q177.488 643.791 177.488 640.064 Q177.488 636.314 175.636 634.161 Q173.807 632.008 170.659 632.008 M179.942 617.356 L179.942 621.615 Q178.182 620.782 176.377 620.342 Q174.595 619.902 172.835 619.902 Q168.206 619.902 165.752 623.027 Q163.321 626.152 162.974 632.471 Q164.34 630.458 166.4 629.393 Q168.46 628.305 170.937 628.305 Q176.145 628.305 179.155 631.476 Q182.187 634.624 182.187 640.064 Q182.187 645.388 179.039 648.606 Q175.891 651.823 170.659 651.823 Q164.664 651.823 161.493 647.24 Q158.321 642.633 158.321 633.907 Q158.321 625.712 162.21 620.851 Q166.099 615.967 172.65 615.967 Q174.409 615.967 176.192 616.314 Q177.997 616.661 179.942 617.356 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M125.914 447.047 Q122.303 447.047 120.474 450.612 Q118.669 454.154 118.669 461.283 Q118.669 468.39 120.474 471.954 Q122.303 475.496 125.914 475.496 Q129.548 475.496 131.354 471.954 Q133.183 468.39 133.183 461.283 Q133.183 454.154 131.354 450.612 Q129.548 447.047 125.914 447.047 M125.914 443.343 Q131.724 443.343 134.78 447.95 Q137.859 452.533 137.859 461.283 Q137.859 470.01 134.78 474.616 Q131.724 479.2 125.914 479.2 Q120.104 479.2 117.025 474.616 Q113.97 470.01 113.97 461.283 Q113.97 452.533 117.025 447.95 Q120.104 443.343 125.914 443.343 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M146.076 472.649 L150.96 472.649 L150.96 478.528 L146.076 478.528 L146.076 472.649 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M159.965 443.968 L182.187 443.968 L182.187 445.959 L169.641 478.528 L164.757 478.528 L176.562 447.904 L159.965 447.904 L159.965 443.968 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M125.104 274.424 Q121.493 274.424 119.664 277.989 Q117.859 281.53 117.859 288.66 Q117.859 295.766 119.664 299.331 Q121.493 302.873 125.104 302.873 Q128.738 302.873 130.544 299.331 Q132.373 295.766 132.373 288.66 Q132.373 281.53 130.544 277.989 Q128.738 274.424 125.104 274.424 M125.104 270.72 Q130.914 270.72 133.97 275.327 Q137.048 279.91 137.048 288.66 Q137.048 297.387 133.97 301.993 Q130.914 306.576 125.104 306.576 Q119.294 306.576 116.215 301.993 Q113.16 297.387 113.16 288.66 Q113.16 279.91 116.215 275.327 Q119.294 270.72 125.104 270.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M145.266 300.025 L150.15 300.025 L150.15 305.905 L145.266 305.905 L145.266 300.025 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M170.335 289.493 Q167.002 289.493 165.081 291.276 Q163.183 293.058 163.183 296.183 Q163.183 299.308 165.081 301.09 Q167.002 302.873 170.335 302.873 Q173.669 302.873 175.59 301.09 Q177.511 299.285 177.511 296.183 Q177.511 293.058 175.59 291.276 Q173.692 289.493 170.335 289.493 M165.659 287.502 Q162.65 286.762 160.96 284.702 Q159.294 282.641 159.294 279.678 Q159.294 275.535 162.233 273.127 Q165.196 270.72 170.335 270.72 Q175.497 270.72 178.437 273.127 Q181.377 275.535 181.377 279.678 Q181.377 282.641 179.687 284.702 Q178.02 286.762 175.034 287.502 Q178.414 288.289 180.289 290.581 Q182.187 292.873 182.187 296.183 Q182.187 301.206 179.108 303.891 Q176.053 306.576 170.335 306.576 Q164.618 306.576 161.539 303.891 Q158.484 301.206 158.484 296.183 Q158.484 292.873 160.382 290.581 Q162.28 288.289 165.659 287.502 M163.946 280.118 Q163.946 282.803 165.613 284.308 Q167.303 285.813 170.335 285.813 Q173.345 285.813 175.034 284.308 Q176.747 282.803 176.747 280.118 Q176.747 277.433 175.034 275.928 Q173.345 274.424 170.335 274.424 Q167.303 274.424 165.613 275.928 Q163.946 277.433 163.946 280.118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M125.197 101.8 Q121.586 101.8 119.757 105.365 Q117.951 108.907 117.951 116.036 Q117.951 123.143 119.757 126.708 Q121.586 130.249 125.197 130.249 Q128.831 130.249 130.636 126.708 Q132.465 123.143 132.465 116.036 Q132.465 108.907 130.636 105.365 Q128.831 101.8 125.197 101.8 M125.197 98.0967 Q131.007 98.0967 134.062 102.703 Q137.141 107.286 137.141 116.036 Q137.141 124.763 134.062 129.37 Q131.007 133.953 125.197 133.953 Q119.386 133.953 116.308 129.37 Q113.252 124.763 113.252 116.036 Q113.252 107.286 116.308 102.703 Q119.386 98.0967 125.197 98.0967 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M145.359 127.402 L150.243 127.402 L150.243 133.282 L145.359 133.282 L145.359 127.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M160.567 132.564 L160.567 128.305 Q162.326 129.138 164.132 129.578 Q165.937 130.018 167.673 130.018 Q172.303 130.018 174.733 126.916 Q177.187 123.791 177.534 117.448 Q176.192 119.439 174.132 120.504 Q172.071 121.569 169.571 121.569 Q164.386 121.569 161.354 118.444 Q158.345 115.296 158.345 109.856 Q158.345 104.532 161.493 101.314 Q164.641 98.0967 169.872 98.0967 Q175.868 98.0967 179.016 102.703 Q182.187 107.286 182.187 116.036 Q182.187 124.208 178.298 129.092 Q174.432 133.953 167.882 133.953 Q166.122 133.953 164.317 133.606 Q162.511 133.259 160.567 132.564 M169.872 117.911 Q173.02 117.911 174.849 115.759 Q176.701 113.606 176.701 109.856 Q176.701 106.129 174.849 103.976 Q173.02 101.8 169.872 101.8 Q166.724 101.8 164.872 103.976 Q163.044 106.129 163.044 109.856 Q163.044 113.606 164.872 115.759 Q166.724 117.911 169.872 117.911 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M34.6456 975.945 L40.1202 975.945 Q38.7515 978.427 38.0831 980.942 Q37.3829 983.424 37.3829 985.971 Q37.3829 991.668 41.0114 994.819 Q44.608 997.97 51.1328 997.97 Q57.6577 997.97 61.2861 994.819 Q64.8828 991.668 64.8828 985.971 Q64.8828 983.424 64.2144 980.942 Q63.5141 978.427 62.1455 975.945 L67.5563 975.945 Q68.7022 978.395 69.2751 981.037 Q69.848 983.647 69.848 986.607 Q69.848 994.66 64.7873 999.402 Q59.7265 1004.14 51.1328 1004.14 Q42.4118 1004.14 37.4147 999.37 Q32.4176 994.564 32.4176 986.225 Q32.4176 983.52 32.9906 980.942 Q33.5316 978.364 34.6456 975.945 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M38.7515 945.103 Q38.1786 946.09 37.924 947.267 Q37.6375 948.413 37.6375 949.813 Q37.6375 954.779 40.884 957.452 Q44.0987 960.094 50.1461 960.094 L68.925 960.094 L68.925 965.982 L33.277 965.982 L33.277 960.094 L38.8152 960.094 Q35.5687 958.248 34.0091 955.288 Q32.4176 952.328 32.4176 948.095 Q32.4176 947.49 32.5131 946.758 Q32.5768 946.026 32.7359 945.135 L38.7515 945.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M37.3829 926.579 Q37.3829 931.289 41.075 934.026 Q44.7353 936.764 51.1328 936.764 Q57.5304 936.764 61.2225 934.058 Q64.8828 931.321 64.8828 926.579 Q64.8828 921.9 61.1906 919.163 Q57.4985 916.425 51.1328 916.425 Q44.7989 916.425 41.1068 919.163 Q37.3829 921.9 37.3829 926.579 M32.4176 926.579 Q32.4176 918.94 37.3829 914.579 Q42.3481 910.219 51.1328 910.219 Q59.8857 910.219 64.8828 914.579 Q69.848 918.94 69.848 926.579 Q69.848 934.249 64.8828 938.61 Q59.8857 942.938 51.1328 942.938 Q42.3481 942.938 37.3829 938.61 Q32.4176 934.249 32.4176 926.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M34.3274 877.785 L39.8655 877.785 Q38.5924 880.268 37.9558 882.942 Q37.3192 885.615 37.3192 888.48 Q37.3192 892.84 38.656 895.037 Q39.9928 897.201 42.6664 897.201 Q44.7035 897.201 45.8811 895.641 Q47.0269 894.082 48.0773 889.371 L48.5229 887.366 Q49.8597 881.127 52.3105 878.518 Q54.7294 875.876 59.09 875.876 Q64.0552 875.876 66.9516 879.823 Q69.848 883.737 69.848 890.612 Q69.848 893.477 69.2751 896.596 Q68.734 899.684 67.62 903.121 L61.5726 903.121 Q63.2595 899.874 64.1189 896.723 Q64.9464 893.572 64.9464 890.485 Q64.9464 886.347 63.546 884.119 Q62.1137 881.891 59.5356 881.891 Q57.1484 881.891 55.8753 883.515 Q54.6021 885.106 53.4245 890.549 L52.9471 892.586 Q51.8012 898.028 49.4459 900.447 Q47.0588 902.866 42.9211 902.866 Q37.8922 902.866 35.1549 899.302 Q32.4176 895.737 32.4176 889.18 Q32.4176 885.934 32.8951 883.069 Q33.3725 880.204 34.3274 877.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M34.3274 843.824 L39.8655 843.824 Q38.5924 846.307 37.9558 848.981 Q37.3192 851.654 37.3192 854.519 Q37.3192 858.879 38.656 861.076 Q39.9928 863.24 42.6664 863.24 Q44.7035 863.24 45.8811 861.68 Q47.0269 860.121 48.0773 855.41 L48.5229 853.405 Q49.8597 847.166 52.3105 844.556 Q54.7294 841.915 59.09 841.915 Q64.0552 841.915 66.9516 845.861 Q69.848 849.776 69.848 856.651 Q69.848 859.516 69.2751 862.635 Q68.734 865.722 67.62 869.16 L61.5726 869.16 Q63.2595 865.913 64.1189 862.762 Q64.9464 859.611 64.9464 856.524 Q64.9464 852.386 63.546 850.158 Q62.1137 847.93 59.5356 847.93 Q57.1484 847.93 55.8753 849.554 Q54.6021 851.145 53.4245 856.588 L52.9471 858.625 Q51.8012 864.067 49.4459 866.486 Q47.0588 868.905 42.9211 868.905 Q37.8922 868.905 35.1549 865.341 Q32.4176 861.776 32.4176 855.219 Q32.4176 851.973 32.8951 849.108 Q33.3725 846.243 34.3274 843.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M49.6369 781.377 L52.5015 781.377 L52.5015 808.304 Q58.5489 807.922 61.7317 804.675 Q64.8828 801.397 64.8828 795.572 Q64.8828 792.199 64.0552 789.048 Q63.2277 785.865 61.5726 782.745 L67.1107 782.745 Q68.4475 785.897 69.1478 789.207 Q69.848 792.517 69.848 795.922 Q69.848 804.453 64.8828 809.45 Q59.9175 814.415 51.4511 814.415 Q42.6983 814.415 37.5739 809.704 Q32.4176 804.962 32.4176 796.941 Q32.4176 789.748 37.0646 785.578 Q41.6797 781.377 49.6369 781.377 M47.9181 787.233 Q43.112 787.297 40.2475 789.939 Q37.3829 792.549 37.3829 796.877 Q37.3829 801.779 40.152 804.739 Q42.9211 807.667 47.95 808.113 L47.9181 787.233 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M47.4089 742.132 L68.925 742.132 L68.925 747.989 L47.5999 747.989 Q42.5391 747.989 40.0247 749.962 Q37.5102 751.935 37.5102 755.882 Q37.5102 760.625 40.5339 763.362 Q43.5576 766.099 48.7775 766.099 L68.925 766.099 L68.925 771.987 L33.277 771.987 L33.277 766.099 L38.8152 766.099 Q35.6005 763.998 34.0091 761.166 Q32.4176 758.301 32.4176 754.577 Q32.4176 748.434 36.2371 745.283 Q40.0247 742.132 47.4089 742.132 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M23.1555 724.658 L33.277 724.658 L33.277 712.595 L37.8285 712.595 L37.8285 724.658 L57.1802 724.658 Q61.5408 724.658 62.7821 723.481 Q64.0234 722.271 64.0234 718.611 L64.0234 712.595 L68.925 712.595 L68.925 718.611 Q68.925 725.39 66.4105 727.969 Q63.8642 730.547 57.1802 730.547 L37.8285 730.547 L37.8285 734.844 L33.277 734.844 L33.277 730.547 L23.1555 730.547 L23.1555 724.658 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M38.7515 684.236 Q38.1786 685.223 37.924 686.401 Q37.6375 687.546 37.6375 688.947 Q37.6375 693.912 40.884 696.586 Q44.0987 699.227 50.1461 699.227 L68.925 699.227 L68.925 705.116 L33.277 705.116 L33.277 699.227 L38.8152 699.227 Q35.5687 697.381 34.0091 694.421 Q32.4176 691.461 32.4176 687.228 Q32.4176 686.623 32.5131 685.891 Q32.5768 685.159 32.7359 684.268 L38.7515 684.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M37.3829 665.712 Q37.3829 670.423 41.075 673.16 Q44.7353 675.897 51.1328 675.897 Q57.5304 675.897 61.2225 673.192 Q64.8828 670.454 64.8828 665.712 Q64.8828 661.033 61.1906 658.296 Q57.4985 655.559 51.1328 655.559 Q44.7989 655.559 41.1068 658.296 Q37.3829 661.033 37.3829 665.712 M32.4176 665.712 Q32.4176 658.073 37.3829 653.713 Q42.3481 649.352 51.1328 649.352 Q59.8857 649.352 64.8828 653.713 Q69.848 658.073 69.848 665.712 Q69.848 673.383 64.8828 677.743 Q59.8857 682.072 51.1328 682.072 Q42.3481 682.072 37.3829 677.743 Q32.4176 673.383 32.4176 665.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M63.5778 633.979 L82.4839 633.979 L82.4839 639.867 L33.277 639.867 L33.277 633.979 L38.6879 633.979 Q35.505 632.133 33.9772 629.332 Q32.4176 626.499 32.4176 622.584 Q32.4176 616.091 37.5739 612.049 Q42.7301 607.975 51.1328 607.975 Q59.5356 607.975 64.6918 612.049 Q69.848 616.091 69.848 622.584 Q69.848 626.499 68.3202 629.332 Q66.7606 632.133 63.5778 633.979 M51.1328 614.054 Q44.6716 614.054 41.0114 616.728 Q37.3192 619.37 37.3192 624.017 Q37.3192 628.664 41.0114 631.337 Q44.6716 633.979 51.1328 633.979 Q57.594 633.979 61.2861 631.337 Q64.9464 628.664 64.9464 624.017 Q64.9464 619.37 61.2861 616.728 Q57.594 614.054 51.1328 614.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M72.2351 583.435 Q78.6008 585.918 80.5424 588.273 Q82.4839 590.628 82.4839 594.575 L82.4839 599.254 L77.5823 599.254 L77.5823 595.817 Q77.5823 593.398 76.4365 592.061 Q75.2907 590.724 71.0257 589.101 L68.3521 588.05 L33.277 602.469 L33.277 596.262 L61.1588 585.122 L33.277 573.982 L33.277 567.776 L72.2351 583.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M37.3829 525.157 Q37.3829 529.868 41.075 532.605 Q44.7353 535.342 51.1328 535.342 Q57.5304 535.342 61.2225 532.637 Q64.8828 529.9 64.8828 525.157 Q64.8828 520.478 61.1906 517.741 Q57.4985 515.004 51.1328 515.004 Q44.7989 515.004 41.1068 517.741 Q37.3829 520.478 37.3829 525.157 M32.4176 525.157 Q32.4176 517.518 37.3829 513.158 Q42.3481 508.797 51.1328 508.797 Q59.8857 508.797 64.8828 513.158 Q69.848 517.518 69.848 525.157 Q69.848 532.828 64.8828 537.188 Q59.8857 541.517 51.1328 541.517 Q42.3481 541.517 37.3829 537.188 Q32.4176 532.828 32.4176 525.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M47.4089 469.457 L68.925 469.457 L68.925 475.314 L47.5999 475.314 Q42.5391 475.314 40.0247 477.287 Q37.5102 479.26 37.5102 483.207 Q37.5102 487.95 40.5339 490.687 Q43.5576 493.424 48.7775 493.424 L68.925 493.424 L68.925 499.312 L33.277 499.312 L33.277 493.424 L38.8152 493.424 Q35.6005 491.323 34.0091 488.491 Q32.4176 485.626 32.4176 481.902 Q32.4176 475.759 36.2371 472.608 Q40.0247 469.457 47.4089 469.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M47.4089 407.423 L68.925 407.423 L68.925 413.28 L47.5999 413.28 Q42.5391 413.28 40.0247 415.253 Q37.5102 417.227 37.5102 421.173 Q37.5102 425.916 40.5339 428.653 Q43.5576 431.39 48.7775 431.39 L68.925 431.39 L68.925 437.279 L19.3998 437.279 L19.3998 431.39 L38.8152 431.39 Q35.6005 429.29 34.0091 426.457 Q32.4176 423.592 32.4176 419.868 Q32.4176 413.725 36.2371 410.574 Q40.0247 407.423 47.4089 407.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M37.3829 381.929 Q37.3829 386.639 41.075 389.377 Q44.7353 392.114 51.1328 392.114 Q57.5304 392.114 61.2225 389.408 Q64.8828 386.671 64.8828 381.929 Q64.8828 377.25 61.1906 374.513 Q57.4985 371.775 51.1328 371.775 Q44.7989 371.775 41.1068 374.513 Q37.3829 377.25 37.3829 381.929 M32.4176 381.929 Q32.4176 374.29 37.3829 369.929 Q42.3481 365.569 51.1328 365.569 Q59.8857 365.569 64.8828 369.929 Q69.848 374.29 69.848 381.929 Q69.848 389.599 64.8828 393.96 Q59.8857 398.289 51.1328 398.289 Q42.3481 398.289 37.3829 393.96 Q32.4176 389.599 32.4176 381.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M19.3998 355.861 L19.3998 350.005 L68.925 350.005 L68.925 355.861 L19.3998 355.861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M38.6879 314.293 L19.3998 314.293 L19.3998 308.437 L68.925 308.437 L68.925 314.293 L63.5778 314.293 Q66.7606 316.139 68.3202 318.972 Q69.848 321.773 69.848 325.72 Q69.848 332.181 64.6918 336.255 Q59.5356 340.297 51.1328 340.297 Q42.7301 340.297 37.5739 336.255 Q32.4176 332.181 32.4176 325.72 Q32.4176 321.773 33.9772 318.972 Q35.505 316.139 38.6879 314.293 M51.1328 334.25 Q57.594 334.25 61.2861 331.608 Q64.9464 328.934 64.9464 324.287 Q64.9464 319.64 61.2861 316.967 Q57.594 314.293 51.1328 314.293 Q44.6716 314.293 41.0114 316.967 Q37.3192 319.64 37.3192 324.287 Q37.3192 328.934 41.0114 331.608 Q44.6716 334.25 51.1328 334.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M37.3829 282.56 Q37.3829 287.271 41.075 290.008 Q44.7353 292.745 51.1328 292.745 Q57.5304 292.745 61.2225 290.04 Q64.8828 287.303 64.8828 282.56 Q64.8828 277.881 61.1906 275.144 Q57.4985 272.407 51.1328 272.407 Q44.7989 272.407 41.1068 275.144 Q37.3829 277.881 37.3829 282.56 M32.4176 282.56 Q32.4176 274.921 37.3829 270.561 Q42.3481 266.2 51.1328 266.2 Q59.8857 266.2 64.8828 270.561 Q69.848 274.921 69.848 282.56 Q69.848 290.231 64.8828 294.591 Q59.8857 298.92 51.1328 298.92 Q42.3481 298.92 37.3829 294.591 Q32.4176 290.231 32.4176 282.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M54.8568 257.097 L33.277 257.097 L33.277 251.241 L54.634 251.241 Q59.6947 251.241 62.241 249.267 Q64.7554 247.294 64.7554 243.347 Q64.7554 238.605 61.7317 235.868 Q58.708 233.099 53.4881 233.099 L33.277 233.099 L33.277 227.242 L68.925 227.242 L68.925 233.099 L63.4505 233.099 Q66.697 235.231 68.2884 238.064 Q69.848 240.865 69.848 244.589 Q69.848 250.732 66.0286 253.914 Q62.2092 257.097 54.8568 257.097 M32.4176 242.361 L32.4176 242.361 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M23.1555 209.386 L33.277 209.386 L33.277 197.323 L37.8285 197.323 L37.8285 209.386 L57.1802 209.386 Q61.5408 209.386 62.7821 208.209 Q64.0234 206.999 64.0234 203.339 L64.0234 197.323 L68.925 197.323 L68.925 203.339 Q68.925 210.118 66.4105 212.696 Q63.8642 215.275 57.1802 215.275 L37.8285 215.275 L37.8285 219.571 L33.277 219.571 L33.277 215.275 L23.1555 215.275 L23.1555 209.386 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M34.3274 146.175 L39.8655 146.175 Q38.5924 148.657 37.9558 151.331 Q37.3192 154.005 37.3192 156.869 Q37.3192 161.23 38.656 163.426 Q39.9928 165.59 42.6664 165.59 Q44.7035 165.59 45.8811 164.031 Q47.0269 162.471 48.0773 157.76 L48.5229 155.755 Q49.8597 149.517 52.3105 146.907 Q54.7294 144.265 59.09 144.265 Q64.0552 144.265 66.9516 148.212 Q69.848 152.127 69.848 159.002 Q69.848 161.866 69.2751 164.985 Q68.734 168.073 67.62 171.51 L61.5726 171.51 Q63.2595 168.264 64.1189 165.113 Q64.9464 161.962 64.9464 158.874 Q64.9464 154.737 63.546 152.509 Q62.1137 150.281 59.5356 150.281 Q57.1484 150.281 55.8753 151.904 Q54.6021 153.495 53.4245 158.938 L52.9471 160.975 Q51.8012 166.418 49.4459 168.837 Q47.0588 171.256 42.9211 171.256 Q37.8922 171.256 35.1549 167.691 Q32.4176 164.126 32.4176 157.569 Q32.4176 154.323 32.8951 151.458 Q33.3725 148.594 34.3274 146.175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M49.6369 104.448 L52.5015 104.448 L52.5015 131.375 Q58.5489 130.993 61.7317 127.746 Q64.8828 124.468 64.8828 118.643 Q64.8828 115.269 64.0552 112.118 Q63.2277 108.935 61.5726 105.816 L67.1107 105.816 Q68.4475 108.967 69.1478 112.277 Q69.848 115.588 69.848 118.993 Q69.848 127.523 64.8828 132.52 Q59.9175 137.486 51.4511 137.486 Q42.6983 137.486 37.5739 132.775 Q32.4176 128.033 32.4176 120.012 Q32.4176 112.819 37.0646 108.649 Q41.6797 104.448 49.6369 104.448 M47.9181 110.304 Q43.112 110.368 40.2475 113.009 Q37.3829 115.619 37.3829 119.948 Q37.3829 124.85 40.152 127.81 Q42.9211 130.738 47.95 131.184 L47.9181 110.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M23.1555 89.0426 L33.277 89.0426 L33.277 76.9796 L37.8285 76.9796 L37.8285 89.0426 L57.1802 89.0426 Q61.5408 89.0426 62.7821 87.8649 Q64.0234 86.6555 64.0234 82.9952 L64.0234 76.9796 L68.925 76.9796 L68.925 82.9952 Q68.925 89.7747 66.4105 92.3528 Q63.8642 94.9309 57.1802 94.9309 L37.8285 94.9309 L37.8285 99.2277 L33.277 99.2277 L33.277 94.9309 L23.1555 94.9309 L23.1555 89.0426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip562)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.733,75.1678 292.486,331.292 325.239,436.29 357.991,567.338 390.744,654.567 423.496,694.619 456.249,732.031 489.001,810.14 521.754,803.081 554.507,798.985 \n",
       "  587.259,840.697 652.764,880.775 718.269,834.891 783.775,915.808 849.28,915.282 947.537,949.011 1045.8,930.738 1176.81,969.325 1307.82,989.3 1471.58,1005.96 \n",
       "  1668.09,994.162 1864.61,951.876 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"\n",
       "M1614.6 183.812 L1856.05 183.812 L1856.05 80.132 L1614.6 80.132  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1614.6,183.812 1856.05,183.812 1856.05,80.132 1614.6,80.132 1614.6,183.812 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1633.5,131.972 1746.91,131.972 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M1779.66 151.659 Q1777.85 156.289 1776.14 157.701 Q1774.43 159.113 1771.56 159.113 L1768.15 159.113 L1768.15 155.548 L1770.65 155.548 Q1772.41 155.548 1773.39 154.715 Q1774.36 153.882 1775.54 150.78 L1776.3 148.835 L1765.82 123.326 L1770.33 123.326 L1778.43 143.604 L1786.53 123.326 L1791.05 123.326 L1779.66 151.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1798.34 145.317 L1805.98 145.317 L1805.98 118.951 L1797.67 120.618 L1797.67 116.359 L1805.93 114.692 L1810.61 114.692 L1810.61 145.317 L1818.25 145.317 L1818.25 149.252 L1798.34 149.252 L1798.34 145.317 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "savefig(\"learning_curve.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CategoricalArrays.CategoricalValue{String, UInt32} \"small\" (1/3)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([\"small\", \"big\", \"huge\"], 10), OrderedFactor);\n",
    "levels!(salary, [\"small\", \"big\", \"huge\"]);\n",
    "small = salary[1]"
   ],
   "metadata": {},
   "execution_count": 50
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10-element Vector{Int64}:\n 3\n 3\n 1\n 2\n 1\n 3\n 0\n 1\n 1\n 1"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "cell_type": "code",
   "source": [
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5 (unpack)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After evaluating the following ..."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬────────────┬────────────┬──────────────────────────────────┐\n",
      "│ a     │ b          │ c          │ d                                │\n",
      "│ Int64 │ Float64    │ Float64    │ CategoricalValue{String, UInt32} │\n",
      "│ Count │ Continuous │ Continuous │ OrderedFactor{2}                 │\n",
      "├───────┼────────────┼────────────┼──────────────────────────────────┤\n",
      "│ 1     │ 0.43       │ 0.157078   │ male                             │\n",
      "│ 2     │ 0.532358   │ 0.0199848  │ female                           │\n",
      "│ 3     │ 0.610197   │ 0.189835   │ female                           │\n",
      "│ 4     │ 0.0804821  │ 0.444646   │ male                             │\n",
      "└───────┴────────────┴────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "        b = rand(4),\n",
    "        c = rand(4),\n",
    "        d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)"
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Tables\n",
    "\n",
    "y, X, w = unpack(data,\n",
    "                 ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous,\n",
    "                 name -> true);"
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "...attempt to guess the evaluations of the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Int64}:\n 1\n 2\n 3\n 4"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {},
   "execution_count": 54
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┐\n",
      "│ b          │ c          │\n",
      "│ Float64    │ Float64    │\n",
      "│ Continuous │ Continuous │\n",
      "├────────────┼────────────┤\n",
      "│ 0.43       │ 0.157078   │\n",
      "│ 0.532358   │ 0.0199848  │\n",
      "│ 0.610197   │ 0.189835   │\n",
      "│ 0.0804821  │ 0.444646   │\n",
      "└────────────┴────────────┘\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "pretty(X)"
   ],
   "metadata": {},
   "execution_count": 55
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"male\"\n \"female\"\n \"female\"\n \"male\""
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "cell_type": "code",
   "source": [
    "w"
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6 (first steps in modeling Horse Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data introduced in Part 1, together with the\n",
    "type coercions we performed there:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌─────────────────────────┬─────────────────────────────────┬──────────────────┐\n│\u001b[22m _.names                 \u001b[0m│\u001b[22m _.types                         \u001b[0m│\u001b[22m _.scitypes       \u001b[0m│\n├─────────────────────────┼─────────────────────────────────┼──────────────────┤\n│ surgery                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n│ age                     │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n│ rectal_temperature      │ Float64                         │ Continuous       │\n│ pulse                   │ Float64                         │ Continuous       │\n│ respiratory_rate        │ Float64                         │ Continuous       │\n│ temperature_extremities │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ mucous_membranes        │ CategoricalValue{Int64, UInt32} │ Multiclass{6}    │\n│ capillary_refill_time   │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    │\n│ pain                    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{5} │\n│ peristalsis             │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ abdominal_distension    │ CategoricalValue{Int64, UInt32} │ OrderedFactor{4} │\n│ packed_cell_volume      │ Float64                         │ Continuous       │\n│ total_protein           │ Float64                         │ Continuous       │\n│ outcome                 │ CategoricalValue{Int64, UInt32} │ Multiclass{3}    │\n│ surgical_lesion         │ CategoricalValue{Int64, UInt32} │ OrderedFactor{2} │\n│ cp_data                 │ CategoricalValue{Int64, UInt32} │ Multiclass{2}    │\n└─────────────────────────┴─────────────────────────────────┴──────────────────┘\n_.nrows = 366\n"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "cell_type": "code",
   "source": [
    "using UrlDownload, CSV\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ],
   "metadata": {},
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable, based on\n",
    "the remaining variables that are `Continuous` (one-hot encoding\n",
    "categorical variables is discussed later in Part 3) *while ignoring\n",
    "the others*.  Extract from the `horse` data set (defined in Part 1)\n",
    "appropriate input features `X` and target variable `y`. (Do not,\n",
    "however, randomize the observations.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (You can convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary with `Dict(v)`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probability of the observed class exceed 50%?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substantially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-3-transformers-and-pipelines'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
