{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Julia (continued)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An introduction to the\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)\n",
    "toolbox."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [MLJ Cheatsheet](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/)\n",
    "- [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/)\n",
    "- [MLJ manual](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [Data Science Tutorials in Julia](https://juliaai.github.io/DataScienceTutorials.jl/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4 - Tuning Hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive tuning of a single parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most naive way to tune a single hyper-parameter is to use\n",
    "`learning_curve`, which we already saw in Part 2. Let's see this in\n",
    "the Horse Colic classification problem, a case where the parameter\n",
    "to be tuned is *nested* (because the model is a pipeline)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Horse Colic data again, with the type coercions we\n",
    "already discussed in Part 1:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using UrlDownload, CSV, DataFrames\n",
    "csv_file = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                   \"MachineLearningInJulia2020/\"*\n",
    "                   \"for-MLJ-version-0.16/data/horse.csv\");\n",
    "horse = DataFrames.DataFrame(csv_file); # convert to data frame\n",
    "coerce!(horse, autotype(horse));\n",
    "coerce!(horse, Count => Continuous);\n",
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "\n",
    "y, X = unpack(horse, ==(:outcome), name -> true);\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for a pipeline model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "model = @pipeline Standardizer ContinuousEncoder LogisticClassifier\n",
    "mach = machine(model, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(model, :(logistic_classifier.lambda), lower = 1e-2, upper=100, scale=:log10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're curious, you can see what `lambda` values this range will\n",
    "generate for a given resolution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iterator(r, 5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(490,300))\n",
    "_, _, lambdas, losses = learning_curve(mach,\n",
    "                                       range=r,\n",
    "                                       resampling=CV(nfolds=6),\n",
    "                                       resolution=30, # default\n",
    "                                       measure=cross_entropy)\n",
    "plt=plot(lambdas, losses, xscale=:log10)\n",
    "xlabel!(plt, \"lambda\")\n",
    "ylabel!(plt, \"cross entropy using 6-fold CV\")\n",
    "savefig(\"learning_curve2.png\")\n",
    "plt\n",
    "\n",
    "\n",
    "best_lambda = lambdas[argmin(losses)]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self tuning models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A more sophisticated way to view hyper-parameter tuning (inspired by\n",
    "MLR) is as a model *wrapper*. The wrapped model is a new model in\n",
    "its own right and when you fit it, it tunes specified\n",
    "hyper-parameters of the model being wrapped, before training on all\n",
    "supplied data. Calling `predict` on the wrapped model is like\n",
    "calling `predict` on the original model, but with the\n",
    "hyper-parameters already optimized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In other words, we can think of the wrapped model as a \"self-tuning\"\n",
    "version of the original."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create a self-tuning version of the pipeline above, adding a\n",
    "parameter from the `ContinuousEncoder` to the parameters we want\n",
    "optimized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's choose a tuning strategy (from [these\n",
    "options](https://github.com/juliaai/MLJTuning.jl#what-is-provided-here)). MLJ\n",
    "supports ordinary `Grid` search (query `?Grid` for\n",
    "details). However, as the utility of `Grid` search is limited to a\n",
    "small number of parameters, and as `Grid` searches are demonstrated\n",
    "elsewhere (see the [resources below](#resources-for-part-4)) we'll\n",
    "demonstrate `RandomSearch` here:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuning = RandomSearch(rng=123)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this strategy each parameter is sampled according to a\n",
    "pre-specified prior distribution that is fit to the one-dimensional\n",
    "range object constructed using `range` as before. While one has a\n",
    "lot of control over the specification of the priors (run\n",
    "`?RandomSearch` for details) we'll let the algorithm generate these\n",
    "priors automatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Unbounded ranges and sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a range does not have to be bounded. In a `RandomSearch` a\n",
    "positive unbounded range is sampled using a `Gamma` distribution, by\n",
    "default:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(model,\n",
    "          :(logistic_classifier.lambda),\n",
    "          lower=0,\n",
    "          origin=6,\n",
    "          unit=5,\n",
    "          scale=:log10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `scale` in a range is ignored in a `RandomSearch`, unless it is a\n",
    "function. (It *is* relevant in a `Grid` search, not demonstrated\n",
    "here.) Note however, the choice of scale *does* effect how later plots\n",
    "will look."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what sampling using a Gamma distribution is going to mean\n",
    "for this range:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Distributions\n",
    "sampler_r = sampler(r, Distributions.Gamma)\n",
    "plt = histogram(rand(sampler_r, 10000), nbins=50)\n",
    "savefig(\"gamma_sampler.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second parameter that we'll add to this is *nominal* (finite) and, by\n",
    "default, will be sampled uniformly. Since it is nominal, we specify\n",
    "`values` instead of `upper` and `lower` bounds:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "s  = range(model, :(continuous_encoder.one_hot_ordered_factors),\n",
    "           values = [true, false])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The tuning wrapper"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for the wrapper, which is an instance of `TunedModel`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_model = TunedModel(model=model,\n",
    "                         ranges=[r, s],\n",
    "                         resampling=CV(nfolds=6),\n",
    "                         measures=cross_entropy,\n",
    "                         tuning=tuning,\n",
    "                         n=15)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can apply the `fit!/predict` work-flow to `tuned_model` just as\n",
    "for any other model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_mach = machine(tuned_model, X, y);\n",
    "fit!(tuned_mach);\n",
    "predict(tuned_mach, rows=1:3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The outcomes of the tuning can be inspected from a detailed\n",
    "report. For example, we have:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rep = report(tuned_mach);\n",
    "rep.best_model"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the special case of two-parameters, you can also plot the results:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plt = plot(tuned_mach)\n",
    "savefig(\"tuning.png\")\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's compare cross-validation estimate of the performance\n",
    "of the self-tuning model with that of the original model (an example\n",
    "of [*nested\n",
    "resampling*]((https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html)\n",
    "here):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "err = evaluate!(mach, resampling=CV(nfolds=3), measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_err = evaluate!(tuned_mach, resampling=CV(nfolds=3), measure=cross_entropy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='resources-for-part-4'></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources for Part 4\n",
    "\n",
    "- From the MLJ manual:\n",
    "   - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "   - [Tuning Models](https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/)\n",
    "- The [MLJTuning repo](https://github.com/juliaai/MLJTuning.jl#who-is-this-repo-for) - mostly for developers\n",
    "\n",
    "- From Data Science Tutorials:\n",
    "    - [Tuning a model](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/model-tuning/)\n",
    "    - [Crabs with XGBoost](https://juliaai.github.io/DataScienceTutorials.jl/end-to-end/crabs-xgb/) `Grid` tuning in stages for a tree-boosting model with many parameters\n",
    "    - [Boston with LightGBM](https://juliaai.github.io/DataScienceTutorials.jl/end-to-end/boston-lgbm/) -  `Grid` tuning for another popular tree-booster\n",
    "    - [Boston with Flux](https://juliaai.github.io/DataScienceTutorials.jl/end-to-end/boston-flux/) - optimizing batch size in a simple neural network regressor\n",
    "- [UCI Horse Colic Data Set](http://archive.ics.uci.edu/ml/datasets/Horse+Colic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercises for Part 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This exercise continues our analysis of the King County House price\n",
    "prediction problem (Part 1, Exercise 3 and Part 2):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "house_csv = urldownload(\"https://raw.githubusercontent.com/ablaom/\"*\n",
    "                        \"MachineLearningInJulia2020/for-MLJ-version-0.16/\"*\n",
    "                        \"data/house.csv\");\n",
    "house = DataFrames.DataFrame(house_csv)\n",
    "coerce!(house, autotype(house_csv));\n",
    "coerce!(house, Count => Continuous, :zipcode => Multiclass);\n",
    "y, X = unpack(house, ==(:price), name -> true, rng=123);\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your task will be to tune the following pipeline regression model,\n",
    "which includes a gradient tree boosting component:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "EvoTreeRegressor = @load EvoTreeRegressor\n",
    "tree_booster = EvoTreeRegressor(nrounds = 70)\n",
    "model = @pipeline ContinuousEncoder tree_booster"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Construct a bounded range `r1` for the `evo_tree_booster`\n",
    "parameter `max_depth`, varying between 1 and 12."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\star&(b) For the `nbins` parameter of the `EvoTreeRegressor`, define the range"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r2 = range(model,\n",
    "           :(evo_tree_regressor.nbins),\n",
    "           lower = 2.5,\n",
    "           upper= 7.5, scale=x->2^round(Int, x))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that in this case we've specified a *function* instead of a\n",
    "canned scale, like `:log10`. In this case the `scale` function is\n",
    "applied after sampling (uniformly) between the limits of `lower` and\n",
    "`upper`. Perhaps you can guess the outputs of the following lines of\n",
    "code?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r2_sampler = sampler(r2, Distributions.Uniform)\n",
    "samples = rand(r2_sampler, 1000);\n",
    "plt = histogram(samples, nbins=50)\n",
    "savefig(\"uniform_sampler.png\")\n",
    "\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](uniform_sampler.png)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sort(unique(samples))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Optimize `model` over these the parameter ranges `r1` and `r2`\n",
    "using a random search with uniform priors (the default). Use\n",
    "`Holdout()` resampling, and implement your search by first\n",
    "constructing a \"self-tuning\" wrap of `model`, as described\n",
    "above. Make `mae` (mean absolute error) the loss function that you\n",
    "optimize, and search over a total of 40 combinations of\n",
    "hyper-parameters.  If you have time, plot the results of your\n",
    "search. Feel free to use all available data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(d) Evaluate the best model found in the search using 3-fold\n",
    "cross-validation and compare with that of the self-tuning model\n",
    "(which is different!). Setting data hygiene concerns aside, feel\n",
    "free to use all available data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='part-5-advanced-model-composition'>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
